I1128 19:31:37.398075 30336 caffe.cpp:218] Using GPUs 0
I1128 19:31:37.402437 30336 caffe.cpp:223] GPU 0: GeForce GTX 1060 6GB
I1128 19:31:37.614984 30336 solver.cpp:44] Initializing solver from parameters: 
train_net: "/home/ljf/caffe-master/examples/ljftest_cifar10_WRN/train.prototxt"
test_net: "/home/ljf/caffe-master/examples/ljftest_cifar10_WRN/test.prototxt"
test_iter: 1000
test_interval: 500
base_lr: 0.1
display: 200
max_iter: 100000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 200000
snapshot: 2500
snapshot_prefix: "/home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train"
solver_mode: GPU
device_id: 0
random_seed: 831486
train_state {
  level: 0
  stage: ""
}
stepvalue: 32000
stepvalue: 48000
stepvalue: 72000
stepvalue: 96000
iter_size: 1
type: "Nesterov"
I1128 19:31:37.615172 30336 solver.cpp:77] Creating training net from train_net file: /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/train.prototxt
I1128 19:31:37.615990 30336 net.cpp:51] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  transform_param {
    mirror: true
    crop_size: 28
  }
  data_param {
    source: "/home/ljf/caffe-master/examples/ljftest_cifar10_WRN/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution3"
  bottom: "Convolution1"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Convolution5"
  bottom: "Eltwise1"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Convolution7"
  bottom: "Eltwise2"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution10"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution9"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Convolution12"
  bottom: "Eltwise4"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Convolution14"
  bottom: "Eltwise5"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution15"
  top: "Convolution15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Convolution15"
  top: "Convolution16"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution17"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution16"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Convolution19"
  bottom: "Eltwise7"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution20"
  top: "Convolution20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Convolution21"
  bottom: "Eltwise8"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution22"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution22"
  top: "Convolution22"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Convolution22"
  top: "Convolution23"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution24"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Convolution24"
  top: "Convolution24"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Convolution23"
  bottom: "Convolution24"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution25"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution25"
  top: "Convolution25"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Convolution25"
  top: "Convolution26"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Convolution26"
  top: "Convolution26"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "Convolution26"
  top: "Convolution26"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Convolution26"
  bottom: "Eltwise10"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution27"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "Convolution27"
  top: "Convolution27"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Convolution27"
  top: "Convolution28"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm28"
  type: "BatchNorm"
  bottom: "Convolution28"
  top: "Convolution28"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale28"
  type: "Scale"
  bottom: "Convolution28"
  top: "Convolution28"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Convolution28"
  bottom: "Eltwise11"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "Eltwise12"
  top: "Eltwise12"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise12"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
I1128 19:31:37.616473 30336 layer_factory.hpp:77] Creating layer Data1
I1128 19:31:37.616562 30336 db_lmdb.cpp:35] Opened lmdb /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/train_lmdb
I1128 19:31:37.616590 30336 net.cpp:84] Creating Layer Data1
I1128 19:31:37.616597 30336 net.cpp:380] Data1 -> Data1
I1128 19:31:37.616616 30336 net.cpp:380] Data1 -> Data2
I1128 19:31:37.617385 30336 data_layer.cpp:45] output data size: 128,3,28,28
I1128 19:31:37.621165 30336 net.cpp:122] Setting up Data1
I1128 19:31:37.621186 30336 net.cpp:129] Top shape: 128 3 28 28 (301056)
I1128 19:31:37.621189 30336 net.cpp:129] Top shape: 128 (128)
I1128 19:31:37.621193 30336 net.cpp:137] Memory required for data: 1204736
I1128 19:31:37.621202 30336 layer_factory.hpp:77] Creating layer Convolution1
I1128 19:31:37.621222 30336 net.cpp:84] Creating Layer Convolution1
I1128 19:31:37.621227 30336 net.cpp:406] Convolution1 <- Data1
I1128 19:31:37.621239 30336 net.cpp:380] Convolution1 -> Convolution1
I1128 19:31:37.932593 30336 net.cpp:122] Setting up Convolution1
I1128 19:31:37.932615 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.932620 30336 net.cpp:137] Memory required for data: 39739904
I1128 19:31:37.932663 30336 layer_factory.hpp:77] Creating layer BatchNorm1
I1128 19:31:37.932677 30336 net.cpp:84] Creating Layer BatchNorm1
I1128 19:31:37.932682 30336 net.cpp:406] BatchNorm1 <- Convolution1
I1128 19:31:37.932713 30336 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1128 19:31:37.932868 30336 net.cpp:122] Setting up BatchNorm1
I1128 19:31:37.932874 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.932876 30336 net.cpp:137] Memory required for data: 78275072
I1128 19:31:37.932900 30336 layer_factory.hpp:77] Creating layer Scale1
I1128 19:31:37.932907 30336 net.cpp:84] Creating Layer Scale1
I1128 19:31:37.932910 30336 net.cpp:406] Scale1 <- Convolution1
I1128 19:31:37.932915 30336 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1128 19:31:37.932943 30336 layer_factory.hpp:77] Creating layer Scale1
I1128 19:31:37.933015 30336 net.cpp:122] Setting up Scale1
I1128 19:31:37.933020 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.933023 30336 net.cpp:137] Memory required for data: 116810240
I1128 19:31:37.933040 30336 layer_factory.hpp:77] Creating layer ReLU1
I1128 19:31:37.933048 30336 net.cpp:84] Creating Layer ReLU1
I1128 19:31:37.933051 30336 net.cpp:406] ReLU1 <- Convolution1
I1128 19:31:37.933055 30336 net.cpp:367] ReLU1 -> Convolution1 (in-place)
I1128 19:31:37.933171 30336 net.cpp:122] Setting up ReLU1
I1128 19:31:37.933178 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.933182 30336 net.cpp:137] Memory required for data: 155345408
I1128 19:31:37.933184 30336 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I1128 19:31:37.933189 30336 net.cpp:84] Creating Layer Convolution1_ReLU1_0_split
I1128 19:31:37.933193 30336 net.cpp:406] Convolution1_ReLU1_0_split <- Convolution1
I1128 19:31:37.933198 30336 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I1128 19:31:37.933205 30336 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I1128 19:31:37.933233 30336 net.cpp:122] Setting up Convolution1_ReLU1_0_split
I1128 19:31:37.933238 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.933243 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.933246 30336 net.cpp:137] Memory required for data: 232415744
I1128 19:31:37.933250 30336 layer_factory.hpp:77] Creating layer Convolution2
I1128 19:31:37.933259 30336 net.cpp:84] Creating Layer Convolution2
I1128 19:31:37.933262 30336 net.cpp:406] Convolution2 <- Convolution1_ReLU1_0_split_0
I1128 19:31:37.933267 30336 net.cpp:380] Convolution2 -> Convolution2
I1128 19:31:37.934600 30336 net.cpp:122] Setting up Convolution2
I1128 19:31:37.934609 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.934612 30336 net.cpp:137] Memory required for data: 270950912
I1128 19:31:37.934622 30336 layer_factory.hpp:77] Creating layer BatchNorm2
I1128 19:31:37.934628 30336 net.cpp:84] Creating Layer BatchNorm2
I1128 19:31:37.934631 30336 net.cpp:406] BatchNorm2 <- Convolution2
I1128 19:31:37.934638 30336 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1128 19:31:37.934751 30336 net.cpp:122] Setting up BatchNorm2
I1128 19:31:37.934757 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.934761 30336 net.cpp:137] Memory required for data: 309486080
I1128 19:31:37.934767 30336 layer_factory.hpp:77] Creating layer Scale2
I1128 19:31:37.934774 30336 net.cpp:84] Creating Layer Scale2
I1128 19:31:37.934777 30336 net.cpp:406] Scale2 <- Convolution2
I1128 19:31:37.934782 30336 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1128 19:31:37.934805 30336 layer_factory.hpp:77] Creating layer Scale2
I1128 19:31:37.934872 30336 net.cpp:122] Setting up Scale2
I1128 19:31:37.934877 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.934880 30336 net.cpp:137] Memory required for data: 348021248
I1128 19:31:37.934886 30336 layer_factory.hpp:77] Creating layer ReLU2
I1128 19:31:37.934891 30336 net.cpp:84] Creating Layer ReLU2
I1128 19:31:37.934895 30336 net.cpp:406] ReLU2 <- Convolution2
I1128 19:31:37.934900 30336 net.cpp:367] ReLU2 -> Convolution2 (in-place)
I1128 19:31:37.935006 30336 net.cpp:122] Setting up ReLU2
I1128 19:31:37.935012 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.935015 30336 net.cpp:137] Memory required for data: 386556416
I1128 19:31:37.935019 30336 layer_factory.hpp:77] Creating layer Convolution3
I1128 19:31:37.935026 30336 net.cpp:84] Creating Layer Convolution3
I1128 19:31:37.935029 30336 net.cpp:406] Convolution3 <- Convolution2
I1128 19:31:37.935034 30336 net.cpp:380] Convolution3 -> Convolution3
I1128 19:31:37.936349 30336 net.cpp:122] Setting up Convolution3
I1128 19:31:37.936358 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.936362 30336 net.cpp:137] Memory required for data: 425091584
I1128 19:31:37.936368 30336 layer_factory.hpp:77] Creating layer BatchNorm3
I1128 19:31:37.936374 30336 net.cpp:84] Creating Layer BatchNorm3
I1128 19:31:37.936379 30336 net.cpp:406] BatchNorm3 <- Convolution3
I1128 19:31:37.936384 30336 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1128 19:31:37.936506 30336 net.cpp:122] Setting up BatchNorm3
I1128 19:31:37.936511 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.936516 30336 net.cpp:137] Memory required for data: 463626752
I1128 19:31:37.936524 30336 layer_factory.hpp:77] Creating layer Scale3
I1128 19:31:37.936529 30336 net.cpp:84] Creating Layer Scale3
I1128 19:31:37.936533 30336 net.cpp:406] Scale3 <- Convolution3
I1128 19:31:37.936540 30336 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1128 19:31:37.936563 30336 layer_factory.hpp:77] Creating layer Scale3
I1128 19:31:37.936631 30336 net.cpp:122] Setting up Scale3
I1128 19:31:37.936636 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.936641 30336 net.cpp:137] Memory required for data: 502161920
I1128 19:31:37.936646 30336 layer_factory.hpp:77] Creating layer Eltwise1
I1128 19:31:37.936652 30336 net.cpp:84] Creating Layer Eltwise1
I1128 19:31:37.936655 30336 net.cpp:406] Eltwise1 <- Convolution3
I1128 19:31:37.936661 30336 net.cpp:406] Eltwise1 <- Convolution1_ReLU1_0_split_1
I1128 19:31:37.936666 30336 net.cpp:380] Eltwise1 -> Eltwise1
I1128 19:31:37.936684 30336 net.cpp:122] Setting up Eltwise1
I1128 19:31:37.936689 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.936693 30336 net.cpp:137] Memory required for data: 540697088
I1128 19:31:37.936697 30336 layer_factory.hpp:77] Creating layer ReLU3
I1128 19:31:37.936702 30336 net.cpp:84] Creating Layer ReLU3
I1128 19:31:37.936707 30336 net.cpp:406] ReLU3 <- Eltwise1
I1128 19:31:37.936712 30336 net.cpp:367] ReLU3 -> Eltwise1 (in-place)
I1128 19:31:37.936816 30336 net.cpp:122] Setting up ReLU3
I1128 19:31:37.936822 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.936825 30336 net.cpp:137] Memory required for data: 579232256
I1128 19:31:37.936828 30336 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I1128 19:31:37.936833 30336 net.cpp:84] Creating Layer Eltwise1_ReLU3_0_split
I1128 19:31:37.936837 30336 net.cpp:406] Eltwise1_ReLU3_0_split <- Eltwise1
I1128 19:31:37.936842 30336 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I1128 19:31:37.936851 30336 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I1128 19:31:37.936875 30336 net.cpp:122] Setting up Eltwise1_ReLU3_0_split
I1128 19:31:37.936880 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.936884 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.936887 30336 net.cpp:137] Memory required for data: 656302592
I1128 19:31:37.936890 30336 layer_factory.hpp:77] Creating layer Convolution4
I1128 19:31:37.936899 30336 net.cpp:84] Creating Layer Convolution4
I1128 19:31:37.936902 30336 net.cpp:406] Convolution4 <- Eltwise1_ReLU3_0_split_0
I1128 19:31:37.936908 30336 net.cpp:380] Convolution4 -> Convolution4
I1128 19:31:37.938211 30336 net.cpp:122] Setting up Convolution4
I1128 19:31:37.938220 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.938222 30336 net.cpp:137] Memory required for data: 694837760
I1128 19:31:37.938230 30336 layer_factory.hpp:77] Creating layer BatchNorm4
I1128 19:31:37.938235 30336 net.cpp:84] Creating Layer BatchNorm4
I1128 19:31:37.938239 30336 net.cpp:406] BatchNorm4 <- Convolution4
I1128 19:31:37.938244 30336 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1128 19:31:37.938369 30336 net.cpp:122] Setting up BatchNorm4
I1128 19:31:37.938374 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.938377 30336 net.cpp:137] Memory required for data: 733372928
I1128 19:31:37.938400 30336 layer_factory.hpp:77] Creating layer Scale4
I1128 19:31:37.938405 30336 net.cpp:84] Creating Layer Scale4
I1128 19:31:37.938408 30336 net.cpp:406] Scale4 <- Convolution4
I1128 19:31:37.938413 30336 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1128 19:31:37.938436 30336 layer_factory.hpp:77] Creating layer Scale4
I1128 19:31:37.938515 30336 net.cpp:122] Setting up Scale4
I1128 19:31:37.938520 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.938524 30336 net.cpp:137] Memory required for data: 771908096
I1128 19:31:37.938549 30336 layer_factory.hpp:77] Creating layer ReLU4
I1128 19:31:37.938555 30336 net.cpp:84] Creating Layer ReLU4
I1128 19:31:37.938558 30336 net.cpp:406] ReLU4 <- Convolution4
I1128 19:31:37.938563 30336 net.cpp:367] ReLU4 -> Convolution4 (in-place)
I1128 19:31:37.938680 30336 net.cpp:122] Setting up ReLU4
I1128 19:31:37.938686 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.938689 30336 net.cpp:137] Memory required for data: 810443264
I1128 19:31:37.938706 30336 layer_factory.hpp:77] Creating layer Convolution5
I1128 19:31:37.938715 30336 net.cpp:84] Creating Layer Convolution5
I1128 19:31:37.938719 30336 net.cpp:406] Convolution5 <- Convolution4
I1128 19:31:37.938724 30336 net.cpp:380] Convolution5 -> Convolution5
I1128 19:31:37.940033 30336 net.cpp:122] Setting up Convolution5
I1128 19:31:37.940042 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.940047 30336 net.cpp:137] Memory required for data: 848978432
I1128 19:31:37.940052 30336 layer_factory.hpp:77] Creating layer BatchNorm5
I1128 19:31:37.940059 30336 net.cpp:84] Creating Layer BatchNorm5
I1128 19:31:37.940062 30336 net.cpp:406] BatchNorm5 <- Convolution5
I1128 19:31:37.940068 30336 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1128 19:31:37.940181 30336 net.cpp:122] Setting up BatchNorm5
I1128 19:31:37.940186 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.940189 30336 net.cpp:137] Memory required for data: 887513600
I1128 19:31:37.940199 30336 layer_factory.hpp:77] Creating layer Scale5
I1128 19:31:37.940204 30336 net.cpp:84] Creating Layer Scale5
I1128 19:31:37.940207 30336 net.cpp:406] Scale5 <- Convolution5
I1128 19:31:37.940212 30336 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1128 19:31:37.940235 30336 layer_factory.hpp:77] Creating layer Scale5
I1128 19:31:37.940305 30336 net.cpp:122] Setting up Scale5
I1128 19:31:37.940311 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.940315 30336 net.cpp:137] Memory required for data: 926048768
I1128 19:31:37.940320 30336 layer_factory.hpp:77] Creating layer Eltwise2
I1128 19:31:37.940325 30336 net.cpp:84] Creating Layer Eltwise2
I1128 19:31:37.940328 30336 net.cpp:406] Eltwise2 <- Convolution5
I1128 19:31:37.940332 30336 net.cpp:406] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I1128 19:31:37.940338 30336 net.cpp:380] Eltwise2 -> Eltwise2
I1128 19:31:37.940356 30336 net.cpp:122] Setting up Eltwise2
I1128 19:31:37.940361 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.940363 30336 net.cpp:137] Memory required for data: 964583936
I1128 19:31:37.940368 30336 layer_factory.hpp:77] Creating layer ReLU5
I1128 19:31:37.940372 30336 net.cpp:84] Creating Layer ReLU5
I1128 19:31:37.940376 30336 net.cpp:406] ReLU5 <- Eltwise2
I1128 19:31:37.940381 30336 net.cpp:367] ReLU5 -> Eltwise2 (in-place)
I1128 19:31:37.940688 30336 net.cpp:122] Setting up ReLU5
I1128 19:31:37.940696 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.940699 30336 net.cpp:137] Memory required for data: 1003119104
I1128 19:31:37.940702 30336 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I1128 19:31:37.940708 30336 net.cpp:84] Creating Layer Eltwise2_ReLU5_0_split
I1128 19:31:37.940711 30336 net.cpp:406] Eltwise2_ReLU5_0_split <- Eltwise2
I1128 19:31:37.940716 30336 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I1128 19:31:37.940722 30336 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I1128 19:31:37.940749 30336 net.cpp:122] Setting up Eltwise2_ReLU5_0_split
I1128 19:31:37.940754 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.940758 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.940762 30336 net.cpp:137] Memory required for data: 1080189440
I1128 19:31:37.940764 30336 layer_factory.hpp:77] Creating layer Convolution6
I1128 19:31:37.940773 30336 net.cpp:84] Creating Layer Convolution6
I1128 19:31:37.940775 30336 net.cpp:406] Convolution6 <- Eltwise2_ReLU5_0_split_0
I1128 19:31:37.940780 30336 net.cpp:380] Convolution6 -> Convolution6
I1128 19:31:37.941910 30336 net.cpp:122] Setting up Convolution6
I1128 19:31:37.941918 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.941921 30336 net.cpp:137] Memory required for data: 1118724608
I1128 19:31:37.941928 30336 layer_factory.hpp:77] Creating layer BatchNorm6
I1128 19:31:37.941934 30336 net.cpp:84] Creating Layer BatchNorm6
I1128 19:31:37.941939 30336 net.cpp:406] BatchNorm6 <- Convolution6
I1128 19:31:37.941944 30336 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1128 19:31:37.942059 30336 net.cpp:122] Setting up BatchNorm6
I1128 19:31:37.942065 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.942070 30336 net.cpp:137] Memory required for data: 1157259776
I1128 19:31:37.942077 30336 layer_factory.hpp:77] Creating layer Scale6
I1128 19:31:37.942081 30336 net.cpp:84] Creating Layer Scale6
I1128 19:31:37.942085 30336 net.cpp:406] Scale6 <- Convolution6
I1128 19:31:37.942091 30336 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1128 19:31:37.942113 30336 layer_factory.hpp:77] Creating layer Scale6
I1128 19:31:37.942183 30336 net.cpp:122] Setting up Scale6
I1128 19:31:37.942188 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.942191 30336 net.cpp:137] Memory required for data: 1195794944
I1128 19:31:37.942196 30336 layer_factory.hpp:77] Creating layer ReLU6
I1128 19:31:37.942201 30336 net.cpp:84] Creating Layer ReLU6
I1128 19:31:37.942204 30336 net.cpp:406] ReLU6 <- Convolution6
I1128 19:31:37.942209 30336 net.cpp:367] ReLU6 -> Convolution6 (in-place)
I1128 19:31:37.942481 30336 net.cpp:122] Setting up ReLU6
I1128 19:31:37.942488 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.942492 30336 net.cpp:137] Memory required for data: 1234330112
I1128 19:31:37.942495 30336 layer_factory.hpp:77] Creating layer Convolution7
I1128 19:31:37.942517 30336 net.cpp:84] Creating Layer Convolution7
I1128 19:31:37.942522 30336 net.cpp:406] Convolution7 <- Convolution6
I1128 19:31:37.942526 30336 net.cpp:380] Convolution7 -> Convolution7
I1128 19:31:37.943910 30336 net.cpp:122] Setting up Convolution7
I1128 19:31:37.943919 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.943922 30336 net.cpp:137] Memory required for data: 1272865280
I1128 19:31:37.943928 30336 layer_factory.hpp:77] Creating layer BatchNorm7
I1128 19:31:37.943934 30336 net.cpp:84] Creating Layer BatchNorm7
I1128 19:31:37.943939 30336 net.cpp:406] BatchNorm7 <- Convolution7
I1128 19:31:37.943944 30336 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1128 19:31:37.944064 30336 net.cpp:122] Setting up BatchNorm7
I1128 19:31:37.944069 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.944073 30336 net.cpp:137] Memory required for data: 1311400448
I1128 19:31:37.944080 30336 layer_factory.hpp:77] Creating layer Scale7
I1128 19:31:37.944088 30336 net.cpp:84] Creating Layer Scale7
I1128 19:31:37.944092 30336 net.cpp:406] Scale7 <- Convolution7
I1128 19:31:37.944097 30336 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1128 19:31:37.944121 30336 layer_factory.hpp:77] Creating layer Scale7
I1128 19:31:37.944190 30336 net.cpp:122] Setting up Scale7
I1128 19:31:37.944196 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.944200 30336 net.cpp:137] Memory required for data: 1349935616
I1128 19:31:37.944205 30336 layer_factory.hpp:77] Creating layer Eltwise3
I1128 19:31:37.944211 30336 net.cpp:84] Creating Layer Eltwise3
I1128 19:31:37.944214 30336 net.cpp:406] Eltwise3 <- Convolution7
I1128 19:31:37.944219 30336 net.cpp:406] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I1128 19:31:37.944226 30336 net.cpp:380] Eltwise3 -> Eltwise3
I1128 19:31:37.944242 30336 net.cpp:122] Setting up Eltwise3
I1128 19:31:37.944247 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.944252 30336 net.cpp:137] Memory required for data: 1388470784
I1128 19:31:37.944254 30336 layer_factory.hpp:77] Creating layer ReLU7
I1128 19:31:37.944258 30336 net.cpp:84] Creating Layer ReLU7
I1128 19:31:37.944262 30336 net.cpp:406] ReLU7 <- Eltwise3
I1128 19:31:37.944273 30336 net.cpp:367] ReLU7 -> Eltwise3 (in-place)
I1128 19:31:37.944382 30336 net.cpp:122] Setting up ReLU7
I1128 19:31:37.944389 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.944392 30336 net.cpp:137] Memory required for data: 1427005952
I1128 19:31:37.944396 30336 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I1128 19:31:37.944401 30336 net.cpp:84] Creating Layer Eltwise3_ReLU7_0_split
I1128 19:31:37.944403 30336 net.cpp:406] Eltwise3_ReLU7_0_split <- Eltwise3
I1128 19:31:37.944408 30336 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I1128 19:31:37.944416 30336 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I1128 19:31:37.944442 30336 net.cpp:122] Setting up Eltwise3_ReLU7_0_split
I1128 19:31:37.944447 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.944452 30336 net.cpp:129] Top shape: 128 96 28 28 (9633792)
I1128 19:31:37.944454 30336 net.cpp:137] Memory required for data: 1504076288
I1128 19:31:37.944458 30336 layer_factory.hpp:77] Creating layer Convolution8
I1128 19:31:37.944465 30336 net.cpp:84] Creating Layer Convolution8
I1128 19:31:37.944468 30336 net.cpp:406] Convolution8 <- Eltwise3_ReLU7_0_split_0
I1128 19:31:37.944474 30336 net.cpp:380] Convolution8 -> Convolution8
I1128 19:31:37.946430 30336 net.cpp:122] Setting up Convolution8
I1128 19:31:37.946440 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.946444 30336 net.cpp:137] Memory required for data: 1523343872
I1128 19:31:37.946450 30336 layer_factory.hpp:77] Creating layer BatchNorm8
I1128 19:31:37.946457 30336 net.cpp:84] Creating Layer BatchNorm8
I1128 19:31:37.946461 30336 net.cpp:406] BatchNorm8 <- Convolution8
I1128 19:31:37.946466 30336 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1128 19:31:37.946590 30336 net.cpp:122] Setting up BatchNorm8
I1128 19:31:37.946596 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.946599 30336 net.cpp:137] Memory required for data: 1542611456
I1128 19:31:37.946619 30336 layer_factory.hpp:77] Creating layer Scale8
I1128 19:31:37.946624 30336 net.cpp:84] Creating Layer Scale8
I1128 19:31:37.946627 30336 net.cpp:406] Scale8 <- Convolution8
I1128 19:31:37.946631 30336 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1128 19:31:37.946655 30336 layer_factory.hpp:77] Creating layer Scale8
I1128 19:31:37.946727 30336 net.cpp:122] Setting up Scale8
I1128 19:31:37.946732 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.946735 30336 net.cpp:137] Memory required for data: 1561879040
I1128 19:31:37.946740 30336 layer_factory.hpp:77] Creating layer ReLU8
I1128 19:31:37.946745 30336 net.cpp:84] Creating Layer ReLU8
I1128 19:31:37.946749 30336 net.cpp:406] ReLU8 <- Convolution8
I1128 19:31:37.946754 30336 net.cpp:367] ReLU8 -> Convolution8 (in-place)
I1128 19:31:37.946894 30336 net.cpp:122] Setting up ReLU8
I1128 19:31:37.946900 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.946904 30336 net.cpp:137] Memory required for data: 1581146624
I1128 19:31:37.946907 30336 layer_factory.hpp:77] Creating layer Convolution9
I1128 19:31:37.946914 30336 net.cpp:84] Creating Layer Convolution9
I1128 19:31:37.946918 30336 net.cpp:406] Convolution9 <- Convolution8
I1128 19:31:37.946923 30336 net.cpp:380] Convolution9 -> Convolution9
I1128 19:31:37.950475 30336 net.cpp:122] Setting up Convolution9
I1128 19:31:37.950487 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.950490 30336 net.cpp:137] Memory required for data: 1600414208
I1128 19:31:37.950498 30336 layer_factory.hpp:77] Creating layer BatchNorm9
I1128 19:31:37.950505 30336 net.cpp:84] Creating Layer BatchNorm9
I1128 19:31:37.950510 30336 net.cpp:406] BatchNorm9 <- Convolution9
I1128 19:31:37.950528 30336 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1128 19:31:37.950680 30336 net.cpp:122] Setting up BatchNorm9
I1128 19:31:37.950685 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.950687 30336 net.cpp:137] Memory required for data: 1619681792
I1128 19:31:37.950721 30336 layer_factory.hpp:77] Creating layer Scale9
I1128 19:31:37.950727 30336 net.cpp:84] Creating Layer Scale9
I1128 19:31:37.950743 30336 net.cpp:406] Scale9 <- Convolution9
I1128 19:31:37.950748 30336 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1128 19:31:37.950788 30336 layer_factory.hpp:77] Creating layer Scale9
I1128 19:31:37.950898 30336 net.cpp:122] Setting up Scale9
I1128 19:31:37.950904 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.950907 30336 net.cpp:137] Memory required for data: 1638949376
I1128 19:31:37.950912 30336 layer_factory.hpp:77] Creating layer Convolution10
I1128 19:31:37.950922 30336 net.cpp:84] Creating Layer Convolution10
I1128 19:31:37.950925 30336 net.cpp:406] Convolution10 <- Eltwise3_ReLU7_0_split_1
I1128 19:31:37.950932 30336 net.cpp:380] Convolution10 -> Convolution10
I1128 19:31:37.952414 30336 net.cpp:122] Setting up Convolution10
I1128 19:31:37.952424 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.952427 30336 net.cpp:137] Memory required for data: 1658216960
I1128 19:31:37.952440 30336 layer_factory.hpp:77] Creating layer BatchNorm10
I1128 19:31:37.952445 30336 net.cpp:84] Creating Layer BatchNorm10
I1128 19:31:37.952450 30336 net.cpp:406] BatchNorm10 <- Convolution10
I1128 19:31:37.952456 30336 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1128 19:31:37.952635 30336 net.cpp:122] Setting up BatchNorm10
I1128 19:31:37.952639 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.952643 30336 net.cpp:137] Memory required for data: 1677484544
I1128 19:31:37.952651 30336 layer_factory.hpp:77] Creating layer Scale10
I1128 19:31:37.952656 30336 net.cpp:84] Creating Layer Scale10
I1128 19:31:37.952661 30336 net.cpp:406] Scale10 <- Convolution10
I1128 19:31:37.952666 30336 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1128 19:31:37.952705 30336 layer_factory.hpp:77] Creating layer Scale10
I1128 19:31:37.952814 30336 net.cpp:122] Setting up Scale10
I1128 19:31:37.952821 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.952824 30336 net.cpp:137] Memory required for data: 1696752128
I1128 19:31:37.952831 30336 layer_factory.hpp:77] Creating layer Eltwise4
I1128 19:31:37.952836 30336 net.cpp:84] Creating Layer Eltwise4
I1128 19:31:37.952839 30336 net.cpp:406] Eltwise4 <- Convolution9
I1128 19:31:37.952844 30336 net.cpp:406] Eltwise4 <- Convolution10
I1128 19:31:37.952850 30336 net.cpp:380] Eltwise4 -> Eltwise4
I1128 19:31:37.952883 30336 net.cpp:122] Setting up Eltwise4
I1128 19:31:37.952888 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.952893 30336 net.cpp:137] Memory required for data: 1716019712
I1128 19:31:37.952908 30336 layer_factory.hpp:77] Creating layer ReLU9
I1128 19:31:37.952913 30336 net.cpp:84] Creating Layer ReLU9
I1128 19:31:37.952931 30336 net.cpp:406] ReLU9 <- Eltwise4
I1128 19:31:37.952937 30336 net.cpp:367] ReLU9 -> Eltwise4 (in-place)
I1128 19:31:37.953061 30336 net.cpp:122] Setting up ReLU9
I1128 19:31:37.953068 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.953070 30336 net.cpp:137] Memory required for data: 1735287296
I1128 19:31:37.953073 30336 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I1128 19:31:37.953078 30336 net.cpp:84] Creating Layer Eltwise4_ReLU9_0_split
I1128 19:31:37.953083 30336 net.cpp:406] Eltwise4_ReLU9_0_split <- Eltwise4
I1128 19:31:37.953088 30336 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I1128 19:31:37.953094 30336 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I1128 19:31:37.953148 30336 net.cpp:122] Setting up Eltwise4_ReLU9_0_split
I1128 19:31:37.953153 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.953172 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.953176 30336 net.cpp:137] Memory required for data: 1773822464
I1128 19:31:37.953179 30336 layer_factory.hpp:77] Creating layer Convolution11
I1128 19:31:37.953187 30336 net.cpp:84] Creating Layer Convolution11
I1128 19:31:37.953191 30336 net.cpp:406] Convolution11 <- Eltwise4_ReLU9_0_split_0
I1128 19:31:37.953203 30336 net.cpp:380] Convolution11 -> Convolution11
I1128 19:31:37.956701 30336 net.cpp:122] Setting up Convolution11
I1128 19:31:37.956712 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.956714 30336 net.cpp:137] Memory required for data: 1793090048
I1128 19:31:37.956735 30336 layer_factory.hpp:77] Creating layer BatchNorm11
I1128 19:31:37.956743 30336 net.cpp:84] Creating Layer BatchNorm11
I1128 19:31:37.956748 30336 net.cpp:406] BatchNorm11 <- Convolution11
I1128 19:31:37.956753 30336 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1128 19:31:37.956913 30336 net.cpp:122] Setting up BatchNorm11
I1128 19:31:37.956918 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.956923 30336 net.cpp:137] Memory required for data: 1812357632
I1128 19:31:37.956944 30336 layer_factory.hpp:77] Creating layer Scale11
I1128 19:31:37.956949 30336 net.cpp:84] Creating Layer Scale11
I1128 19:31:37.956953 30336 net.cpp:406] Scale11 <- Convolution11
I1128 19:31:37.956959 30336 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1128 19:31:37.956996 30336 layer_factory.hpp:77] Creating layer Scale11
I1128 19:31:37.957095 30336 net.cpp:122] Setting up Scale11
I1128 19:31:37.957101 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.957104 30336 net.cpp:137] Memory required for data: 1831625216
I1128 19:31:37.957126 30336 layer_factory.hpp:77] Creating layer ReLU10
I1128 19:31:37.957132 30336 net.cpp:84] Creating Layer ReLU10
I1128 19:31:37.957136 30336 net.cpp:406] ReLU10 <- Convolution11
I1128 19:31:37.957141 30336 net.cpp:367] ReLU10 -> Convolution11 (in-place)
I1128 19:31:37.957265 30336 net.cpp:122] Setting up ReLU10
I1128 19:31:37.957271 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.957274 30336 net.cpp:137] Memory required for data: 1850892800
I1128 19:31:37.957289 30336 layer_factory.hpp:77] Creating layer Convolution12
I1128 19:31:37.957298 30336 net.cpp:84] Creating Layer Convolution12
I1128 19:31:37.957303 30336 net.cpp:406] Convolution12 <- Convolution11
I1128 19:31:37.957307 30336 net.cpp:380] Convolution12 -> Convolution12
I1128 19:31:37.960755 30336 net.cpp:122] Setting up Convolution12
I1128 19:31:37.960769 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.960773 30336 net.cpp:137] Memory required for data: 1870160384
I1128 19:31:37.960794 30336 layer_factory.hpp:77] Creating layer BatchNorm12
I1128 19:31:37.960803 30336 net.cpp:84] Creating Layer BatchNorm12
I1128 19:31:37.960805 30336 net.cpp:406] BatchNorm12 <- Convolution12
I1128 19:31:37.960811 30336 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I1128 19:31:37.960958 30336 net.cpp:122] Setting up BatchNorm12
I1128 19:31:37.960963 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.960966 30336 net.cpp:137] Memory required for data: 1889427968
I1128 19:31:37.960988 30336 layer_factory.hpp:77] Creating layer Scale12
I1128 19:31:37.960994 30336 net.cpp:84] Creating Layer Scale12
I1128 19:31:37.960997 30336 net.cpp:406] Scale12 <- Convolution12
I1128 19:31:37.961001 30336 net.cpp:367] Scale12 -> Convolution12 (in-place)
I1128 19:31:37.961057 30336 layer_factory.hpp:77] Creating layer Scale12
I1128 19:31:37.961171 30336 net.cpp:122] Setting up Scale12
I1128 19:31:37.961176 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.961179 30336 net.cpp:137] Memory required for data: 1908695552
I1128 19:31:37.961200 30336 layer_factory.hpp:77] Creating layer Eltwise5
I1128 19:31:37.961206 30336 net.cpp:84] Creating Layer Eltwise5
I1128 19:31:37.961210 30336 net.cpp:406] Eltwise5 <- Convolution12
I1128 19:31:37.961215 30336 net.cpp:406] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I1128 19:31:37.961220 30336 net.cpp:380] Eltwise5 -> Eltwise5
I1128 19:31:37.961239 30336 net.cpp:122] Setting up Eltwise5
I1128 19:31:37.961257 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.961261 30336 net.cpp:137] Memory required for data: 1927963136
I1128 19:31:37.961280 30336 layer_factory.hpp:77] Creating layer ReLU11
I1128 19:31:37.961294 30336 net.cpp:84] Creating Layer ReLU11
I1128 19:31:37.961298 30336 net.cpp:406] ReLU11 <- Eltwise5
I1128 19:31:37.961316 30336 net.cpp:367] ReLU11 -> Eltwise5 (in-place)
I1128 19:31:37.961462 30336 net.cpp:122] Setting up ReLU11
I1128 19:31:37.961468 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.961472 30336 net.cpp:137] Memory required for data: 1947230720
I1128 19:31:37.961488 30336 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I1128 19:31:37.961495 30336 net.cpp:84] Creating Layer Eltwise5_ReLU11_0_split
I1128 19:31:37.961499 30336 net.cpp:406] Eltwise5_ReLU11_0_split <- Eltwise5
I1128 19:31:37.961504 30336 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I1128 19:31:37.961511 30336 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I1128 19:31:37.961540 30336 net.cpp:122] Setting up Eltwise5_ReLU11_0_split
I1128 19:31:37.961545 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.961562 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.961565 30336 net.cpp:137] Memory required for data: 1985765888
I1128 19:31:37.961570 30336 layer_factory.hpp:77] Creating layer Convolution13
I1128 19:31:37.961591 30336 net.cpp:84] Creating Layer Convolution13
I1128 19:31:37.961593 30336 net.cpp:406] Convolution13 <- Eltwise5_ReLU11_0_split_0
I1128 19:31:37.961598 30336 net.cpp:380] Convolution13 -> Convolution13
I1128 19:31:37.965339 30336 net.cpp:122] Setting up Convolution13
I1128 19:31:37.965354 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.965358 30336 net.cpp:137] Memory required for data: 2005033472
I1128 19:31:37.965364 30336 layer_factory.hpp:77] Creating layer BatchNorm13
I1128 19:31:37.965373 30336 net.cpp:84] Creating Layer BatchNorm13
I1128 19:31:37.965378 30336 net.cpp:406] BatchNorm13 <- Convolution13
I1128 19:31:37.965382 30336 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I1128 19:31:37.965541 30336 net.cpp:122] Setting up BatchNorm13
I1128 19:31:37.965548 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.965550 30336 net.cpp:137] Memory required for data: 2024301056
I1128 19:31:37.965557 30336 layer_factory.hpp:77] Creating layer Scale13
I1128 19:31:37.965564 30336 net.cpp:84] Creating Layer Scale13
I1128 19:31:37.965567 30336 net.cpp:406] Scale13 <- Convolution13
I1128 19:31:37.965574 30336 net.cpp:367] Scale13 -> Convolution13 (in-place)
I1128 19:31:37.965652 30336 layer_factory.hpp:77] Creating layer Scale13
I1128 19:31:37.965767 30336 net.cpp:122] Setting up Scale13
I1128 19:31:37.965772 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.965775 30336 net.cpp:137] Memory required for data: 2043568640
I1128 19:31:37.965780 30336 layer_factory.hpp:77] Creating layer ReLU12
I1128 19:31:37.965786 30336 net.cpp:84] Creating Layer ReLU12
I1128 19:31:37.965790 30336 net.cpp:406] ReLU12 <- Convolution13
I1128 19:31:37.965796 30336 net.cpp:367] ReLU12 -> Convolution13 (in-place)
I1128 19:31:37.966096 30336 net.cpp:122] Setting up ReLU12
I1128 19:31:37.966104 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.966107 30336 net.cpp:137] Memory required for data: 2062836224
I1128 19:31:37.966112 30336 layer_factory.hpp:77] Creating layer Convolution14
I1128 19:31:37.966120 30336 net.cpp:84] Creating Layer Convolution14
I1128 19:31:37.966123 30336 net.cpp:406] Convolution14 <- Convolution13
I1128 19:31:37.966130 30336 net.cpp:380] Convolution14 -> Convolution14
I1128 19:31:37.969571 30336 net.cpp:122] Setting up Convolution14
I1128 19:31:37.969581 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.969584 30336 net.cpp:137] Memory required for data: 2082103808
I1128 19:31:37.969591 30336 layer_factory.hpp:77] Creating layer BatchNorm14
I1128 19:31:37.969610 30336 net.cpp:84] Creating Layer BatchNorm14
I1128 19:31:37.969614 30336 net.cpp:406] BatchNorm14 <- Convolution14
I1128 19:31:37.969619 30336 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I1128 19:31:37.969748 30336 net.cpp:122] Setting up BatchNorm14
I1128 19:31:37.969763 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.969765 30336 net.cpp:137] Memory required for data: 2101371392
I1128 19:31:37.969774 30336 layer_factory.hpp:77] Creating layer Scale14
I1128 19:31:37.969779 30336 net.cpp:84] Creating Layer Scale14
I1128 19:31:37.969784 30336 net.cpp:406] Scale14 <- Convolution14
I1128 19:31:37.969789 30336 net.cpp:367] Scale14 -> Convolution14 (in-place)
I1128 19:31:37.969815 30336 layer_factory.hpp:77] Creating layer Scale14
I1128 19:31:37.969926 30336 net.cpp:122] Setting up Scale14
I1128 19:31:37.969933 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.969935 30336 net.cpp:137] Memory required for data: 2120638976
I1128 19:31:37.969940 30336 layer_factory.hpp:77] Creating layer Eltwise6
I1128 19:31:37.969946 30336 net.cpp:84] Creating Layer Eltwise6
I1128 19:31:37.969951 30336 net.cpp:406] Eltwise6 <- Convolution14
I1128 19:31:37.969956 30336 net.cpp:406] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I1128 19:31:37.969960 30336 net.cpp:380] Eltwise6 -> Eltwise6
I1128 19:31:37.969980 30336 net.cpp:122] Setting up Eltwise6
I1128 19:31:37.969985 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.969990 30336 net.cpp:137] Memory required for data: 2139906560
I1128 19:31:37.969992 30336 layer_factory.hpp:77] Creating layer ReLU13
I1128 19:31:37.969998 30336 net.cpp:84] Creating Layer ReLU13
I1128 19:31:37.970002 30336 net.cpp:406] ReLU13 <- Eltwise6
I1128 19:31:37.970006 30336 net.cpp:367] ReLU13 -> Eltwise6 (in-place)
I1128 19:31:37.970155 30336 net.cpp:122] Setting up ReLU13
I1128 19:31:37.970160 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.970165 30336 net.cpp:137] Memory required for data: 2159174144
I1128 19:31:37.970167 30336 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I1128 19:31:37.970172 30336 net.cpp:84] Creating Layer Eltwise6_ReLU13_0_split
I1128 19:31:37.970176 30336 net.cpp:406] Eltwise6_ReLU13_0_split <- Eltwise6
I1128 19:31:37.970182 30336 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I1128 19:31:37.970190 30336 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I1128 19:31:37.970217 30336 net.cpp:122] Setting up Eltwise6_ReLU13_0_split
I1128 19:31:37.970222 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.970227 30336 net.cpp:129] Top shape: 128 192 14 14 (4816896)
I1128 19:31:37.970230 30336 net.cpp:137] Memory required for data: 2197709312
I1128 19:31:37.970233 30336 layer_factory.hpp:77] Creating layer Convolution15
I1128 19:31:37.970240 30336 net.cpp:84] Creating Layer Convolution15
I1128 19:31:37.970244 30336 net.cpp:406] Convolution15 <- Eltwise6_ReLU13_0_split_0
I1128 19:31:37.970249 30336 net.cpp:380] Convolution15 -> Convolution15
I1128 19:31:37.975666 30336 net.cpp:122] Setting up Convolution15
I1128 19:31:37.975675 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.975678 30336 net.cpp:137] Memory required for data: 2207343104
I1128 19:31:37.975684 30336 layer_factory.hpp:77] Creating layer BatchNorm15
I1128 19:31:37.975692 30336 net.cpp:84] Creating Layer BatchNorm15
I1128 19:31:37.975695 30336 net.cpp:406] BatchNorm15 <- Convolution15
I1128 19:31:37.975702 30336 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I1128 19:31:37.975857 30336 net.cpp:122] Setting up BatchNorm15
I1128 19:31:37.975862 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.975865 30336 net.cpp:137] Memory required for data: 2216976896
I1128 19:31:37.975872 30336 layer_factory.hpp:77] Creating layer Scale15
I1128 19:31:37.975877 30336 net.cpp:84] Creating Layer Scale15
I1128 19:31:37.975880 30336 net.cpp:406] Scale15 <- Convolution15
I1128 19:31:37.975884 30336 net.cpp:367] Scale15 -> Convolution15 (in-place)
I1128 19:31:37.975908 30336 layer_factory.hpp:77] Creating layer Scale15
I1128 19:31:37.975981 30336 net.cpp:122] Setting up Scale15
I1128 19:31:37.975987 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.975991 30336 net.cpp:137] Memory required for data: 2226610688
I1128 19:31:37.976003 30336 layer_factory.hpp:77] Creating layer ReLU14
I1128 19:31:37.976009 30336 net.cpp:84] Creating Layer ReLU14
I1128 19:31:37.976013 30336 net.cpp:406] ReLU14 <- Convolution15
I1128 19:31:37.976018 30336 net.cpp:367] ReLU14 -> Convolution15 (in-place)
I1128 19:31:37.976161 30336 net.cpp:122] Setting up ReLU14
I1128 19:31:37.976167 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.976171 30336 net.cpp:137] Memory required for data: 2236244480
I1128 19:31:37.976173 30336 layer_factory.hpp:77] Creating layer Convolution16
I1128 19:31:37.976181 30336 net.cpp:84] Creating Layer Convolution16
I1128 19:31:37.976184 30336 net.cpp:406] Convolution16 <- Convolution15
I1128 19:31:37.976189 30336 net.cpp:380] Convolution16 -> Convolution16
I1128 19:31:37.986413 30336 net.cpp:122] Setting up Convolution16
I1128 19:31:37.986429 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.986433 30336 net.cpp:137] Memory required for data: 2245878272
I1128 19:31:37.986440 30336 layer_factory.hpp:77] Creating layer BatchNorm16
I1128 19:31:37.986449 30336 net.cpp:84] Creating Layer BatchNorm16
I1128 19:31:37.986454 30336 net.cpp:406] BatchNorm16 <- Convolution16
I1128 19:31:37.986461 30336 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I1128 19:31:37.986591 30336 net.cpp:122] Setting up BatchNorm16
I1128 19:31:37.986596 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.986599 30336 net.cpp:137] Memory required for data: 2255512064
I1128 19:31:37.986608 30336 layer_factory.hpp:77] Creating layer Scale16
I1128 19:31:37.986613 30336 net.cpp:84] Creating Layer Scale16
I1128 19:31:37.986618 30336 net.cpp:406] Scale16 <- Convolution16
I1128 19:31:37.986623 30336 net.cpp:367] Scale16 -> Convolution16 (in-place)
I1128 19:31:37.986647 30336 layer_factory.hpp:77] Creating layer Scale16
I1128 19:31:37.986722 30336 net.cpp:122] Setting up Scale16
I1128 19:31:37.986728 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.986732 30336 net.cpp:137] Memory required for data: 2265145856
I1128 19:31:37.986737 30336 layer_factory.hpp:77] Creating layer Convolution17
I1128 19:31:37.986747 30336 net.cpp:84] Creating Layer Convolution17
I1128 19:31:37.986750 30336 net.cpp:406] Convolution17 <- Eltwise6_ReLU13_0_split_1
I1128 19:31:37.986757 30336 net.cpp:380] Convolution17 -> Convolution17
I1128 19:31:37.987920 30336 net.cpp:122] Setting up Convolution17
I1128 19:31:37.987929 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.987932 30336 net.cpp:137] Memory required for data: 2274779648
I1128 19:31:37.987938 30336 layer_factory.hpp:77] Creating layer BatchNorm17
I1128 19:31:37.987944 30336 net.cpp:84] Creating Layer BatchNorm17
I1128 19:31:37.987949 30336 net.cpp:406] BatchNorm17 <- Convolution17
I1128 19:31:37.987954 30336 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I1128 19:31:37.988080 30336 net.cpp:122] Setting up BatchNorm17
I1128 19:31:37.988085 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.988088 30336 net.cpp:137] Memory required for data: 2284413440
I1128 19:31:37.988095 30336 layer_factory.hpp:77] Creating layer Scale17
I1128 19:31:37.988099 30336 net.cpp:84] Creating Layer Scale17
I1128 19:31:37.988103 30336 net.cpp:406] Scale17 <- Convolution17
I1128 19:31:37.988108 30336 net.cpp:367] Scale17 -> Convolution17 (in-place)
I1128 19:31:37.988131 30336 layer_factory.hpp:77] Creating layer Scale17
I1128 19:31:37.988205 30336 net.cpp:122] Setting up Scale17
I1128 19:31:37.988211 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.988214 30336 net.cpp:137] Memory required for data: 2294047232
I1128 19:31:37.988220 30336 layer_factory.hpp:77] Creating layer Eltwise7
I1128 19:31:37.988225 30336 net.cpp:84] Creating Layer Eltwise7
I1128 19:31:37.988229 30336 net.cpp:406] Eltwise7 <- Convolution16
I1128 19:31:37.988234 30336 net.cpp:406] Eltwise7 <- Convolution17
I1128 19:31:37.988239 30336 net.cpp:380] Eltwise7 -> Eltwise7
I1128 19:31:37.988256 30336 net.cpp:122] Setting up Eltwise7
I1128 19:31:37.988270 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.988274 30336 net.cpp:137] Memory required for data: 2303681024
I1128 19:31:37.988277 30336 layer_factory.hpp:77] Creating layer ReLU15
I1128 19:31:37.988283 30336 net.cpp:84] Creating Layer ReLU15
I1128 19:31:37.988287 30336 net.cpp:406] ReLU15 <- Eltwise7
I1128 19:31:37.988291 30336 net.cpp:367] ReLU15 -> Eltwise7 (in-place)
I1128 19:31:37.988404 30336 net.cpp:122] Setting up ReLU15
I1128 19:31:37.988409 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.988412 30336 net.cpp:137] Memory required for data: 2313314816
I1128 19:31:37.988415 30336 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I1128 19:31:37.988421 30336 net.cpp:84] Creating Layer Eltwise7_ReLU15_0_split
I1128 19:31:37.988440 30336 net.cpp:406] Eltwise7_ReLU15_0_split <- Eltwise7
I1128 19:31:37.988445 30336 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I1128 19:31:37.988451 30336 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I1128 19:31:37.988493 30336 net.cpp:122] Setting up Eltwise7_ReLU15_0_split
I1128 19:31:37.988512 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.988517 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.988520 30336 net.cpp:137] Memory required for data: 2332582400
I1128 19:31:37.988523 30336 layer_factory.hpp:77] Creating layer Convolution18
I1128 19:31:37.988530 30336 net.cpp:84] Creating Layer Convolution18
I1128 19:31:37.988534 30336 net.cpp:406] Convolution18 <- Eltwise7_ReLU15_0_split_0
I1128 19:31:37.988539 30336 net.cpp:380] Convolution18 -> Convolution18
I1128 19:31:37.998911 30336 net.cpp:122] Setting up Convolution18
I1128 19:31:37.998930 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.998934 30336 net.cpp:137] Memory required for data: 2342216192
I1128 19:31:37.998940 30336 layer_factory.hpp:77] Creating layer BatchNorm18
I1128 19:31:37.998948 30336 net.cpp:84] Creating Layer BatchNorm18
I1128 19:31:37.998952 30336 net.cpp:406] BatchNorm18 <- Convolution18
I1128 19:31:37.998960 30336 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I1128 19:31:37.999095 30336 net.cpp:122] Setting up BatchNorm18
I1128 19:31:37.999101 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.999104 30336 net.cpp:137] Memory required for data: 2351849984
I1128 19:31:37.999111 30336 layer_factory.hpp:77] Creating layer Scale18
I1128 19:31:37.999116 30336 net.cpp:84] Creating Layer Scale18
I1128 19:31:37.999121 30336 net.cpp:406] Scale18 <- Convolution18
I1128 19:31:37.999126 30336 net.cpp:367] Scale18 -> Convolution18 (in-place)
I1128 19:31:37.999152 30336 layer_factory.hpp:77] Creating layer Scale18
I1128 19:31:37.999263 30336 net.cpp:122] Setting up Scale18
I1128 19:31:37.999269 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.999271 30336 net.cpp:137] Memory required for data: 2361483776
I1128 19:31:37.999276 30336 layer_factory.hpp:77] Creating layer ReLU16
I1128 19:31:37.999282 30336 net.cpp:84] Creating Layer ReLU16
I1128 19:31:37.999286 30336 net.cpp:406] ReLU16 <- Convolution18
I1128 19:31:37.999290 30336 net.cpp:367] ReLU16 -> Convolution18 (in-place)
I1128 19:31:37.999400 30336 net.cpp:122] Setting up ReLU16
I1128 19:31:37.999406 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:37.999410 30336 net.cpp:137] Memory required for data: 2371117568
I1128 19:31:37.999413 30336 layer_factory.hpp:77] Creating layer Convolution19
I1128 19:31:37.999421 30336 net.cpp:84] Creating Layer Convolution19
I1128 19:31:37.999425 30336 net.cpp:406] Convolution19 <- Convolution18
I1128 19:31:37.999431 30336 net.cpp:380] Convolution19 -> Convolution19
I1128 19:31:38.009527 30336 net.cpp:122] Setting up Convolution19
I1128 19:31:38.009547 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.009551 30336 net.cpp:137] Memory required for data: 2380751360
I1128 19:31:38.009559 30336 layer_factory.hpp:77] Creating layer BatchNorm19
I1128 19:31:38.009567 30336 net.cpp:84] Creating Layer BatchNorm19
I1128 19:31:38.009582 30336 net.cpp:406] BatchNorm19 <- Convolution19
I1128 19:31:38.009588 30336 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I1128 19:31:38.009724 30336 net.cpp:122] Setting up BatchNorm19
I1128 19:31:38.009730 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.009733 30336 net.cpp:137] Memory required for data: 2390385152
I1128 19:31:38.009747 30336 layer_factory.hpp:77] Creating layer Scale19
I1128 19:31:38.009753 30336 net.cpp:84] Creating Layer Scale19
I1128 19:31:38.009757 30336 net.cpp:406] Scale19 <- Convolution19
I1128 19:31:38.009763 30336 net.cpp:367] Scale19 -> Convolution19 (in-place)
I1128 19:31:38.009790 30336 layer_factory.hpp:77] Creating layer Scale19
I1128 19:31:38.009904 30336 net.cpp:122] Setting up Scale19
I1128 19:31:38.009909 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.009912 30336 net.cpp:137] Memory required for data: 2400018944
I1128 19:31:38.009917 30336 layer_factory.hpp:77] Creating layer Eltwise8
I1128 19:31:38.009924 30336 net.cpp:84] Creating Layer Eltwise8
I1128 19:31:38.009928 30336 net.cpp:406] Eltwise8 <- Convolution19
I1128 19:31:38.009932 30336 net.cpp:406] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I1128 19:31:38.009938 30336 net.cpp:380] Eltwise8 -> Eltwise8
I1128 19:31:38.009954 30336 net.cpp:122] Setting up Eltwise8
I1128 19:31:38.009960 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.009963 30336 net.cpp:137] Memory required for data: 2409652736
I1128 19:31:38.009966 30336 layer_factory.hpp:77] Creating layer ReLU17
I1128 19:31:38.009974 30336 net.cpp:84] Creating Layer ReLU17
I1128 19:31:38.009976 30336 net.cpp:406] ReLU17 <- Eltwise8
I1128 19:31:38.009981 30336 net.cpp:367] ReLU17 -> Eltwise8 (in-place)
I1128 19:31:38.010093 30336 net.cpp:122] Setting up ReLU17
I1128 19:31:38.010100 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.010103 30336 net.cpp:137] Memory required for data: 2419286528
I1128 19:31:38.010105 30336 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I1128 19:31:38.010112 30336 net.cpp:84] Creating Layer Eltwise8_ReLU17_0_split
I1128 19:31:38.010114 30336 net.cpp:406] Eltwise8_ReLU17_0_split <- Eltwise8
I1128 19:31:38.010119 30336 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I1128 19:31:38.010126 30336 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I1128 19:31:38.010155 30336 net.cpp:122] Setting up Eltwise8_ReLU17_0_split
I1128 19:31:38.010161 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.010165 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.010169 30336 net.cpp:137] Memory required for data: 2438554112
I1128 19:31:38.010171 30336 layer_factory.hpp:77] Creating layer Convolution20
I1128 19:31:38.010180 30336 net.cpp:84] Creating Layer Convolution20
I1128 19:31:38.010184 30336 net.cpp:406] Convolution20 <- Eltwise8_ReLU17_0_split_0
I1128 19:31:38.010190 30336 net.cpp:380] Convolution20 -> Convolution20
I1128 19:31:38.020220 30336 net.cpp:122] Setting up Convolution20
I1128 19:31:38.020234 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.020237 30336 net.cpp:137] Memory required for data: 2448187904
I1128 19:31:38.020258 30336 layer_factory.hpp:77] Creating layer BatchNorm20
I1128 19:31:38.020267 30336 net.cpp:84] Creating Layer BatchNorm20
I1128 19:31:38.020272 30336 net.cpp:406] BatchNorm20 <- Convolution20
I1128 19:31:38.020278 30336 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I1128 19:31:38.020422 30336 net.cpp:122] Setting up BatchNorm20
I1128 19:31:38.020428 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.020431 30336 net.cpp:137] Memory required for data: 2457821696
I1128 19:31:38.020453 30336 layer_factory.hpp:77] Creating layer Scale20
I1128 19:31:38.020459 30336 net.cpp:84] Creating Layer Scale20
I1128 19:31:38.020463 30336 net.cpp:406] Scale20 <- Convolution20
I1128 19:31:38.020467 30336 net.cpp:367] Scale20 -> Convolution20 (in-place)
I1128 19:31:38.020519 30336 layer_factory.hpp:77] Creating layer Scale20
I1128 19:31:38.020619 30336 net.cpp:122] Setting up Scale20
I1128 19:31:38.020625 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.020628 30336 net.cpp:137] Memory required for data: 2467455488
I1128 19:31:38.020634 30336 layer_factory.hpp:77] Creating layer ReLU18
I1128 19:31:38.020651 30336 net.cpp:84] Creating Layer ReLU18
I1128 19:31:38.020656 30336 net.cpp:406] ReLU18 <- Convolution20
I1128 19:31:38.020674 30336 net.cpp:367] ReLU18 -> Convolution20 (in-place)
I1128 19:31:38.020998 30336 net.cpp:122] Setting up ReLU18
I1128 19:31:38.021006 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.021009 30336 net.cpp:137] Memory required for data: 2477089280
I1128 19:31:38.021026 30336 layer_factory.hpp:77] Creating layer Convolution21
I1128 19:31:38.021034 30336 net.cpp:84] Creating Layer Convolution21
I1128 19:31:38.021039 30336 net.cpp:406] Convolution21 <- Convolution20
I1128 19:31:38.021044 30336 net.cpp:380] Convolution21 -> Convolution21
I1128 19:31:38.031268 30336 net.cpp:122] Setting up Convolution21
I1128 19:31:38.031287 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.031291 30336 net.cpp:137] Memory required for data: 2486723072
I1128 19:31:38.031312 30336 layer_factory.hpp:77] Creating layer BatchNorm21
I1128 19:31:38.031322 30336 net.cpp:84] Creating Layer BatchNorm21
I1128 19:31:38.031324 30336 net.cpp:406] BatchNorm21 <- Convolution21
I1128 19:31:38.031332 30336 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I1128 19:31:38.031466 30336 net.cpp:122] Setting up BatchNorm21
I1128 19:31:38.031471 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.031473 30336 net.cpp:137] Memory required for data: 2496356864
I1128 19:31:38.031481 30336 layer_factory.hpp:77] Creating layer Scale21
I1128 19:31:38.031487 30336 net.cpp:84] Creating Layer Scale21
I1128 19:31:38.031491 30336 net.cpp:406] Scale21 <- Convolution21
I1128 19:31:38.031496 30336 net.cpp:367] Scale21 -> Convolution21 (in-place)
I1128 19:31:38.031520 30336 layer_factory.hpp:77] Creating layer Scale21
I1128 19:31:38.031599 30336 net.cpp:122] Setting up Scale21
I1128 19:31:38.031605 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.031607 30336 net.cpp:137] Memory required for data: 2505990656
I1128 19:31:38.031613 30336 layer_factory.hpp:77] Creating layer Eltwise9
I1128 19:31:38.031653 30336 net.cpp:84] Creating Layer Eltwise9
I1128 19:31:38.031657 30336 net.cpp:406] Eltwise9 <- Convolution21
I1128 19:31:38.031662 30336 net.cpp:406] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I1128 19:31:38.031668 30336 net.cpp:380] Eltwise9 -> Eltwise9
I1128 19:31:38.031699 30336 net.cpp:122] Setting up Eltwise9
I1128 19:31:38.031704 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.031708 30336 net.cpp:137] Memory required for data: 2515624448
I1128 19:31:38.031710 30336 layer_factory.hpp:77] Creating layer ReLU19
I1128 19:31:38.031729 30336 net.cpp:84] Creating Layer ReLU19
I1128 19:31:38.031733 30336 net.cpp:406] ReLU19 <- Eltwise9
I1128 19:31:38.031738 30336 net.cpp:367] ReLU19 -> Eltwise9 (in-place)
I1128 19:31:38.032130 30336 net.cpp:122] Setting up ReLU19
I1128 19:31:38.032137 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.032141 30336 net.cpp:137] Memory required for data: 2525258240
I1128 19:31:38.032160 30336 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
I1128 19:31:38.032165 30336 net.cpp:84] Creating Layer Eltwise9_ReLU19_0_split
I1128 19:31:38.032168 30336 net.cpp:406] Eltwise9_ReLU19_0_split <- Eltwise9
I1128 19:31:38.032176 30336 net.cpp:380] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
I1128 19:31:38.032182 30336 net.cpp:380] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
I1128 19:31:38.032240 30336 net.cpp:122] Setting up Eltwise9_ReLU19_0_split
I1128 19:31:38.032245 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.032251 30336 net.cpp:129] Top shape: 128 384 7 7 (2408448)
I1128 19:31:38.032254 30336 net.cpp:137] Memory required for data: 2544525824
I1128 19:31:38.032270 30336 layer_factory.hpp:77] Creating layer Convolution22
I1128 19:31:38.032305 30336 net.cpp:84] Creating Layer Convolution22
I1128 19:31:38.032308 30336 net.cpp:406] Convolution22 <- Eltwise9_ReLU19_0_split_0
I1128 19:31:38.032315 30336 net.cpp:380] Convolution22 -> Convolution22
I1128 19:31:38.042516 30336 net.cpp:122] Setting up Convolution22
I1128 19:31:38.042531 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.042534 30336 net.cpp:137] Memory required for data: 2547671552
I1128 19:31:38.042541 30336 layer_factory.hpp:77] Creating layer BatchNorm22
I1128 19:31:38.042549 30336 net.cpp:84] Creating Layer BatchNorm22
I1128 19:31:38.042552 30336 net.cpp:406] BatchNorm22 <- Convolution22
I1128 19:31:38.042559 30336 net.cpp:367] BatchNorm22 -> Convolution22 (in-place)
I1128 19:31:38.042728 30336 net.cpp:122] Setting up BatchNorm22
I1128 19:31:38.042734 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.042737 30336 net.cpp:137] Memory required for data: 2550817280
I1128 19:31:38.042744 30336 layer_factory.hpp:77] Creating layer Scale22
I1128 19:31:38.042750 30336 net.cpp:84] Creating Layer Scale22
I1128 19:31:38.042753 30336 net.cpp:406] Scale22 <- Convolution22
I1128 19:31:38.042758 30336 net.cpp:367] Scale22 -> Convolution22 (in-place)
I1128 19:31:38.042784 30336 layer_factory.hpp:77] Creating layer Scale22
I1128 19:31:38.042863 30336 net.cpp:122] Setting up Scale22
I1128 19:31:38.042870 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.042872 30336 net.cpp:137] Memory required for data: 2553963008
I1128 19:31:38.042877 30336 layer_factory.hpp:77] Creating layer ReLU20
I1128 19:31:38.042883 30336 net.cpp:84] Creating Layer ReLU20
I1128 19:31:38.042887 30336 net.cpp:406] ReLU20 <- Convolution22
I1128 19:31:38.042891 30336 net.cpp:367] ReLU20 -> Convolution22 (in-place)
I1128 19:31:38.043005 30336 net.cpp:122] Setting up ReLU20
I1128 19:31:38.043011 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.043015 30336 net.cpp:137] Memory required for data: 2557108736
I1128 19:31:38.043017 30336 layer_factory.hpp:77] Creating layer Convolution23
I1128 19:31:38.043026 30336 net.cpp:84] Creating Layer Convolution23
I1128 19:31:38.043030 30336 net.cpp:406] Convolution23 <- Convolution22
I1128 19:31:38.043035 30336 net.cpp:380] Convolution23 -> Convolution23
I1128 19:31:38.053112 30336 net.cpp:122] Setting up Convolution23
I1128 19:31:38.053128 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.053131 30336 net.cpp:137] Memory required for data: 2560254464
I1128 19:31:38.053139 30336 layer_factory.hpp:77] Creating layer BatchNorm23
I1128 19:31:38.053148 30336 net.cpp:84] Creating Layer BatchNorm23
I1128 19:31:38.053153 30336 net.cpp:406] BatchNorm23 <- Convolution23
I1128 19:31:38.053160 30336 net.cpp:367] BatchNorm23 -> Convolution23 (in-place)
I1128 19:31:38.053311 30336 net.cpp:122] Setting up BatchNorm23
I1128 19:31:38.053318 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.053320 30336 net.cpp:137] Memory required for data: 2563400192
I1128 19:31:38.053341 30336 layer_factory.hpp:77] Creating layer Scale23
I1128 19:31:38.053349 30336 net.cpp:84] Creating Layer Scale23
I1128 19:31:38.053352 30336 net.cpp:406] Scale23 <- Convolution23
I1128 19:31:38.053357 30336 net.cpp:367] Scale23 -> Convolution23 (in-place)
I1128 19:31:38.053385 30336 layer_factory.hpp:77] Creating layer Scale23
I1128 19:31:38.053468 30336 net.cpp:122] Setting up Scale23
I1128 19:31:38.053486 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.053489 30336 net.cpp:137] Memory required for data: 2566545920
I1128 19:31:38.053510 30336 layer_factory.hpp:77] Creating layer Convolution24
I1128 19:31:38.053520 30336 net.cpp:84] Creating Layer Convolution24
I1128 19:31:38.053524 30336 net.cpp:406] Convolution24 <- Eltwise9_ReLU19_0_split_1
I1128 19:31:38.053532 30336 net.cpp:380] Convolution24 -> Convolution24
I1128 19:31:38.055246 30336 net.cpp:122] Setting up Convolution24
I1128 19:31:38.055255 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.055258 30336 net.cpp:137] Memory required for data: 2569691648
I1128 19:31:38.055277 30336 layer_factory.hpp:77] Creating layer BatchNorm24
I1128 19:31:38.055285 30336 net.cpp:84] Creating Layer BatchNorm24
I1128 19:31:38.055289 30336 net.cpp:406] BatchNorm24 <- Convolution24
I1128 19:31:38.055295 30336 net.cpp:367] BatchNorm24 -> Convolution24 (in-place)
I1128 19:31:38.055433 30336 net.cpp:122] Setting up BatchNorm24
I1128 19:31:38.055438 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.055441 30336 net.cpp:137] Memory required for data: 2572837376
I1128 19:31:38.055447 30336 layer_factory.hpp:77] Creating layer Scale24
I1128 19:31:38.055454 30336 net.cpp:84] Creating Layer Scale24
I1128 19:31:38.055459 30336 net.cpp:406] Scale24 <- Convolution24
I1128 19:31:38.055462 30336 net.cpp:367] Scale24 -> Convolution24 (in-place)
I1128 19:31:38.055487 30336 layer_factory.hpp:77] Creating layer Scale24
I1128 19:31:38.055569 30336 net.cpp:122] Setting up Scale24
I1128 19:31:38.055575 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.055578 30336 net.cpp:137] Memory required for data: 2575983104
I1128 19:31:38.055583 30336 layer_factory.hpp:77] Creating layer Eltwise10
I1128 19:31:38.055604 30336 net.cpp:84] Creating Layer Eltwise10
I1128 19:31:38.055609 30336 net.cpp:406] Eltwise10 <- Convolution23
I1128 19:31:38.055613 30336 net.cpp:406] Eltwise10 <- Convolution24
I1128 19:31:38.055634 30336 net.cpp:380] Eltwise10 -> Eltwise10
I1128 19:31:38.055652 30336 net.cpp:122] Setting up Eltwise10
I1128 19:31:38.055657 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.055660 30336 net.cpp:137] Memory required for data: 2579128832
I1128 19:31:38.055677 30336 layer_factory.hpp:77] Creating layer ReLU21
I1128 19:31:38.055685 30336 net.cpp:84] Creating Layer ReLU21
I1128 19:31:38.055703 30336 net.cpp:406] ReLU21 <- Eltwise10
I1128 19:31:38.055708 30336 net.cpp:367] ReLU21 -> Eltwise10 (in-place)
I1128 19:31:38.055826 30336 net.cpp:122] Setting up ReLU21
I1128 19:31:38.055833 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.055836 30336 net.cpp:137] Memory required for data: 2582274560
I1128 19:31:38.055840 30336 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
I1128 19:31:38.055845 30336 net.cpp:84] Creating Layer Eltwise10_ReLU21_0_split
I1128 19:31:38.055850 30336 net.cpp:406] Eltwise10_ReLU21_0_split <- Eltwise10
I1128 19:31:38.055855 30336 net.cpp:380] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
I1128 19:31:38.055863 30336 net.cpp:380] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
I1128 19:31:38.055907 30336 net.cpp:122] Setting up Eltwise10_ReLU21_0_split
I1128 19:31:38.055913 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.055917 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.055922 30336 net.cpp:137] Memory required for data: 2588566016
I1128 19:31:38.055924 30336 layer_factory.hpp:77] Creating layer Convolution25
I1128 19:31:38.055948 30336 net.cpp:84] Creating Layer Convolution25
I1128 19:31:38.055950 30336 net.cpp:406] Convolution25 <- Eltwise10_ReLU21_0_split_0
I1128 19:31:38.055958 30336 net.cpp:380] Convolution25 -> Convolution25
I1128 19:31:38.066347 30336 net.cpp:122] Setting up Convolution25
I1128 19:31:38.066365 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.066370 30336 net.cpp:137] Memory required for data: 2591711744
I1128 19:31:38.066376 30336 layer_factory.hpp:77] Creating layer BatchNorm25
I1128 19:31:38.066385 30336 net.cpp:84] Creating Layer BatchNorm25
I1128 19:31:38.066388 30336 net.cpp:406] BatchNorm25 <- Convolution25
I1128 19:31:38.066396 30336 net.cpp:367] BatchNorm25 -> Convolution25 (in-place)
I1128 19:31:38.066540 30336 net.cpp:122] Setting up BatchNorm25
I1128 19:31:38.066546 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.066548 30336 net.cpp:137] Memory required for data: 2594857472
I1128 19:31:38.066557 30336 layer_factory.hpp:77] Creating layer Scale25
I1128 19:31:38.066565 30336 net.cpp:84] Creating Layer Scale25
I1128 19:31:38.066568 30336 net.cpp:406] Scale25 <- Convolution25
I1128 19:31:38.066582 30336 net.cpp:367] Scale25 -> Convolution25 (in-place)
I1128 19:31:38.066609 30336 layer_factory.hpp:77] Creating layer Scale25
I1128 19:31:38.066694 30336 net.cpp:122] Setting up Scale25
I1128 19:31:38.066700 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.066704 30336 net.cpp:137] Memory required for data: 2598003200
I1128 19:31:38.066710 30336 layer_factory.hpp:77] Creating layer ReLU22
I1128 19:31:38.066716 30336 net.cpp:84] Creating Layer ReLU22
I1128 19:31:38.066720 30336 net.cpp:406] ReLU22 <- Convolution25
I1128 19:31:38.066725 30336 net.cpp:367] ReLU22 -> Convolution25 (in-place)
I1128 19:31:38.066843 30336 net.cpp:122] Setting up ReLU22
I1128 19:31:38.066849 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.066853 30336 net.cpp:137] Memory required for data: 2601148928
I1128 19:31:38.066856 30336 layer_factory.hpp:77] Creating layer Convolution26
I1128 19:31:38.066866 30336 net.cpp:84] Creating Layer Convolution26
I1128 19:31:38.066869 30336 net.cpp:406] Convolution26 <- Convolution25
I1128 19:31:38.066875 30336 net.cpp:380] Convolution26 -> Convolution26
I1128 19:31:38.077011 30336 net.cpp:122] Setting up Convolution26
I1128 19:31:38.077025 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.077028 30336 net.cpp:137] Memory required for data: 2604294656
I1128 19:31:38.077049 30336 layer_factory.hpp:77] Creating layer BatchNorm26
I1128 19:31:38.077060 30336 net.cpp:84] Creating Layer BatchNorm26
I1128 19:31:38.077065 30336 net.cpp:406] BatchNorm26 <- Convolution26
I1128 19:31:38.077071 30336 net.cpp:367] BatchNorm26 -> Convolution26 (in-place)
I1128 19:31:38.077229 30336 net.cpp:122] Setting up BatchNorm26
I1128 19:31:38.077235 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.077239 30336 net.cpp:137] Memory required for data: 2607440384
I1128 19:31:38.077261 30336 layer_factory.hpp:77] Creating layer Scale26
I1128 19:31:38.077267 30336 net.cpp:84] Creating Layer Scale26
I1128 19:31:38.077272 30336 net.cpp:406] Scale26 <- Convolution26
I1128 19:31:38.077294 30336 net.cpp:367] Scale26 -> Convolution26 (in-place)
I1128 19:31:38.077363 30336 layer_factory.hpp:77] Creating layer Scale26
I1128 19:31:38.077487 30336 net.cpp:122] Setting up Scale26
I1128 19:31:38.077492 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.077497 30336 net.cpp:137] Memory required for data: 2610586112
I1128 19:31:38.077517 30336 layer_factory.hpp:77] Creating layer Eltwise11
I1128 19:31:38.077522 30336 net.cpp:84] Creating Layer Eltwise11
I1128 19:31:38.077527 30336 net.cpp:406] Eltwise11 <- Convolution26
I1128 19:31:38.077533 30336 net.cpp:406] Eltwise11 <- Eltwise10_ReLU21_0_split_1
I1128 19:31:38.077555 30336 net.cpp:380] Eltwise11 -> Eltwise11
I1128 19:31:38.077587 30336 net.cpp:122] Setting up Eltwise11
I1128 19:31:38.077592 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.077596 30336 net.cpp:137] Memory required for data: 2613731840
I1128 19:31:38.077599 30336 layer_factory.hpp:77] Creating layer ReLU23
I1128 19:31:38.077626 30336 net.cpp:84] Creating Layer ReLU23
I1128 19:31:38.077642 30336 net.cpp:406] ReLU23 <- Eltwise11
I1128 19:31:38.077649 30336 net.cpp:367] ReLU23 -> Eltwise11 (in-place)
I1128 19:31:38.077798 30336 net.cpp:122] Setting up ReLU23
I1128 19:31:38.077805 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.077808 30336 net.cpp:137] Memory required for data: 2616877568
I1128 19:31:38.077812 30336 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
I1128 19:31:38.077817 30336 net.cpp:84] Creating Layer Eltwise11_ReLU23_0_split
I1128 19:31:38.077821 30336 net.cpp:406] Eltwise11_ReLU23_0_split <- Eltwise11
I1128 19:31:38.077826 30336 net.cpp:380] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
I1128 19:31:38.077834 30336 net.cpp:380] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
I1128 19:31:38.077867 30336 net.cpp:122] Setting up Eltwise11_ReLU23_0_split
I1128 19:31:38.077872 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.077886 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.077890 30336 net.cpp:137] Memory required for data: 2623169024
I1128 19:31:38.077893 30336 layer_factory.hpp:77] Creating layer Convolution27
I1128 19:31:38.077903 30336 net.cpp:84] Creating Layer Convolution27
I1128 19:31:38.077906 30336 net.cpp:406] Convolution27 <- Eltwise11_ReLU23_0_split_0
I1128 19:31:38.077913 30336 net.cpp:380] Convolution27 -> Convolution27
I1128 19:31:38.088069 30336 net.cpp:122] Setting up Convolution27
I1128 19:31:38.088085 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.088088 30336 net.cpp:137] Memory required for data: 2626314752
I1128 19:31:38.088096 30336 layer_factory.hpp:77] Creating layer BatchNorm27
I1128 19:31:38.088106 30336 net.cpp:84] Creating Layer BatchNorm27
I1128 19:31:38.088110 30336 net.cpp:406] BatchNorm27 <- Convolution27
I1128 19:31:38.088119 30336 net.cpp:367] BatchNorm27 -> Convolution27 (in-place)
I1128 19:31:38.088261 30336 net.cpp:122] Setting up BatchNorm27
I1128 19:31:38.088268 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.088270 30336 net.cpp:137] Memory required for data: 2629460480
I1128 19:31:38.088276 30336 layer_factory.hpp:77] Creating layer Scale27
I1128 19:31:38.088284 30336 net.cpp:84] Creating Layer Scale27
I1128 19:31:38.088287 30336 net.cpp:406] Scale27 <- Convolution27
I1128 19:31:38.088292 30336 net.cpp:367] Scale27 -> Convolution27 (in-place)
I1128 19:31:38.088318 30336 layer_factory.hpp:77] Creating layer Scale27
I1128 19:31:38.088403 30336 net.cpp:122] Setting up Scale27
I1128 19:31:38.088409 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.088413 30336 net.cpp:137] Memory required for data: 2632606208
I1128 19:31:38.088418 30336 layer_factory.hpp:77] Creating layer ReLU24
I1128 19:31:38.088433 30336 net.cpp:84] Creating Layer ReLU24
I1128 19:31:38.088436 30336 net.cpp:406] ReLU24 <- Convolution27
I1128 19:31:38.088441 30336 net.cpp:367] ReLU24 -> Convolution27 (in-place)
I1128 19:31:38.088560 30336 net.cpp:122] Setting up ReLU24
I1128 19:31:38.088567 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.088569 30336 net.cpp:137] Memory required for data: 2635751936
I1128 19:31:38.088573 30336 layer_factory.hpp:77] Creating layer Convolution28
I1128 19:31:38.088582 30336 net.cpp:84] Creating Layer Convolution28
I1128 19:31:38.088585 30336 net.cpp:406] Convolution28 <- Convolution27
I1128 19:31:38.088591 30336 net.cpp:380] Convolution28 -> Convolution28
I1128 19:31:38.099031 30336 net.cpp:122] Setting up Convolution28
I1128 19:31:38.099051 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.099054 30336 net.cpp:137] Memory required for data: 2638897664
I1128 19:31:38.099061 30336 layer_factory.hpp:77] Creating layer BatchNorm28
I1128 19:31:38.099069 30336 net.cpp:84] Creating Layer BatchNorm28
I1128 19:31:38.099073 30336 net.cpp:406] BatchNorm28 <- Convolution28
I1128 19:31:38.099081 30336 net.cpp:367] BatchNorm28 -> Convolution28 (in-place)
I1128 19:31:38.099226 30336 net.cpp:122] Setting up BatchNorm28
I1128 19:31:38.099233 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.099236 30336 net.cpp:137] Memory required for data: 2642043392
I1128 19:31:38.099244 30336 layer_factory.hpp:77] Creating layer Scale28
I1128 19:31:38.099251 30336 net.cpp:84] Creating Layer Scale28
I1128 19:31:38.099254 30336 net.cpp:406] Scale28 <- Convolution28
I1128 19:31:38.099259 30336 net.cpp:367] Scale28 -> Convolution28 (in-place)
I1128 19:31:38.099287 30336 layer_factory.hpp:77] Creating layer Scale28
I1128 19:31:38.099406 30336 net.cpp:122] Setting up Scale28
I1128 19:31:38.099412 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.099416 30336 net.cpp:137] Memory required for data: 2645189120
I1128 19:31:38.099421 30336 layer_factory.hpp:77] Creating layer Eltwise12
I1128 19:31:38.099427 30336 net.cpp:84] Creating Layer Eltwise12
I1128 19:31:38.099431 30336 net.cpp:406] Eltwise12 <- Convolution28
I1128 19:31:38.099437 30336 net.cpp:406] Eltwise12 <- Eltwise11_ReLU23_0_split_1
I1128 19:31:38.099455 30336 net.cpp:380] Eltwise12 -> Eltwise12
I1128 19:31:38.099473 30336 net.cpp:122] Setting up Eltwise12
I1128 19:31:38.099480 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.099484 30336 net.cpp:137] Memory required for data: 2648334848
I1128 19:31:38.099488 30336 layer_factory.hpp:77] Creating layer ReLU25
I1128 19:31:38.099493 30336 net.cpp:84] Creating Layer ReLU25
I1128 19:31:38.099496 30336 net.cpp:406] ReLU25 <- Eltwise12
I1128 19:31:38.099503 30336 net.cpp:367] ReLU25 -> Eltwise12 (in-place)
I1128 19:31:38.099844 30336 net.cpp:122] Setting up ReLU25
I1128 19:31:38.099853 30336 net.cpp:129] Top shape: 128 384 4 4 (786432)
I1128 19:31:38.099856 30336 net.cpp:137] Memory required for data: 2651480576
I1128 19:31:38.099859 30336 layer_factory.hpp:77] Creating layer Pooling1
I1128 19:31:38.099869 30336 net.cpp:84] Creating Layer Pooling1
I1128 19:31:38.099872 30336 net.cpp:406] Pooling1 <- Eltwise12
I1128 19:31:38.099877 30336 net.cpp:380] Pooling1 -> Pooling1
I1128 19:31:38.100021 30336 net.cpp:122] Setting up Pooling1
I1128 19:31:38.100028 30336 net.cpp:129] Top shape: 128 384 1 1 (49152)
I1128 19:31:38.100031 30336 net.cpp:137] Memory required for data: 2651677184
I1128 19:31:38.100034 30336 layer_factory.hpp:77] Creating layer InnerProduct1
I1128 19:31:38.100041 30336 net.cpp:84] Creating Layer InnerProduct1
I1128 19:31:38.100045 30336 net.cpp:406] InnerProduct1 <- Pooling1
I1128 19:31:38.100050 30336 net.cpp:380] InnerProduct1 -> InnerProduct1
I1128 19:31:38.100169 30336 net.cpp:122] Setting up InnerProduct1
I1128 19:31:38.100175 30336 net.cpp:129] Top shape: 128 10 (1280)
I1128 19:31:38.100178 30336 net.cpp:137] Memory required for data: 2651682304
I1128 19:31:38.100183 30336 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1128 19:31:38.100190 30336 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1128 19:31:38.100194 30336 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1
I1128 19:31:38.100200 30336 net.cpp:406] SoftmaxWithLoss1 <- Data2
I1128 19:31:38.100206 30336 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1128 19:31:38.100216 30336 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1128 19:31:38.100395 30336 net.cpp:122] Setting up SoftmaxWithLoss1
I1128 19:31:38.100401 30336 net.cpp:129] Top shape: (1)
I1128 19:31:38.100404 30336 net.cpp:132]     with loss weight 1
I1128 19:31:38.100417 30336 net.cpp:137] Memory required for data: 2651682308
I1128 19:31:38.100421 30336 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1128 19:31:38.100428 30336 net.cpp:198] InnerProduct1 needs backward computation.
I1128 19:31:38.100432 30336 net.cpp:198] Pooling1 needs backward computation.
I1128 19:31:38.100436 30336 net.cpp:198] ReLU25 needs backward computation.
I1128 19:31:38.100438 30336 net.cpp:198] Eltwise12 needs backward computation.
I1128 19:31:38.100442 30336 net.cpp:198] Scale28 needs backward computation.
I1128 19:31:38.100445 30336 net.cpp:198] BatchNorm28 needs backward computation.
I1128 19:31:38.100448 30336 net.cpp:198] Convolution28 needs backward computation.
I1128 19:31:38.100451 30336 net.cpp:198] ReLU24 needs backward computation.
I1128 19:31:38.100455 30336 net.cpp:198] Scale27 needs backward computation.
I1128 19:31:38.100458 30336 net.cpp:198] BatchNorm27 needs backward computation.
I1128 19:31:38.100462 30336 net.cpp:198] Convolution27 needs backward computation.
I1128 19:31:38.100466 30336 net.cpp:198] Eltwise11_ReLU23_0_split needs backward computation.
I1128 19:31:38.100471 30336 net.cpp:198] ReLU23 needs backward computation.
I1128 19:31:38.100474 30336 net.cpp:198] Eltwise11 needs backward computation.
I1128 19:31:38.100478 30336 net.cpp:198] Scale26 needs backward computation.
I1128 19:31:38.100481 30336 net.cpp:198] BatchNorm26 needs backward computation.
I1128 19:31:38.100486 30336 net.cpp:198] Convolution26 needs backward computation.
I1128 19:31:38.100488 30336 net.cpp:198] ReLU22 needs backward computation.
I1128 19:31:38.100514 30336 net.cpp:198] Scale25 needs backward computation.
I1128 19:31:38.100517 30336 net.cpp:198] BatchNorm25 needs backward computation.
I1128 19:31:38.100539 30336 net.cpp:198] Convolution25 needs backward computation.
I1128 19:31:38.100543 30336 net.cpp:198] Eltwise10_ReLU21_0_split needs backward computation.
I1128 19:31:38.100548 30336 net.cpp:198] ReLU21 needs backward computation.
I1128 19:31:38.100553 30336 net.cpp:198] Eltwise10 needs backward computation.
I1128 19:31:38.100556 30336 net.cpp:198] Scale24 needs backward computation.
I1128 19:31:38.100574 30336 net.cpp:198] BatchNorm24 needs backward computation.
I1128 19:31:38.100577 30336 net.cpp:198] Convolution24 needs backward computation.
I1128 19:31:38.100580 30336 net.cpp:198] Scale23 needs backward computation.
I1128 19:31:38.100584 30336 net.cpp:198] BatchNorm23 needs backward computation.
I1128 19:31:38.100587 30336 net.cpp:198] Convolution23 needs backward computation.
I1128 19:31:38.100590 30336 net.cpp:198] ReLU20 needs backward computation.
I1128 19:31:38.100594 30336 net.cpp:198] Scale22 needs backward computation.
I1128 19:31:38.100597 30336 net.cpp:198] BatchNorm22 needs backward computation.
I1128 19:31:38.100600 30336 net.cpp:198] Convolution22 needs backward computation.
I1128 19:31:38.100603 30336 net.cpp:198] Eltwise9_ReLU19_0_split needs backward computation.
I1128 19:31:38.100606 30336 net.cpp:198] ReLU19 needs backward computation.
I1128 19:31:38.100610 30336 net.cpp:198] Eltwise9 needs backward computation.
I1128 19:31:38.100615 30336 net.cpp:198] Scale21 needs backward computation.
I1128 19:31:38.100617 30336 net.cpp:198] BatchNorm21 needs backward computation.
I1128 19:31:38.100620 30336 net.cpp:198] Convolution21 needs backward computation.
I1128 19:31:38.100623 30336 net.cpp:198] ReLU18 needs backward computation.
I1128 19:31:38.100626 30336 net.cpp:198] Scale20 needs backward computation.
I1128 19:31:38.100630 30336 net.cpp:198] BatchNorm20 needs backward computation.
I1128 19:31:38.100631 30336 net.cpp:198] Convolution20 needs backward computation.
I1128 19:31:38.100636 30336 net.cpp:198] Eltwise8_ReLU17_0_split needs backward computation.
I1128 19:31:38.100638 30336 net.cpp:198] ReLU17 needs backward computation.
I1128 19:31:38.100642 30336 net.cpp:198] Eltwise8 needs backward computation.
I1128 19:31:38.100646 30336 net.cpp:198] Scale19 needs backward computation.
I1128 19:31:38.100648 30336 net.cpp:198] BatchNorm19 needs backward computation.
I1128 19:31:38.100651 30336 net.cpp:198] Convolution19 needs backward computation.
I1128 19:31:38.100654 30336 net.cpp:198] ReLU16 needs backward computation.
I1128 19:31:38.100657 30336 net.cpp:198] Scale18 needs backward computation.
I1128 19:31:38.100661 30336 net.cpp:198] BatchNorm18 needs backward computation.
I1128 19:31:38.100663 30336 net.cpp:198] Convolution18 needs backward computation.
I1128 19:31:38.100666 30336 net.cpp:198] Eltwise7_ReLU15_0_split needs backward computation.
I1128 19:31:38.100670 30336 net.cpp:198] ReLU15 needs backward computation.
I1128 19:31:38.100673 30336 net.cpp:198] Eltwise7 needs backward computation.
I1128 19:31:38.100677 30336 net.cpp:198] Scale17 needs backward computation.
I1128 19:31:38.100679 30336 net.cpp:198] BatchNorm17 needs backward computation.
I1128 19:31:38.100682 30336 net.cpp:198] Convolution17 needs backward computation.
I1128 19:31:38.100687 30336 net.cpp:198] Scale16 needs backward computation.
I1128 19:31:38.100689 30336 net.cpp:198] BatchNorm16 needs backward computation.
I1128 19:31:38.100692 30336 net.cpp:198] Convolution16 needs backward computation.
I1128 19:31:38.100694 30336 net.cpp:198] ReLU14 needs backward computation.
I1128 19:31:38.100698 30336 net.cpp:198] Scale15 needs backward computation.
I1128 19:31:38.100700 30336 net.cpp:198] BatchNorm15 needs backward computation.
I1128 19:31:38.100703 30336 net.cpp:198] Convolution15 needs backward computation.
I1128 19:31:38.100706 30336 net.cpp:198] Eltwise6_ReLU13_0_split needs backward computation.
I1128 19:31:38.100710 30336 net.cpp:198] ReLU13 needs backward computation.
I1128 19:31:38.100713 30336 net.cpp:198] Eltwise6 needs backward computation.
I1128 19:31:38.100718 30336 net.cpp:198] Scale14 needs backward computation.
I1128 19:31:38.100724 30336 net.cpp:198] BatchNorm14 needs backward computation.
I1128 19:31:38.100728 30336 net.cpp:198] Convolution14 needs backward computation.
I1128 19:31:38.100731 30336 net.cpp:198] ReLU12 needs backward computation.
I1128 19:31:38.100734 30336 net.cpp:198] Scale13 needs backward computation.
I1128 19:31:38.100738 30336 net.cpp:198] BatchNorm13 needs backward computation.
I1128 19:31:38.100740 30336 net.cpp:198] Convolution13 needs backward computation.
I1128 19:31:38.100744 30336 net.cpp:198] Eltwise5_ReLU11_0_split needs backward computation.
I1128 19:31:38.100747 30336 net.cpp:198] ReLU11 needs backward computation.
I1128 19:31:38.100750 30336 net.cpp:198] Eltwise5 needs backward computation.
I1128 19:31:38.100754 30336 net.cpp:198] Scale12 needs backward computation.
I1128 19:31:38.100756 30336 net.cpp:198] BatchNorm12 needs backward computation.
I1128 19:31:38.100759 30336 net.cpp:198] Convolution12 needs backward computation.
I1128 19:31:38.100762 30336 net.cpp:198] ReLU10 needs backward computation.
I1128 19:31:38.100766 30336 net.cpp:198] Scale11 needs backward computation.
I1128 19:31:38.100769 30336 net.cpp:198] BatchNorm11 needs backward computation.
I1128 19:31:38.100771 30336 net.cpp:198] Convolution11 needs backward computation.
I1128 19:31:38.100775 30336 net.cpp:198] Eltwise4_ReLU9_0_split needs backward computation.
I1128 19:31:38.100778 30336 net.cpp:198] ReLU9 needs backward computation.
I1128 19:31:38.100781 30336 net.cpp:198] Eltwise4 needs backward computation.
I1128 19:31:38.100785 30336 net.cpp:198] Scale10 needs backward computation.
I1128 19:31:38.100787 30336 net.cpp:198] BatchNorm10 needs backward computation.
I1128 19:31:38.100790 30336 net.cpp:198] Convolution10 needs backward computation.
I1128 19:31:38.100793 30336 net.cpp:198] Scale9 needs backward computation.
I1128 19:31:38.100796 30336 net.cpp:198] BatchNorm9 needs backward computation.
I1128 19:31:38.100800 30336 net.cpp:198] Convolution9 needs backward computation.
I1128 19:31:38.100803 30336 net.cpp:198] ReLU8 needs backward computation.
I1128 19:31:38.100806 30336 net.cpp:198] Scale8 needs backward computation.
I1128 19:31:38.100809 30336 net.cpp:198] BatchNorm8 needs backward computation.
I1128 19:31:38.100812 30336 net.cpp:198] Convolution8 needs backward computation.
I1128 19:31:38.100816 30336 net.cpp:198] Eltwise3_ReLU7_0_split needs backward computation.
I1128 19:31:38.100819 30336 net.cpp:198] ReLU7 needs backward computation.
I1128 19:31:38.100822 30336 net.cpp:198] Eltwise3 needs backward computation.
I1128 19:31:38.100826 30336 net.cpp:198] Scale7 needs backward computation.
I1128 19:31:38.100829 30336 net.cpp:198] BatchNorm7 needs backward computation.
I1128 19:31:38.100832 30336 net.cpp:198] Convolution7 needs backward computation.
I1128 19:31:38.100834 30336 net.cpp:198] ReLU6 needs backward computation.
I1128 19:31:38.100837 30336 net.cpp:198] Scale6 needs backward computation.
I1128 19:31:38.100841 30336 net.cpp:198] BatchNorm6 needs backward computation.
I1128 19:31:38.100844 30336 net.cpp:198] Convolution6 needs backward computation.
I1128 19:31:38.100849 30336 net.cpp:198] Eltwise2_ReLU5_0_split needs backward computation.
I1128 19:31:38.100852 30336 net.cpp:198] ReLU5 needs backward computation.
I1128 19:31:38.100855 30336 net.cpp:198] Eltwise2 needs backward computation.
I1128 19:31:38.100859 30336 net.cpp:198] Scale5 needs backward computation.
I1128 19:31:38.100862 30336 net.cpp:198] BatchNorm5 needs backward computation.
I1128 19:31:38.100865 30336 net.cpp:198] Convolution5 needs backward computation.
I1128 19:31:38.100868 30336 net.cpp:198] ReLU4 needs backward computation.
I1128 19:31:38.100872 30336 net.cpp:198] Scale4 needs backward computation.
I1128 19:31:38.100874 30336 net.cpp:198] BatchNorm4 needs backward computation.
I1128 19:31:38.100878 30336 net.cpp:198] Convolution4 needs backward computation.
I1128 19:31:38.100881 30336 net.cpp:198] Eltwise1_ReLU3_0_split needs backward computation.
I1128 19:31:38.100884 30336 net.cpp:198] ReLU3 needs backward computation.
I1128 19:31:38.100891 30336 net.cpp:198] Eltwise1 needs backward computation.
I1128 19:31:38.100895 30336 net.cpp:198] Scale3 needs backward computation.
I1128 19:31:38.100900 30336 net.cpp:198] BatchNorm3 needs backward computation.
I1128 19:31:38.100903 30336 net.cpp:198] Convolution3 needs backward computation.
I1128 19:31:38.100906 30336 net.cpp:198] ReLU2 needs backward computation.
I1128 19:31:38.100909 30336 net.cpp:198] Scale2 needs backward computation.
I1128 19:31:38.100914 30336 net.cpp:198] BatchNorm2 needs backward computation.
I1128 19:31:38.100915 30336 net.cpp:198] Convolution2 needs backward computation.
I1128 19:31:38.100919 30336 net.cpp:198] Convolution1_ReLU1_0_split needs backward computation.
I1128 19:31:38.100922 30336 net.cpp:198] ReLU1 needs backward computation.
I1128 19:31:38.100939 30336 net.cpp:198] Scale1 needs backward computation.
I1128 19:31:38.100941 30336 net.cpp:198] BatchNorm1 needs backward computation.
I1128 19:31:38.100944 30336 net.cpp:198] Convolution1 needs backward computation.
I1128 19:31:38.100947 30336 net.cpp:200] Data1 does not need backward computation.
I1128 19:31:38.100950 30336 net.cpp:242] This network produces output SoftmaxWithLoss1
I1128 19:31:38.100996 30336 net.cpp:255] Network initialization done.
I1128 19:31:38.101510 30336 solver.cpp:172] Creating test net (#0) specified by test_net file: /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/test.prototxt
I1128 19:31:38.101821 30336 net.cpp:51] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  transform_param {
    crop_size: 28
  }
  data_param {
    source: "/home/ljf/caffe-master/examples/ljftest_cifar10_WRN/test_lmdb"
    batch_size: 20
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution3"
  bottom: "Convolution1"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Convolution5"
  bottom: "Eltwise1"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  convolution_param {
    num_output: 96
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Convolution7"
  bottom: "Eltwise2"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution10"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution9"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Convolution12"
  bottom: "Eltwise4"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Convolution14"
  bottom: "Eltwise5"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution15"
  top: "Convolution15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Convolution15"
  top: "Convolution16"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution17"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution16"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Convolution19"
  bottom: "Eltwise7"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution20"
  top: "Convolution20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Convolution21"
  bottom: "Eltwise8"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution22"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution22"
  top: "Convolution22"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Convolution22"
  top: "Convolution23"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution24"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Convolution24"
  top: "Convolution24"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Convolution23"
  bottom: "Convolution24"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution25"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution25"
  top: "Convolution25"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Convolution25"
  top: "Convolution26"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Convolution26"
  top: "Convolution26"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "Convolution26"
  top: "Convolution26"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Convolution26"
  bottom: "Eltwise10"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution27"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "Convolution27"
  top: "Convolution27"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Convolution27"
  top: "Convolution28"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm28"
  type: "BatchNorm"
  bottom: "Convolution28"
  top: "Convolution28"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale28"
  type: "Scale"
  bottom: "Convolution28"
  top: "Convolution28"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Convolution28"
  bottom: "Eltwise11"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "Eltwise12"
  top: "Eltwise12"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise12"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Accuracy1"
  type: "Accuracy"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "Accuracy1"
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
I1128 19:31:38.102145 30336 layer_factory.hpp:77] Creating layer Data1
I1128 19:31:38.102190 30336 db_lmdb.cpp:35] Opened lmdb /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/test_lmdb
I1128 19:31:38.102202 30336 net.cpp:84] Creating Layer Data1
I1128 19:31:38.102206 30336 net.cpp:380] Data1 -> Data1
I1128 19:31:38.102216 30336 net.cpp:380] Data1 -> Data2
I1128 19:31:38.102313 30336 data_layer.cpp:45] output data size: 20,3,28,28
I1128 19:31:38.103044 30336 net.cpp:122] Setting up Data1
I1128 19:31:38.103052 30336 net.cpp:129] Top shape: 20 3 28 28 (47040)
I1128 19:31:38.103057 30336 net.cpp:129] Top shape: 20 (20)
I1128 19:31:38.103060 30336 net.cpp:137] Memory required for data: 188240
I1128 19:31:38.103065 30336 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I1128 19:31:38.103071 30336 net.cpp:84] Creating Layer Data2_Data1_1_split
I1128 19:31:38.103076 30336 net.cpp:406] Data2_Data1_1_split <- Data2
I1128 19:31:38.103081 30336 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_0
I1128 19:31:38.103090 30336 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_1
I1128 19:31:38.103405 30336 net.cpp:122] Setting up Data2_Data1_1_split
I1128 19:31:38.103411 30336 net.cpp:129] Top shape: 20 (20)
I1128 19:31:38.103415 30336 net.cpp:129] Top shape: 20 (20)
I1128 19:31:38.103418 30336 net.cpp:137] Memory required for data: 188400
I1128 19:31:38.103420 30336 layer_factory.hpp:77] Creating layer Convolution1
I1128 19:31:38.103431 30336 net.cpp:84] Creating Layer Convolution1
I1128 19:31:38.103435 30336 net.cpp:406] Convolution1 <- Data1
I1128 19:31:38.103440 30336 net.cpp:380] Convolution1 -> Convolution1
I1128 19:31:38.104423 30336 net.cpp:122] Setting up Convolution1
I1128 19:31:38.104434 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.104439 30336 net.cpp:137] Memory required for data: 6209520
I1128 19:31:38.104446 30336 layer_factory.hpp:77] Creating layer BatchNorm1
I1128 19:31:38.104454 30336 net.cpp:84] Creating Layer BatchNorm1
I1128 19:31:38.104459 30336 net.cpp:406] BatchNorm1 <- Convolution1
I1128 19:31:38.104463 30336 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1128 19:31:38.104624 30336 net.cpp:122] Setting up BatchNorm1
I1128 19:31:38.104630 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.104634 30336 net.cpp:137] Memory required for data: 12230640
I1128 19:31:38.104643 30336 layer_factory.hpp:77] Creating layer Scale1
I1128 19:31:38.104650 30336 net.cpp:84] Creating Layer Scale1
I1128 19:31:38.104652 30336 net.cpp:406] Scale1 <- Convolution1
I1128 19:31:38.104658 30336 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1128 19:31:38.104696 30336 layer_factory.hpp:77] Creating layer Scale1
I1128 19:31:38.104818 30336 net.cpp:122] Setting up Scale1
I1128 19:31:38.104825 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.104830 30336 net.cpp:137] Memory required for data: 18251760
I1128 19:31:38.104835 30336 layer_factory.hpp:77] Creating layer ReLU1
I1128 19:31:38.104841 30336 net.cpp:84] Creating Layer ReLU1
I1128 19:31:38.104845 30336 net.cpp:406] ReLU1 <- Convolution1
I1128 19:31:38.104851 30336 net.cpp:367] ReLU1 -> Convolution1 (in-place)
I1128 19:31:38.104995 30336 net.cpp:122] Setting up ReLU1
I1128 19:31:38.105002 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.105005 30336 net.cpp:137] Memory required for data: 24272880
I1128 19:31:38.105008 30336 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I1128 19:31:38.105013 30336 net.cpp:84] Creating Layer Convolution1_ReLU1_0_split
I1128 19:31:38.105017 30336 net.cpp:406] Convolution1_ReLU1_0_split <- Convolution1
I1128 19:31:38.105023 30336 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I1128 19:31:38.105031 30336 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I1128 19:31:38.105067 30336 net.cpp:122] Setting up Convolution1_ReLU1_0_split
I1128 19:31:38.105072 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.105075 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.105079 30336 net.cpp:137] Memory required for data: 36315120
I1128 19:31:38.105082 30336 layer_factory.hpp:77] Creating layer Convolution2
I1128 19:31:38.105090 30336 net.cpp:84] Creating Layer Convolution2
I1128 19:31:38.105094 30336 net.cpp:406] Convolution2 <- Convolution1_ReLU1_0_split_0
I1128 19:31:38.105103 30336 net.cpp:380] Convolution2 -> Convolution2
I1128 19:31:38.106627 30336 net.cpp:122] Setting up Convolution2
I1128 19:31:38.106637 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.106642 30336 net.cpp:137] Memory required for data: 42336240
I1128 19:31:38.106649 30336 layer_factory.hpp:77] Creating layer BatchNorm2
I1128 19:31:38.106658 30336 net.cpp:84] Creating Layer BatchNorm2
I1128 19:31:38.106662 30336 net.cpp:406] BatchNorm2 <- Convolution2
I1128 19:31:38.106667 30336 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1128 19:31:38.106820 30336 net.cpp:122] Setting up BatchNorm2
I1128 19:31:38.106827 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.106830 30336 net.cpp:137] Memory required for data: 48357360
I1128 19:31:38.106837 30336 layer_factory.hpp:77] Creating layer Scale2
I1128 19:31:38.106842 30336 net.cpp:84] Creating Layer Scale2
I1128 19:31:38.106847 30336 net.cpp:406] Scale2 <- Convolution2
I1128 19:31:38.106851 30336 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1128 19:31:38.106910 30336 layer_factory.hpp:77] Creating layer Scale2
I1128 19:31:38.107025 30336 net.cpp:122] Setting up Scale2
I1128 19:31:38.107031 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.107034 30336 net.cpp:137] Memory required for data: 54378480
I1128 19:31:38.107059 30336 layer_factory.hpp:77] Creating layer ReLU2
I1128 19:31:38.107065 30336 net.cpp:84] Creating Layer ReLU2
I1128 19:31:38.107069 30336 net.cpp:406] ReLU2 <- Convolution2
I1128 19:31:38.107075 30336 net.cpp:367] ReLU2 -> Convolution2 (in-place)
I1128 19:31:38.107398 30336 net.cpp:122] Setting up ReLU2
I1128 19:31:38.107406 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.107410 30336 net.cpp:137] Memory required for data: 60399600
I1128 19:31:38.107414 30336 layer_factory.hpp:77] Creating layer Convolution3
I1128 19:31:38.107421 30336 net.cpp:84] Creating Layer Convolution3
I1128 19:31:38.107426 30336 net.cpp:406] Convolution3 <- Convolution2
I1128 19:31:38.107431 30336 net.cpp:380] Convolution3 -> Convolution3
I1128 19:31:38.108716 30336 net.cpp:122] Setting up Convolution3
I1128 19:31:38.108726 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.108729 30336 net.cpp:137] Memory required for data: 66420720
I1128 19:31:38.108736 30336 layer_factory.hpp:77] Creating layer BatchNorm3
I1128 19:31:38.108741 30336 net.cpp:84] Creating Layer BatchNorm3
I1128 19:31:38.108745 30336 net.cpp:406] BatchNorm3 <- Convolution3
I1128 19:31:38.108752 30336 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1128 19:31:38.108909 30336 net.cpp:122] Setting up BatchNorm3
I1128 19:31:38.108914 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.108917 30336 net.cpp:137] Memory required for data: 72441840
I1128 19:31:38.108928 30336 layer_factory.hpp:77] Creating layer Scale3
I1128 19:31:38.108933 30336 net.cpp:84] Creating Layer Scale3
I1128 19:31:38.108937 30336 net.cpp:406] Scale3 <- Convolution3
I1128 19:31:38.108942 30336 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1128 19:31:38.108978 30336 layer_factory.hpp:77] Creating layer Scale3
I1128 19:31:38.109067 30336 net.cpp:122] Setting up Scale3
I1128 19:31:38.109074 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.109077 30336 net.cpp:137] Memory required for data: 78462960
I1128 19:31:38.109084 30336 layer_factory.hpp:77] Creating layer Eltwise1
I1128 19:31:38.109091 30336 net.cpp:84] Creating Layer Eltwise1
I1128 19:31:38.109093 30336 net.cpp:406] Eltwise1 <- Convolution3
I1128 19:31:38.109097 30336 net.cpp:406] Eltwise1 <- Convolution1_ReLU1_0_split_1
I1128 19:31:38.109103 30336 net.cpp:380] Eltwise1 -> Eltwise1
I1128 19:31:38.109124 30336 net.cpp:122] Setting up Eltwise1
I1128 19:31:38.109130 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.109133 30336 net.cpp:137] Memory required for data: 84484080
I1128 19:31:38.109136 30336 layer_factory.hpp:77] Creating layer ReLU3
I1128 19:31:38.109140 30336 net.cpp:84] Creating Layer ReLU3
I1128 19:31:38.109144 30336 net.cpp:406] ReLU3 <- Eltwise1
I1128 19:31:38.109149 30336 net.cpp:367] ReLU3 -> Eltwise1 (in-place)
I1128 19:31:38.109488 30336 net.cpp:122] Setting up ReLU3
I1128 19:31:38.109496 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.109499 30336 net.cpp:137] Memory required for data: 90505200
I1128 19:31:38.109503 30336 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I1128 19:31:38.109526 30336 net.cpp:84] Creating Layer Eltwise1_ReLU3_0_split
I1128 19:31:38.109530 30336 net.cpp:406] Eltwise1_ReLU3_0_split <- Eltwise1
I1128 19:31:38.109535 30336 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I1128 19:31:38.109541 30336 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I1128 19:31:38.109602 30336 net.cpp:122] Setting up Eltwise1_ReLU3_0_split
I1128 19:31:38.109608 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.109624 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.109628 30336 net.cpp:137] Memory required for data: 102547440
I1128 19:31:38.109645 30336 layer_factory.hpp:77] Creating layer Convolution4
I1128 19:31:38.109655 30336 net.cpp:84] Creating Layer Convolution4
I1128 19:31:38.109658 30336 net.cpp:406] Convolution4 <- Eltwise1_ReLU3_0_split_0
I1128 19:31:38.109665 30336 net.cpp:380] Convolution4 -> Convolution4
I1128 19:31:38.111156 30336 net.cpp:122] Setting up Convolution4
I1128 19:31:38.111166 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.111168 30336 net.cpp:137] Memory required for data: 108568560
I1128 19:31:38.111188 30336 layer_factory.hpp:77] Creating layer BatchNorm4
I1128 19:31:38.111196 30336 net.cpp:84] Creating Layer BatchNorm4
I1128 19:31:38.111202 30336 net.cpp:406] BatchNorm4 <- Convolution4
I1128 19:31:38.111207 30336 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1128 19:31:38.111402 30336 net.cpp:122] Setting up BatchNorm4
I1128 19:31:38.111408 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.111412 30336 net.cpp:137] Memory required for data: 114589680
I1128 19:31:38.111433 30336 layer_factory.hpp:77] Creating layer Scale4
I1128 19:31:38.111438 30336 net.cpp:84] Creating Layer Scale4
I1128 19:31:38.111443 30336 net.cpp:406] Scale4 <- Convolution4
I1128 19:31:38.111449 30336 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1128 19:31:38.111493 30336 layer_factory.hpp:77] Creating layer Scale4
I1128 19:31:38.111584 30336 net.cpp:122] Setting up Scale4
I1128 19:31:38.111590 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.111593 30336 net.cpp:137] Memory required for data: 120610800
I1128 19:31:38.111598 30336 layer_factory.hpp:77] Creating layer ReLU4
I1128 19:31:38.111603 30336 net.cpp:84] Creating Layer ReLU4
I1128 19:31:38.111608 30336 net.cpp:406] ReLU4 <- Convolution4
I1128 19:31:38.111613 30336 net.cpp:367] ReLU4 -> Convolution4 (in-place)
I1128 19:31:38.111740 30336 net.cpp:122] Setting up ReLU4
I1128 19:31:38.111747 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.111750 30336 net.cpp:137] Memory required for data: 126631920
I1128 19:31:38.111753 30336 layer_factory.hpp:77] Creating layer Convolution5
I1128 19:31:38.111762 30336 net.cpp:84] Creating Layer Convolution5
I1128 19:31:38.111764 30336 net.cpp:406] Convolution5 <- Convolution4
I1128 19:31:38.111770 30336 net.cpp:380] Convolution5 -> Convolution5
I1128 19:31:38.113229 30336 net.cpp:122] Setting up Convolution5
I1128 19:31:38.113237 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.113241 30336 net.cpp:137] Memory required for data: 132653040
I1128 19:31:38.113247 30336 layer_factory.hpp:77] Creating layer BatchNorm5
I1128 19:31:38.113255 30336 net.cpp:84] Creating Layer BatchNorm5
I1128 19:31:38.113258 30336 net.cpp:406] BatchNorm5 <- Convolution5
I1128 19:31:38.113265 30336 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1128 19:31:38.113420 30336 net.cpp:122] Setting up BatchNorm5
I1128 19:31:38.113425 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.113428 30336 net.cpp:137] Memory required for data: 138674160
I1128 19:31:38.113438 30336 layer_factory.hpp:77] Creating layer Scale5
I1128 19:31:38.113443 30336 net.cpp:84] Creating Layer Scale5
I1128 19:31:38.113447 30336 net.cpp:406] Scale5 <- Convolution5
I1128 19:31:38.113451 30336 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1128 19:31:38.113485 30336 layer_factory.hpp:77] Creating layer Scale5
I1128 19:31:38.113576 30336 net.cpp:122] Setting up Scale5
I1128 19:31:38.113582 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.113585 30336 net.cpp:137] Memory required for data: 144695280
I1128 19:31:38.113590 30336 layer_factory.hpp:77] Creating layer Eltwise2
I1128 19:31:38.113595 30336 net.cpp:84] Creating Layer Eltwise2
I1128 19:31:38.113600 30336 net.cpp:406] Eltwise2 <- Convolution5
I1128 19:31:38.113605 30336 net.cpp:406] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I1128 19:31:38.113608 30336 net.cpp:380] Eltwise2 -> Eltwise2
I1128 19:31:38.113644 30336 net.cpp:122] Setting up Eltwise2
I1128 19:31:38.113651 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.113653 30336 net.cpp:137] Memory required for data: 150716400
I1128 19:31:38.113656 30336 layer_factory.hpp:77] Creating layer ReLU5
I1128 19:31:38.113674 30336 net.cpp:84] Creating Layer ReLU5
I1128 19:31:38.113678 30336 net.cpp:406] ReLU5 <- Eltwise2
I1128 19:31:38.113682 30336 net.cpp:367] ReLU5 -> Eltwise2 (in-place)
I1128 19:31:38.113842 30336 net.cpp:122] Setting up ReLU5
I1128 19:31:38.113848 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.113852 30336 net.cpp:137] Memory required for data: 156737520
I1128 19:31:38.113869 30336 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I1128 19:31:38.113875 30336 net.cpp:84] Creating Layer Eltwise2_ReLU5_0_split
I1128 19:31:38.113879 30336 net.cpp:406] Eltwise2_ReLU5_0_split <- Eltwise2
I1128 19:31:38.113886 30336 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I1128 19:31:38.113893 30336 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I1128 19:31:38.113955 30336 net.cpp:122] Setting up Eltwise2_ReLU5_0_split
I1128 19:31:38.113963 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.113981 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.113984 30336 net.cpp:137] Memory required for data: 168779760
I1128 19:31:38.113987 30336 layer_factory.hpp:77] Creating layer Convolution6
I1128 19:31:38.114012 30336 net.cpp:84] Creating Layer Convolution6
I1128 19:31:38.114015 30336 net.cpp:406] Convolution6 <- Eltwise2_ReLU5_0_split_0
I1128 19:31:38.114022 30336 net.cpp:380] Convolution6 -> Convolution6
I1128 19:31:38.115515 30336 net.cpp:122] Setting up Convolution6
I1128 19:31:38.115525 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.115530 30336 net.cpp:137] Memory required for data: 174800880
I1128 19:31:38.115550 30336 layer_factory.hpp:77] Creating layer BatchNorm6
I1128 19:31:38.115556 30336 net.cpp:84] Creating Layer BatchNorm6
I1128 19:31:38.115559 30336 net.cpp:406] BatchNorm6 <- Convolution6
I1128 19:31:38.115566 30336 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1128 19:31:38.115739 30336 net.cpp:122] Setting up BatchNorm6
I1128 19:31:38.115746 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.115749 30336 net.cpp:137] Memory required for data: 180822000
I1128 19:31:38.115756 30336 layer_factory.hpp:77] Creating layer Scale6
I1128 19:31:38.115761 30336 net.cpp:84] Creating Layer Scale6
I1128 19:31:38.115766 30336 net.cpp:406] Scale6 <- Convolution6
I1128 19:31:38.115770 30336 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1128 19:31:38.115804 30336 layer_factory.hpp:77] Creating layer Scale6
I1128 19:31:38.115896 30336 net.cpp:122] Setting up Scale6
I1128 19:31:38.115901 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.115905 30336 net.cpp:137] Memory required for data: 186843120
I1128 19:31:38.115911 30336 layer_factory.hpp:77] Creating layer ReLU6
I1128 19:31:38.115916 30336 net.cpp:84] Creating Layer ReLU6
I1128 19:31:38.115918 30336 net.cpp:406] ReLU6 <- Convolution6
I1128 19:31:38.115926 30336 net.cpp:367] ReLU6 -> Convolution6 (in-place)
I1128 19:31:38.116048 30336 net.cpp:122] Setting up ReLU6
I1128 19:31:38.116055 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.116058 30336 net.cpp:137] Memory required for data: 192864240
I1128 19:31:38.116061 30336 layer_factory.hpp:77] Creating layer Convolution7
I1128 19:31:38.116070 30336 net.cpp:84] Creating Layer Convolution7
I1128 19:31:38.116073 30336 net.cpp:406] Convolution7 <- Convolution6
I1128 19:31:38.116080 30336 net.cpp:380] Convolution7 -> Convolution7
I1128 19:31:38.117638 30336 net.cpp:122] Setting up Convolution7
I1128 19:31:38.117647 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.117651 30336 net.cpp:137] Memory required for data: 198885360
I1128 19:31:38.117657 30336 layer_factory.hpp:77] Creating layer BatchNorm7
I1128 19:31:38.117667 30336 net.cpp:84] Creating Layer BatchNorm7
I1128 19:31:38.117671 30336 net.cpp:406] BatchNorm7 <- Convolution7
I1128 19:31:38.117676 30336 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1128 19:31:38.117833 30336 net.cpp:122] Setting up BatchNorm7
I1128 19:31:38.117839 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.117842 30336 net.cpp:137] Memory required for data: 204906480
I1128 19:31:38.117849 30336 layer_factory.hpp:77] Creating layer Scale7
I1128 19:31:38.117861 30336 net.cpp:84] Creating Layer Scale7
I1128 19:31:38.117866 30336 net.cpp:406] Scale7 <- Convolution7
I1128 19:31:38.117871 30336 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1128 19:31:38.117918 30336 layer_factory.hpp:77] Creating layer Scale7
I1128 19:31:38.118054 30336 net.cpp:122] Setting up Scale7
I1128 19:31:38.118060 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.118063 30336 net.cpp:137] Memory required for data: 210927600
I1128 19:31:38.118069 30336 layer_factory.hpp:77] Creating layer Eltwise3
I1128 19:31:38.118077 30336 net.cpp:84] Creating Layer Eltwise3
I1128 19:31:38.118080 30336 net.cpp:406] Eltwise3 <- Convolution7
I1128 19:31:38.118085 30336 net.cpp:406] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I1128 19:31:38.118091 30336 net.cpp:380] Eltwise3 -> Eltwise3
I1128 19:31:38.118139 30336 net.cpp:122] Setting up Eltwise3
I1128 19:31:38.118145 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.118149 30336 net.cpp:137] Memory required for data: 216948720
I1128 19:31:38.118151 30336 layer_factory.hpp:77] Creating layer ReLU7
I1128 19:31:38.118156 30336 net.cpp:84] Creating Layer ReLU7
I1128 19:31:38.118160 30336 net.cpp:406] ReLU7 <- Eltwise3
I1128 19:31:38.118165 30336 net.cpp:367] ReLU7 -> Eltwise3 (in-place)
I1128 19:31:38.118288 30336 net.cpp:122] Setting up ReLU7
I1128 19:31:38.118294 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.118296 30336 net.cpp:137] Memory required for data: 222969840
I1128 19:31:38.118299 30336 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I1128 19:31:38.118304 30336 net.cpp:84] Creating Layer Eltwise3_ReLU7_0_split
I1128 19:31:38.118309 30336 net.cpp:406] Eltwise3_ReLU7_0_split <- Eltwise3
I1128 19:31:38.118314 30336 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I1128 19:31:38.118321 30336 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I1128 19:31:38.118356 30336 net.cpp:122] Setting up Eltwise3_ReLU7_0_split
I1128 19:31:38.118361 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.118366 30336 net.cpp:129] Top shape: 20 96 28 28 (1505280)
I1128 19:31:38.118368 30336 net.cpp:137] Memory required for data: 235012080
I1128 19:31:38.118372 30336 layer_factory.hpp:77] Creating layer Convolution8
I1128 19:31:38.118381 30336 net.cpp:84] Creating Layer Convolution8
I1128 19:31:38.118383 30336 net.cpp:406] Convolution8 <- Eltwise3_ReLU7_0_split_0
I1128 19:31:38.118391 30336 net.cpp:380] Convolution8 -> Convolution8
I1128 19:31:38.120719 30336 net.cpp:122] Setting up Convolution8
I1128 19:31:38.120728 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.120731 30336 net.cpp:137] Memory required for data: 238022640
I1128 19:31:38.120738 30336 layer_factory.hpp:77] Creating layer BatchNorm8
I1128 19:31:38.120745 30336 net.cpp:84] Creating Layer BatchNorm8
I1128 19:31:38.120749 30336 net.cpp:406] BatchNorm8 <- Convolution8
I1128 19:31:38.120755 30336 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1128 19:31:38.120913 30336 net.cpp:122] Setting up BatchNorm8
I1128 19:31:38.120919 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.120923 30336 net.cpp:137] Memory required for data: 241033200
I1128 19:31:38.120929 30336 layer_factory.hpp:77] Creating layer Scale8
I1128 19:31:38.120935 30336 net.cpp:84] Creating Layer Scale8
I1128 19:31:38.120939 30336 net.cpp:406] Scale8 <- Convolution8
I1128 19:31:38.120944 30336 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1128 19:31:38.120978 30336 layer_factory.hpp:77] Creating layer Scale8
I1128 19:31:38.121070 30336 net.cpp:122] Setting up Scale8
I1128 19:31:38.121076 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.121079 30336 net.cpp:137] Memory required for data: 244043760
I1128 19:31:38.121084 30336 layer_factory.hpp:77] Creating layer ReLU8
I1128 19:31:38.121089 30336 net.cpp:84] Creating Layer ReLU8
I1128 19:31:38.121093 30336 net.cpp:406] ReLU8 <- Convolution8
I1128 19:31:38.121099 30336 net.cpp:367] ReLU8 -> Convolution8 (in-place)
I1128 19:31:38.121419 30336 net.cpp:122] Setting up ReLU8
I1128 19:31:38.121434 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.121438 30336 net.cpp:137] Memory required for data: 247054320
I1128 19:31:38.121441 30336 layer_factory.hpp:77] Creating layer Convolution9
I1128 19:31:38.121449 30336 net.cpp:84] Creating Layer Convolution9
I1128 19:31:38.121454 30336 net.cpp:406] Convolution9 <- Convolution8
I1128 19:31:38.121460 30336 net.cpp:380] Convolution9 -> Convolution9
I1128 19:31:38.124258 30336 net.cpp:122] Setting up Convolution9
I1128 19:31:38.124267 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.124271 30336 net.cpp:137] Memory required for data: 250064880
I1128 19:31:38.124277 30336 layer_factory.hpp:77] Creating layer BatchNorm9
I1128 19:31:38.124284 30336 net.cpp:84] Creating Layer BatchNorm9
I1128 19:31:38.124289 30336 net.cpp:406] BatchNorm9 <- Convolution9
I1128 19:31:38.124295 30336 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1128 19:31:38.124455 30336 net.cpp:122] Setting up BatchNorm9
I1128 19:31:38.124461 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.124464 30336 net.cpp:137] Memory required for data: 253075440
I1128 19:31:38.124470 30336 layer_factory.hpp:77] Creating layer Scale9
I1128 19:31:38.124475 30336 net.cpp:84] Creating Layer Scale9
I1128 19:31:38.124480 30336 net.cpp:406] Scale9 <- Convolution9
I1128 19:31:38.124485 30336 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1128 19:31:38.124518 30336 layer_factory.hpp:77] Creating layer Scale9
I1128 19:31:38.124609 30336 net.cpp:122] Setting up Scale9
I1128 19:31:38.124615 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.124617 30336 net.cpp:137] Memory required for data: 256086000
I1128 19:31:38.124624 30336 layer_factory.hpp:77] Creating layer Convolution10
I1128 19:31:38.124631 30336 net.cpp:84] Creating Layer Convolution10
I1128 19:31:38.124636 30336 net.cpp:406] Convolution10 <- Eltwise3_ReLU7_0_split_1
I1128 19:31:38.124642 30336 net.cpp:380] Convolution10 -> Convolution10
I1128 19:31:38.125915 30336 net.cpp:122] Setting up Convolution10
I1128 19:31:38.125926 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.125931 30336 net.cpp:137] Memory required for data: 259096560
I1128 19:31:38.125944 30336 layer_factory.hpp:77] Creating layer BatchNorm10
I1128 19:31:38.125952 30336 net.cpp:84] Creating Layer BatchNorm10
I1128 19:31:38.125955 30336 net.cpp:406] BatchNorm10 <- Convolution10
I1128 19:31:38.125962 30336 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1128 19:31:38.126114 30336 net.cpp:122] Setting up BatchNorm10
I1128 19:31:38.126121 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.126123 30336 net.cpp:137] Memory required for data: 262107120
I1128 19:31:38.126130 30336 layer_factory.hpp:77] Creating layer Scale10
I1128 19:31:38.126137 30336 net.cpp:84] Creating Layer Scale10
I1128 19:31:38.126140 30336 net.cpp:406] Scale10 <- Convolution10
I1128 19:31:38.126144 30336 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1128 19:31:38.126179 30336 layer_factory.hpp:77] Creating layer Scale10
I1128 19:31:38.126271 30336 net.cpp:122] Setting up Scale10
I1128 19:31:38.126276 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.126279 30336 net.cpp:137] Memory required for data: 265117680
I1128 19:31:38.126284 30336 layer_factory.hpp:77] Creating layer Eltwise4
I1128 19:31:38.126291 30336 net.cpp:84] Creating Layer Eltwise4
I1128 19:31:38.126293 30336 net.cpp:406] Eltwise4 <- Convolution9
I1128 19:31:38.126297 30336 net.cpp:406] Eltwise4 <- Convolution10
I1128 19:31:38.126304 30336 net.cpp:380] Eltwise4 -> Eltwise4
I1128 19:31:38.126324 30336 net.cpp:122] Setting up Eltwise4
I1128 19:31:38.126329 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.126332 30336 net.cpp:137] Memory required for data: 268128240
I1128 19:31:38.126335 30336 layer_factory.hpp:77] Creating layer ReLU9
I1128 19:31:38.126341 30336 net.cpp:84] Creating Layer ReLU9
I1128 19:31:38.126345 30336 net.cpp:406] ReLU9 <- Eltwise4
I1128 19:31:38.126349 30336 net.cpp:367] ReLU9 -> Eltwise4 (in-place)
I1128 19:31:38.126682 30336 net.cpp:122] Setting up ReLU9
I1128 19:31:38.126691 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.126694 30336 net.cpp:137] Memory required for data: 271138800
I1128 19:31:38.126698 30336 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I1128 19:31:38.126705 30336 net.cpp:84] Creating Layer Eltwise4_ReLU9_0_split
I1128 19:31:38.126709 30336 net.cpp:406] Eltwise4_ReLU9_0_split <- Eltwise4
I1128 19:31:38.126714 30336 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I1128 19:31:38.126744 30336 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I1128 19:31:38.126806 30336 net.cpp:122] Setting up Eltwise4_ReLU9_0_split
I1128 19:31:38.126811 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.126816 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.126818 30336 net.cpp:137] Memory required for data: 277159920
I1128 19:31:38.126834 30336 layer_factory.hpp:77] Creating layer Convolution11
I1128 19:31:38.126844 30336 net.cpp:84] Creating Layer Convolution11
I1128 19:31:38.126847 30336 net.cpp:406] Convolution11 <- Eltwise4_ReLU9_0_split_0
I1128 19:31:38.126854 30336 net.cpp:380] Convolution11 -> Convolution11
I1128 19:31:38.130368 30336 net.cpp:122] Setting up Convolution11
I1128 19:31:38.130383 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.130386 30336 net.cpp:137] Memory required for data: 280170480
I1128 19:31:38.130393 30336 layer_factory.hpp:77] Creating layer BatchNorm11
I1128 19:31:38.130403 30336 net.cpp:84] Creating Layer BatchNorm11
I1128 19:31:38.130406 30336 net.cpp:406] BatchNorm11 <- Convolution11
I1128 19:31:38.130412 30336 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1128 19:31:38.130630 30336 net.cpp:122] Setting up BatchNorm11
I1128 19:31:38.130635 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.130638 30336 net.cpp:137] Memory required for data: 283181040
I1128 19:31:38.130645 30336 layer_factory.hpp:77] Creating layer Scale11
I1128 19:31:38.130650 30336 net.cpp:84] Creating Layer Scale11
I1128 19:31:38.130653 30336 net.cpp:406] Scale11 <- Convolution11
I1128 19:31:38.130659 30336 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1128 19:31:38.130753 30336 layer_factory.hpp:77] Creating layer Scale11
I1128 19:31:38.130877 30336 net.cpp:122] Setting up Scale11
I1128 19:31:38.130882 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.130884 30336 net.cpp:137] Memory required for data: 286191600
I1128 19:31:38.130889 30336 layer_factory.hpp:77] Creating layer ReLU10
I1128 19:31:38.130895 30336 net.cpp:84] Creating Layer ReLU10
I1128 19:31:38.130899 30336 net.cpp:406] ReLU10 <- Convolution11
I1128 19:31:38.130904 30336 net.cpp:367] ReLU10 -> Convolution11 (in-place)
I1128 19:31:38.131266 30336 net.cpp:122] Setting up ReLU10
I1128 19:31:38.131275 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.131278 30336 net.cpp:137] Memory required for data: 289202160
I1128 19:31:38.131294 30336 layer_factory.hpp:77] Creating layer Convolution12
I1128 19:31:38.131304 30336 net.cpp:84] Creating Layer Convolution12
I1128 19:31:38.131307 30336 net.cpp:406] Convolution12 <- Convolution11
I1128 19:31:38.131314 30336 net.cpp:380] Convolution12 -> Convolution12
I1128 19:31:38.134948 30336 net.cpp:122] Setting up Convolution12
I1128 19:31:38.134956 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.134960 30336 net.cpp:137] Memory required for data: 292212720
I1128 19:31:38.134966 30336 layer_factory.hpp:77] Creating layer BatchNorm12
I1128 19:31:38.134974 30336 net.cpp:84] Creating Layer BatchNorm12
I1128 19:31:38.134979 30336 net.cpp:406] BatchNorm12 <- Convolution12
I1128 19:31:38.134982 30336 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I1128 19:31:38.135179 30336 net.cpp:122] Setting up BatchNorm12
I1128 19:31:38.135184 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.135186 30336 net.cpp:137] Memory required for data: 295223280
I1128 19:31:38.135192 30336 layer_factory.hpp:77] Creating layer Scale12
I1128 19:31:38.135205 30336 net.cpp:84] Creating Layer Scale12
I1128 19:31:38.135210 30336 net.cpp:406] Scale12 <- Convolution12
I1128 19:31:38.135215 30336 net.cpp:367] Scale12 -> Convolution12 (in-place)
I1128 19:31:38.135251 30336 layer_factory.hpp:77] Creating layer Scale12
I1128 19:31:38.135342 30336 net.cpp:122] Setting up Scale12
I1128 19:31:38.135349 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.135351 30336 net.cpp:137] Memory required for data: 298233840
I1128 19:31:38.135356 30336 layer_factory.hpp:77] Creating layer Eltwise5
I1128 19:31:38.135362 30336 net.cpp:84] Creating Layer Eltwise5
I1128 19:31:38.135366 30336 net.cpp:406] Eltwise5 <- Convolution12
I1128 19:31:38.135370 30336 net.cpp:406] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I1128 19:31:38.135376 30336 net.cpp:380] Eltwise5 -> Eltwise5
I1128 19:31:38.135398 30336 net.cpp:122] Setting up Eltwise5
I1128 19:31:38.135403 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.135406 30336 net.cpp:137] Memory required for data: 301244400
I1128 19:31:38.135411 30336 layer_factory.hpp:77] Creating layer ReLU11
I1128 19:31:38.135416 30336 net.cpp:84] Creating Layer ReLU11
I1128 19:31:38.135421 30336 net.cpp:406] ReLU11 <- Eltwise5
I1128 19:31:38.135424 30336 net.cpp:367] ReLU11 -> Eltwise5 (in-place)
I1128 19:31:38.135545 30336 net.cpp:122] Setting up ReLU11
I1128 19:31:38.135551 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.135555 30336 net.cpp:137] Memory required for data: 304254960
I1128 19:31:38.135557 30336 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I1128 19:31:38.135563 30336 net.cpp:84] Creating Layer Eltwise5_ReLU11_0_split
I1128 19:31:38.135567 30336 net.cpp:406] Eltwise5_ReLU11_0_split <- Eltwise5
I1128 19:31:38.135572 30336 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I1128 19:31:38.135581 30336 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I1128 19:31:38.135625 30336 net.cpp:122] Setting up Eltwise5_ReLU11_0_split
I1128 19:31:38.135632 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.135637 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.135641 30336 net.cpp:137] Memory required for data: 310276080
I1128 19:31:38.135644 30336 layer_factory.hpp:77] Creating layer Convolution13
I1128 19:31:38.135654 30336 net.cpp:84] Creating Layer Convolution13
I1128 19:31:38.135658 30336 net.cpp:406] Convolution13 <- Eltwise5_ReLU11_0_split_0
I1128 19:31:38.135663 30336 net.cpp:380] Convolution13 -> Convolution13
I1128 19:31:38.139230 30336 net.cpp:122] Setting up Convolution13
I1128 19:31:38.139240 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.139243 30336 net.cpp:137] Memory required for data: 313286640
I1128 19:31:38.139250 30336 layer_factory.hpp:77] Creating layer BatchNorm13
I1128 19:31:38.139256 30336 net.cpp:84] Creating Layer BatchNorm13
I1128 19:31:38.139259 30336 net.cpp:406] BatchNorm13 <- Convolution13
I1128 19:31:38.139266 30336 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I1128 19:31:38.139430 30336 net.cpp:122] Setting up BatchNorm13
I1128 19:31:38.139436 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.139438 30336 net.cpp:137] Memory required for data: 316297200
I1128 19:31:38.139446 30336 layer_factory.hpp:77] Creating layer Scale13
I1128 19:31:38.139451 30336 net.cpp:84] Creating Layer Scale13
I1128 19:31:38.139456 30336 net.cpp:406] Scale13 <- Convolution13
I1128 19:31:38.139461 30336 net.cpp:367] Scale13 -> Convolution13 (in-place)
I1128 19:31:38.139497 30336 layer_factory.hpp:77] Creating layer Scale13
I1128 19:31:38.139588 30336 net.cpp:122] Setting up Scale13
I1128 19:31:38.139595 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.139597 30336 net.cpp:137] Memory required for data: 319307760
I1128 19:31:38.139602 30336 layer_factory.hpp:77] Creating layer ReLU12
I1128 19:31:38.139607 30336 net.cpp:84] Creating Layer ReLU12
I1128 19:31:38.139611 30336 net.cpp:406] ReLU12 <- Convolution13
I1128 19:31:38.139657 30336 net.cpp:367] ReLU12 -> Convolution13 (in-place)
I1128 19:31:38.139796 30336 net.cpp:122] Setting up ReLU12
I1128 19:31:38.139803 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.139806 30336 net.cpp:137] Memory required for data: 322318320
I1128 19:31:38.139809 30336 layer_factory.hpp:77] Creating layer Convolution14
I1128 19:31:38.139823 30336 net.cpp:84] Creating Layer Convolution14
I1128 19:31:38.139827 30336 net.cpp:406] Convolution14 <- Convolution13
I1128 19:31:38.139832 30336 net.cpp:380] Convolution14 -> Convolution14
I1128 19:31:38.143421 30336 net.cpp:122] Setting up Convolution14
I1128 19:31:38.143431 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.143435 30336 net.cpp:137] Memory required for data: 325328880
I1128 19:31:38.143442 30336 layer_factory.hpp:77] Creating layer BatchNorm14
I1128 19:31:38.143450 30336 net.cpp:84] Creating Layer BatchNorm14
I1128 19:31:38.143453 30336 net.cpp:406] BatchNorm14 <- Convolution14
I1128 19:31:38.143458 30336 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I1128 19:31:38.143631 30336 net.cpp:122] Setting up BatchNorm14
I1128 19:31:38.143637 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.143640 30336 net.cpp:137] Memory required for data: 328339440
I1128 19:31:38.143666 30336 layer_factory.hpp:77] Creating layer Scale14
I1128 19:31:38.143672 30336 net.cpp:84] Creating Layer Scale14
I1128 19:31:38.143677 30336 net.cpp:406] Scale14 <- Convolution14
I1128 19:31:38.143682 30336 net.cpp:367] Scale14 -> Convolution14 (in-place)
I1128 19:31:38.143731 30336 layer_factory.hpp:77] Creating layer Scale14
I1128 19:31:38.143826 30336 net.cpp:122] Setting up Scale14
I1128 19:31:38.143831 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.143834 30336 net.cpp:137] Memory required for data: 331350000
I1128 19:31:38.143839 30336 layer_factory.hpp:77] Creating layer Eltwise6
I1128 19:31:38.143847 30336 net.cpp:84] Creating Layer Eltwise6
I1128 19:31:38.143851 30336 net.cpp:406] Eltwise6 <- Convolution14
I1128 19:31:38.143856 30336 net.cpp:406] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I1128 19:31:38.143862 30336 net.cpp:380] Eltwise6 -> Eltwise6
I1128 19:31:38.143883 30336 net.cpp:122] Setting up Eltwise6
I1128 19:31:38.143889 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.143892 30336 net.cpp:137] Memory required for data: 334360560
I1128 19:31:38.143894 30336 layer_factory.hpp:77] Creating layer ReLU13
I1128 19:31:38.143900 30336 net.cpp:84] Creating Layer ReLU13
I1128 19:31:38.143903 30336 net.cpp:406] ReLU13 <- Eltwise6
I1128 19:31:38.143909 30336 net.cpp:367] ReLU13 -> Eltwise6 (in-place)
I1128 19:31:38.144033 30336 net.cpp:122] Setting up ReLU13
I1128 19:31:38.144040 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.144043 30336 net.cpp:137] Memory required for data: 337371120
I1128 19:31:38.144047 30336 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I1128 19:31:38.144052 30336 net.cpp:84] Creating Layer Eltwise6_ReLU13_0_split
I1128 19:31:38.144057 30336 net.cpp:406] Eltwise6_ReLU13_0_split <- Eltwise6
I1128 19:31:38.144062 30336 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I1128 19:31:38.144068 30336 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I1128 19:31:38.144104 30336 net.cpp:122] Setting up Eltwise6_ReLU13_0_split
I1128 19:31:38.144109 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.144114 30336 net.cpp:129] Top shape: 20 192 14 14 (752640)
I1128 19:31:38.144115 30336 net.cpp:137] Memory required for data: 343392240
I1128 19:31:38.144119 30336 layer_factory.hpp:77] Creating layer Convolution15
I1128 19:31:38.144127 30336 net.cpp:84] Creating Layer Convolution15
I1128 19:31:38.144130 30336 net.cpp:406] Convolution15 <- Eltwise6_ReLU13_0_split_0
I1128 19:31:38.144137 30336 net.cpp:380] Convolution15 -> Convolution15
I1128 19:31:38.149659 30336 net.cpp:122] Setting up Convolution15
I1128 19:31:38.149670 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.149685 30336 net.cpp:137] Memory required for data: 344897520
I1128 19:31:38.149691 30336 layer_factory.hpp:77] Creating layer BatchNorm15
I1128 19:31:38.149698 30336 net.cpp:84] Creating Layer BatchNorm15
I1128 19:31:38.149703 30336 net.cpp:406] BatchNorm15 <- Convolution15
I1128 19:31:38.149709 30336 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I1128 19:31:38.149871 30336 net.cpp:122] Setting up BatchNorm15
I1128 19:31:38.149878 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.149881 30336 net.cpp:137] Memory required for data: 346402800
I1128 19:31:38.149888 30336 layer_factory.hpp:77] Creating layer Scale15
I1128 19:31:38.149893 30336 net.cpp:84] Creating Layer Scale15
I1128 19:31:38.149896 30336 net.cpp:406] Scale15 <- Convolution15
I1128 19:31:38.149902 30336 net.cpp:367] Scale15 -> Convolution15 (in-place)
I1128 19:31:38.149935 30336 layer_factory.hpp:77] Creating layer Scale15
I1128 19:31:38.150028 30336 net.cpp:122] Setting up Scale15
I1128 19:31:38.150034 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.150039 30336 net.cpp:137] Memory required for data: 347908080
I1128 19:31:38.150044 30336 layer_factory.hpp:77] Creating layer ReLU14
I1128 19:31:38.150049 30336 net.cpp:84] Creating Layer ReLU14
I1128 19:31:38.150053 30336 net.cpp:406] ReLU14 <- Convolution15
I1128 19:31:38.150058 30336 net.cpp:367] ReLU14 -> Convolution15 (in-place)
I1128 19:31:38.150183 30336 net.cpp:122] Setting up ReLU14
I1128 19:31:38.150189 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.150192 30336 net.cpp:137] Memory required for data: 349413360
I1128 19:31:38.150197 30336 layer_factory.hpp:77] Creating layer Convolution16
I1128 19:31:38.150205 30336 net.cpp:84] Creating Layer Convolution16
I1128 19:31:38.150208 30336 net.cpp:406] Convolution16 <- Convolution15
I1128 19:31:38.150213 30336 net.cpp:380] Convolution16 -> Convolution16
I1128 19:31:38.160917 30336 net.cpp:122] Setting up Convolution16
I1128 19:31:38.160938 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.160941 30336 net.cpp:137] Memory required for data: 350918640
I1128 19:31:38.160948 30336 layer_factory.hpp:77] Creating layer BatchNorm16
I1128 19:31:38.160957 30336 net.cpp:84] Creating Layer BatchNorm16
I1128 19:31:38.160964 30336 net.cpp:406] BatchNorm16 <- Convolution16
I1128 19:31:38.160972 30336 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I1128 19:31:38.161206 30336 net.cpp:122] Setting up BatchNorm16
I1128 19:31:38.161211 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.161214 30336 net.cpp:137] Memory required for data: 352423920
I1128 19:31:38.161221 30336 layer_factory.hpp:77] Creating layer Scale16
I1128 19:31:38.161227 30336 net.cpp:84] Creating Layer Scale16
I1128 19:31:38.161231 30336 net.cpp:406] Scale16 <- Convolution16
I1128 19:31:38.161237 30336 net.cpp:367] Scale16 -> Convolution16 (in-place)
I1128 19:31:38.161303 30336 layer_factory.hpp:77] Creating layer Scale16
I1128 19:31:38.161396 30336 net.cpp:122] Setting up Scale16
I1128 19:31:38.161402 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.161406 30336 net.cpp:137] Memory required for data: 353929200
I1128 19:31:38.161411 30336 layer_factory.hpp:77] Creating layer Convolution17
I1128 19:31:38.161443 30336 net.cpp:84] Creating Layer Convolution17
I1128 19:31:38.161448 30336 net.cpp:406] Convolution17 <- Eltwise6_ReLU13_0_split_1
I1128 19:31:38.161456 30336 net.cpp:380] Convolution17 -> Convolution17
I1128 19:31:38.162726 30336 net.cpp:122] Setting up Convolution17
I1128 19:31:38.162735 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.162739 30336 net.cpp:137] Memory required for data: 355434480
I1128 19:31:38.162745 30336 layer_factory.hpp:77] Creating layer BatchNorm17
I1128 19:31:38.162751 30336 net.cpp:84] Creating Layer BatchNorm17
I1128 19:31:38.162755 30336 net.cpp:406] BatchNorm17 <- Convolution17
I1128 19:31:38.162761 30336 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I1128 19:31:38.162928 30336 net.cpp:122] Setting up BatchNorm17
I1128 19:31:38.162935 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.162959 30336 net.cpp:137] Memory required for data: 356939760
I1128 19:31:38.162966 30336 layer_factory.hpp:77] Creating layer Scale17
I1128 19:31:38.162972 30336 net.cpp:84] Creating Layer Scale17
I1128 19:31:38.162976 30336 net.cpp:406] Scale17 <- Convolution17
I1128 19:31:38.162982 30336 net.cpp:367] Scale17 -> Convolution17 (in-place)
I1128 19:31:38.163014 30336 layer_factory.hpp:77] Creating layer Scale17
I1128 19:31:38.163143 30336 net.cpp:122] Setting up Scale17
I1128 19:31:38.163151 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.163166 30336 net.cpp:137] Memory required for data: 358445040
I1128 19:31:38.163172 30336 layer_factory.hpp:77] Creating layer Eltwise7
I1128 19:31:38.163178 30336 net.cpp:84] Creating Layer Eltwise7
I1128 19:31:38.163182 30336 net.cpp:406] Eltwise7 <- Convolution16
I1128 19:31:38.163185 30336 net.cpp:406] Eltwise7 <- Convolution17
I1128 19:31:38.163192 30336 net.cpp:380] Eltwise7 -> Eltwise7
I1128 19:31:38.163214 30336 net.cpp:122] Setting up Eltwise7
I1128 19:31:38.163220 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.163223 30336 net.cpp:137] Memory required for data: 359950320
I1128 19:31:38.163225 30336 layer_factory.hpp:77] Creating layer ReLU15
I1128 19:31:38.163231 30336 net.cpp:84] Creating Layer ReLU15
I1128 19:31:38.163235 30336 net.cpp:406] ReLU15 <- Eltwise7
I1128 19:31:38.163240 30336 net.cpp:367] ReLU15 -> Eltwise7 (in-place)
I1128 19:31:38.163359 30336 net.cpp:122] Setting up ReLU15
I1128 19:31:38.163367 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.163369 30336 net.cpp:137] Memory required for data: 361455600
I1128 19:31:38.163372 30336 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I1128 19:31:38.163378 30336 net.cpp:84] Creating Layer Eltwise7_ReLU15_0_split
I1128 19:31:38.163380 30336 net.cpp:406] Eltwise7_ReLU15_0_split <- Eltwise7
I1128 19:31:38.163386 30336 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I1128 19:31:38.163393 30336 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I1128 19:31:38.163429 30336 net.cpp:122] Setting up Eltwise7_ReLU15_0_split
I1128 19:31:38.163434 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.163439 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.163441 30336 net.cpp:137] Memory required for data: 364466160
I1128 19:31:38.163444 30336 layer_factory.hpp:77] Creating layer Convolution18
I1128 19:31:38.163452 30336 net.cpp:84] Creating Layer Convolution18
I1128 19:31:38.163456 30336 net.cpp:406] Convolution18 <- Eltwise7_ReLU15_0_split_0
I1128 19:31:38.163463 30336 net.cpp:380] Convolution18 -> Convolution18
I1128 19:31:38.173764 30336 net.cpp:122] Setting up Convolution18
I1128 19:31:38.173781 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.173784 30336 net.cpp:137] Memory required for data: 365971440
I1128 19:31:38.173794 30336 layer_factory.hpp:77] Creating layer BatchNorm18
I1128 19:31:38.173802 30336 net.cpp:84] Creating Layer BatchNorm18
I1128 19:31:38.173806 30336 net.cpp:406] BatchNorm18 <- Convolution18
I1128 19:31:38.173813 30336 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I1128 19:31:38.173979 30336 net.cpp:122] Setting up BatchNorm18
I1128 19:31:38.173985 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.173987 30336 net.cpp:137] Memory required for data: 367476720
I1128 19:31:38.173995 30336 layer_factory.hpp:77] Creating layer Scale18
I1128 19:31:38.174001 30336 net.cpp:84] Creating Layer Scale18
I1128 19:31:38.174005 30336 net.cpp:406] Scale18 <- Convolution18
I1128 19:31:38.174010 30336 net.cpp:367] Scale18 -> Convolution18 (in-place)
I1128 19:31:38.174046 30336 layer_factory.hpp:77] Creating layer Scale18
I1128 19:31:38.174145 30336 net.cpp:122] Setting up Scale18
I1128 19:31:38.174151 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.174154 30336 net.cpp:137] Memory required for data: 368982000
I1128 19:31:38.174160 30336 layer_factory.hpp:77] Creating layer ReLU16
I1128 19:31:38.174176 30336 net.cpp:84] Creating Layer ReLU16
I1128 19:31:38.174180 30336 net.cpp:406] ReLU16 <- Convolution18
I1128 19:31:38.174185 30336 net.cpp:367] ReLU16 -> Convolution18 (in-place)
I1128 19:31:38.174533 30336 net.cpp:122] Setting up ReLU16
I1128 19:31:38.174541 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.174545 30336 net.cpp:137] Memory required for data: 370487280
I1128 19:31:38.174547 30336 layer_factory.hpp:77] Creating layer Convolution19
I1128 19:31:38.174557 30336 net.cpp:84] Creating Layer Convolution19
I1128 19:31:38.174561 30336 net.cpp:406] Convolution19 <- Convolution18
I1128 19:31:38.174568 30336 net.cpp:380] Convolution19 -> Convolution19
I1128 19:31:38.184765 30336 net.cpp:122] Setting up Convolution19
I1128 19:31:38.184782 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.184785 30336 net.cpp:137] Memory required for data: 371992560
I1128 19:31:38.184792 30336 layer_factory.hpp:77] Creating layer BatchNorm19
I1128 19:31:38.184801 30336 net.cpp:84] Creating Layer BatchNorm19
I1128 19:31:38.184806 30336 net.cpp:406] BatchNorm19 <- Convolution19
I1128 19:31:38.184813 30336 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I1128 19:31:38.184984 30336 net.cpp:122] Setting up BatchNorm19
I1128 19:31:38.184989 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.184993 30336 net.cpp:137] Memory required for data: 373497840
I1128 19:31:38.185009 30336 layer_factory.hpp:77] Creating layer Scale19
I1128 19:31:38.185015 30336 net.cpp:84] Creating Layer Scale19
I1128 19:31:38.185019 30336 net.cpp:406] Scale19 <- Convolution19
I1128 19:31:38.185024 30336 net.cpp:367] Scale19 -> Convolution19 (in-place)
I1128 19:31:38.185060 30336 layer_factory.hpp:77] Creating layer Scale19
I1128 19:31:38.185158 30336 net.cpp:122] Setting up Scale19
I1128 19:31:38.185164 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.185168 30336 net.cpp:137] Memory required for data: 375003120
I1128 19:31:38.185173 30336 layer_factory.hpp:77] Creating layer Eltwise8
I1128 19:31:38.185179 30336 net.cpp:84] Creating Layer Eltwise8
I1128 19:31:38.185184 30336 net.cpp:406] Eltwise8 <- Convolution19
I1128 19:31:38.185189 30336 net.cpp:406] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I1128 19:31:38.185194 30336 net.cpp:380] Eltwise8 -> Eltwise8
I1128 19:31:38.185217 30336 net.cpp:122] Setting up Eltwise8
I1128 19:31:38.185222 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.185225 30336 net.cpp:137] Memory required for data: 376508400
I1128 19:31:38.185228 30336 layer_factory.hpp:77] Creating layer ReLU17
I1128 19:31:38.185235 30336 net.cpp:84] Creating Layer ReLU17
I1128 19:31:38.185238 30336 net.cpp:406] ReLU17 <- Eltwise8
I1128 19:31:38.185243 30336 net.cpp:367] ReLU17 -> Eltwise8 (in-place)
I1128 19:31:38.185365 30336 net.cpp:122] Setting up ReLU17
I1128 19:31:38.185372 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.185375 30336 net.cpp:137] Memory required for data: 378013680
I1128 19:31:38.185377 30336 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I1128 19:31:38.185384 30336 net.cpp:84] Creating Layer Eltwise8_ReLU17_0_split
I1128 19:31:38.185387 30336 net.cpp:406] Eltwise8_ReLU17_0_split <- Eltwise8
I1128 19:31:38.185394 30336 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I1128 19:31:38.185400 30336 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I1128 19:31:38.185462 30336 net.cpp:122] Setting up Eltwise8_ReLU17_0_split
I1128 19:31:38.185468 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.185472 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.185475 30336 net.cpp:137] Memory required for data: 381024240
I1128 19:31:38.185477 30336 layer_factory.hpp:77] Creating layer Convolution20
I1128 19:31:38.185487 30336 net.cpp:84] Creating Layer Convolution20
I1128 19:31:38.185490 30336 net.cpp:406] Convolution20 <- Eltwise8_ReLU17_0_split_0
I1128 19:31:38.185519 30336 net.cpp:380] Convolution20 -> Convolution20
I1128 19:31:38.196123 30336 net.cpp:122] Setting up Convolution20
I1128 19:31:38.196158 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.196161 30336 net.cpp:137] Memory required for data: 382529520
I1128 19:31:38.196182 30336 layer_factory.hpp:77] Creating layer BatchNorm20
I1128 19:31:38.196192 30336 net.cpp:84] Creating Layer BatchNorm20
I1128 19:31:38.196197 30336 net.cpp:406] BatchNorm20 <- Convolution20
I1128 19:31:38.196204 30336 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I1128 19:31:38.196383 30336 net.cpp:122] Setting up BatchNorm20
I1128 19:31:38.196389 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.196393 30336 net.cpp:137] Memory required for data: 384034800
I1128 19:31:38.196413 30336 layer_factory.hpp:77] Creating layer Scale20
I1128 19:31:38.196419 30336 net.cpp:84] Creating Layer Scale20
I1128 19:31:38.196424 30336 net.cpp:406] Scale20 <- Convolution20
I1128 19:31:38.196429 30336 net.cpp:367] Scale20 -> Convolution20 (in-place)
I1128 19:31:38.196465 30336 layer_factory.hpp:77] Creating layer Scale20
I1128 19:31:38.196573 30336 net.cpp:122] Setting up Scale20
I1128 19:31:38.196578 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.196581 30336 net.cpp:137] Memory required for data: 385540080
I1128 19:31:38.196601 30336 layer_factory.hpp:77] Creating layer ReLU18
I1128 19:31:38.196609 30336 net.cpp:84] Creating Layer ReLU18
I1128 19:31:38.196611 30336 net.cpp:406] ReLU18 <- Convolution20
I1128 19:31:38.196616 30336 net.cpp:367] ReLU18 -> Convolution20 (in-place)
I1128 19:31:38.196740 30336 net.cpp:122] Setting up ReLU18
I1128 19:31:38.196748 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.196750 30336 net.cpp:137] Memory required for data: 387045360
I1128 19:31:38.196753 30336 layer_factory.hpp:77] Creating layer Convolution21
I1128 19:31:38.196763 30336 net.cpp:84] Creating Layer Convolution21
I1128 19:31:38.196765 30336 net.cpp:406] Convolution21 <- Convolution20
I1128 19:31:38.196772 30336 net.cpp:380] Convolution21 -> Convolution21
I1128 19:31:38.206940 30336 net.cpp:122] Setting up Convolution21
I1128 19:31:38.206956 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.206959 30336 net.cpp:137] Memory required for data: 388550640
I1128 19:31:38.206979 30336 layer_factory.hpp:77] Creating layer BatchNorm21
I1128 19:31:38.206990 30336 net.cpp:84] Creating Layer BatchNorm21
I1128 19:31:38.206995 30336 net.cpp:406] BatchNorm21 <- Convolution21
I1128 19:31:38.207000 30336 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I1128 19:31:38.207185 30336 net.cpp:122] Setting up BatchNorm21
I1128 19:31:38.207190 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.207193 30336 net.cpp:137] Memory required for data: 390055920
I1128 19:31:38.207212 30336 layer_factory.hpp:77] Creating layer Scale21
I1128 19:31:38.207218 30336 net.cpp:84] Creating Layer Scale21
I1128 19:31:38.207223 30336 net.cpp:406] Scale21 <- Convolution21
I1128 19:31:38.207229 30336 net.cpp:367] Scale21 -> Convolution21 (in-place)
I1128 19:31:38.207267 30336 layer_factory.hpp:77] Creating layer Scale21
I1128 19:31:38.207365 30336 net.cpp:122] Setting up Scale21
I1128 19:31:38.207371 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.207375 30336 net.cpp:137] Memory required for data: 391561200
I1128 19:31:38.207379 30336 layer_factory.hpp:77] Creating layer Eltwise9
I1128 19:31:38.207386 30336 net.cpp:84] Creating Layer Eltwise9
I1128 19:31:38.207389 30336 net.cpp:406] Eltwise9 <- Convolution21
I1128 19:31:38.207393 30336 net.cpp:406] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I1128 19:31:38.207399 30336 net.cpp:380] Eltwise9 -> Eltwise9
I1128 19:31:38.207422 30336 net.cpp:122] Setting up Eltwise9
I1128 19:31:38.207428 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.207432 30336 net.cpp:137] Memory required for data: 393066480
I1128 19:31:38.207434 30336 layer_factory.hpp:77] Creating layer ReLU19
I1128 19:31:38.207438 30336 net.cpp:84] Creating Layer ReLU19
I1128 19:31:38.207443 30336 net.cpp:406] ReLU19 <- Eltwise9
I1128 19:31:38.207449 30336 net.cpp:367] ReLU19 -> Eltwise9 (in-place)
I1128 19:31:38.207582 30336 net.cpp:122] Setting up ReLU19
I1128 19:31:38.207588 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.207592 30336 net.cpp:137] Memory required for data: 394571760
I1128 19:31:38.207594 30336 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
I1128 19:31:38.207600 30336 net.cpp:84] Creating Layer Eltwise9_ReLU19_0_split
I1128 19:31:38.207604 30336 net.cpp:406] Eltwise9_ReLU19_0_split <- Eltwise9
I1128 19:31:38.207610 30336 net.cpp:380] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
I1128 19:31:38.207643 30336 net.cpp:380] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
I1128 19:31:38.207697 30336 net.cpp:122] Setting up Eltwise9_ReLU19_0_split
I1128 19:31:38.207703 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.207707 30336 net.cpp:129] Top shape: 20 384 7 7 (376320)
I1128 19:31:38.207710 30336 net.cpp:137] Memory required for data: 397582320
I1128 19:31:38.207715 30336 layer_factory.hpp:77] Creating layer Convolution22
I1128 19:31:38.207722 30336 net.cpp:84] Creating Layer Convolution22
I1128 19:31:38.207726 30336 net.cpp:406] Convolution22 <- Eltwise9_ReLU19_0_split_0
I1128 19:31:38.207731 30336 net.cpp:380] Convolution22 -> Convolution22
I1128 19:31:38.217929 30336 net.cpp:122] Setting up Convolution22
I1128 19:31:38.217947 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.217949 30336 net.cpp:137] Memory required for data: 398073840
I1128 19:31:38.217972 30336 layer_factory.hpp:77] Creating layer BatchNorm22
I1128 19:31:38.217980 30336 net.cpp:84] Creating Layer BatchNorm22
I1128 19:31:38.217985 30336 net.cpp:406] BatchNorm22 <- Convolution22
I1128 19:31:38.217993 30336 net.cpp:367] BatchNorm22 -> Convolution22 (in-place)
I1128 19:31:38.218163 30336 net.cpp:122] Setting up BatchNorm22
I1128 19:31:38.218169 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.218173 30336 net.cpp:137] Memory required for data: 398565360
I1128 19:31:38.218178 30336 layer_factory.hpp:77] Creating layer Scale22
I1128 19:31:38.218185 30336 net.cpp:84] Creating Layer Scale22
I1128 19:31:38.218191 30336 net.cpp:406] Scale22 <- Convolution22
I1128 19:31:38.218195 30336 net.cpp:367] Scale22 -> Convolution22 (in-place)
I1128 19:31:38.218243 30336 layer_factory.hpp:77] Creating layer Scale22
I1128 19:31:38.218365 30336 net.cpp:122] Setting up Scale22
I1128 19:31:38.218371 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.218374 30336 net.cpp:137] Memory required for data: 399056880
I1128 19:31:38.218394 30336 layer_factory.hpp:77] Creating layer ReLU20
I1128 19:31:38.218400 30336 net.cpp:84] Creating Layer ReLU20
I1128 19:31:38.218403 30336 net.cpp:406] ReLU20 <- Convolution22
I1128 19:31:38.218410 30336 net.cpp:367] ReLU20 -> Convolution22 (in-place)
I1128 19:31:38.218538 30336 net.cpp:122] Setting up ReLU20
I1128 19:31:38.218545 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.218549 30336 net.cpp:137] Memory required for data: 399548400
I1128 19:31:38.218552 30336 layer_factory.hpp:77] Creating layer Convolution23
I1128 19:31:38.218562 30336 net.cpp:84] Creating Layer Convolution23
I1128 19:31:38.218565 30336 net.cpp:406] Convolution23 <- Convolution22
I1128 19:31:38.218571 30336 net.cpp:380] Convolution23 -> Convolution23
I1128 19:31:38.229138 30336 net.cpp:122] Setting up Convolution23
I1128 19:31:38.229158 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.229161 30336 net.cpp:137] Memory required for data: 400039920
I1128 19:31:38.229183 30336 layer_factory.hpp:77] Creating layer BatchNorm23
I1128 19:31:38.229192 30336 net.cpp:84] Creating Layer BatchNorm23
I1128 19:31:38.229197 30336 net.cpp:406] BatchNorm23 <- Convolution23
I1128 19:31:38.229205 30336 net.cpp:367] BatchNorm23 -> Convolution23 (in-place)
I1128 19:31:38.229387 30336 net.cpp:122] Setting up BatchNorm23
I1128 19:31:38.229393 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.229396 30336 net.cpp:137] Memory required for data: 400531440
I1128 19:31:38.229416 30336 layer_factory.hpp:77] Creating layer Scale23
I1128 19:31:38.229432 30336 net.cpp:84] Creating Layer Scale23
I1128 19:31:38.229436 30336 net.cpp:406] Scale23 <- Convolution23
I1128 19:31:38.229442 30336 net.cpp:367] Scale23 -> Convolution23 (in-place)
I1128 19:31:38.229480 30336 layer_factory.hpp:77] Creating layer Scale23
I1128 19:31:38.229578 30336 net.cpp:122] Setting up Scale23
I1128 19:31:38.229583 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.229598 30336 net.cpp:137] Memory required for data: 401022960
I1128 19:31:38.229604 30336 layer_factory.hpp:77] Creating layer Convolution24
I1128 19:31:38.229627 30336 net.cpp:84] Creating Layer Convolution24
I1128 19:31:38.229631 30336 net.cpp:406] Convolution24 <- Eltwise9_ReLU19_0_split_1
I1128 19:31:38.229637 30336 net.cpp:380] Convolution24 -> Convolution24
I1128 19:31:38.231348 30336 net.cpp:122] Setting up Convolution24
I1128 19:31:38.231359 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.231361 30336 net.cpp:137] Memory required for data: 401514480
I1128 19:31:38.231366 30336 layer_factory.hpp:77] Creating layer BatchNorm24
I1128 19:31:38.231374 30336 net.cpp:84] Creating Layer BatchNorm24
I1128 19:31:38.231379 30336 net.cpp:406] BatchNorm24 <- Convolution24
I1128 19:31:38.231384 30336 net.cpp:367] BatchNorm24 -> Convolution24 (in-place)
I1128 19:31:38.231549 30336 net.cpp:122] Setting up BatchNorm24
I1128 19:31:38.231554 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.231557 30336 net.cpp:137] Memory required for data: 402006000
I1128 19:31:38.231564 30336 layer_factory.hpp:77] Creating layer Scale24
I1128 19:31:38.231568 30336 net.cpp:84] Creating Layer Scale24
I1128 19:31:38.231572 30336 net.cpp:406] Scale24 <- Convolution24
I1128 19:31:38.231578 30336 net.cpp:367] Scale24 -> Convolution24 (in-place)
I1128 19:31:38.231611 30336 layer_factory.hpp:77] Creating layer Scale24
I1128 19:31:38.231753 30336 net.cpp:122] Setting up Scale24
I1128 19:31:38.231760 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.231763 30336 net.cpp:137] Memory required for data: 402497520
I1128 19:31:38.231770 30336 layer_factory.hpp:77] Creating layer Eltwise10
I1128 19:31:38.231776 30336 net.cpp:84] Creating Layer Eltwise10
I1128 19:31:38.231779 30336 net.cpp:406] Eltwise10 <- Convolution23
I1128 19:31:38.231784 30336 net.cpp:406] Eltwise10 <- Convolution24
I1128 19:31:38.231791 30336 net.cpp:380] Eltwise10 -> Eltwise10
I1128 19:31:38.231812 30336 net.cpp:122] Setting up Eltwise10
I1128 19:31:38.231817 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.231822 30336 net.cpp:137] Memory required for data: 402989040
I1128 19:31:38.231824 30336 layer_factory.hpp:77] Creating layer ReLU21
I1128 19:31:38.231830 30336 net.cpp:84] Creating Layer ReLU21
I1128 19:31:38.231834 30336 net.cpp:406] ReLU21 <- Eltwise10
I1128 19:31:38.231838 30336 net.cpp:367] ReLU21 -> Eltwise10 (in-place)
I1128 19:31:38.231961 30336 net.cpp:122] Setting up ReLU21
I1128 19:31:38.231966 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.231969 30336 net.cpp:137] Memory required for data: 403480560
I1128 19:31:38.231972 30336 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
I1128 19:31:38.231978 30336 net.cpp:84] Creating Layer Eltwise10_ReLU21_0_split
I1128 19:31:38.231982 30336 net.cpp:406] Eltwise10_ReLU21_0_split <- Eltwise10
I1128 19:31:38.231987 30336 net.cpp:380] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
I1128 19:31:38.231994 30336 net.cpp:380] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
I1128 19:31:38.232030 30336 net.cpp:122] Setting up Eltwise10_ReLU21_0_split
I1128 19:31:38.232035 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.232039 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.232043 30336 net.cpp:137] Memory required for data: 404463600
I1128 19:31:38.232046 30336 layer_factory.hpp:77] Creating layer Convolution25
I1128 19:31:38.232054 30336 net.cpp:84] Creating Layer Convolution25
I1128 19:31:38.232058 30336 net.cpp:406] Convolution25 <- Eltwise10_ReLU21_0_split_0
I1128 19:31:38.232070 30336 net.cpp:380] Convolution25 -> Convolution25
I1128 19:31:38.242374 30336 net.cpp:122] Setting up Convolution25
I1128 19:31:38.242391 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.242394 30336 net.cpp:137] Memory required for data: 404955120
I1128 19:31:38.242403 30336 layer_factory.hpp:77] Creating layer BatchNorm25
I1128 19:31:38.242410 30336 net.cpp:84] Creating Layer BatchNorm25
I1128 19:31:38.242414 30336 net.cpp:406] BatchNorm25 <- Convolution25
I1128 19:31:38.242422 30336 net.cpp:367] BatchNorm25 -> Convolution25 (in-place)
I1128 19:31:38.242633 30336 net.cpp:122] Setting up BatchNorm25
I1128 19:31:38.242640 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.242642 30336 net.cpp:137] Memory required for data: 405446640
I1128 19:31:38.242648 30336 layer_factory.hpp:77] Creating layer Scale25
I1128 19:31:38.242655 30336 net.cpp:84] Creating Layer Scale25
I1128 19:31:38.242658 30336 net.cpp:406] Scale25 <- Convolution25
I1128 19:31:38.242666 30336 net.cpp:367] Scale25 -> Convolution25 (in-place)
I1128 19:31:38.242735 30336 layer_factory.hpp:77] Creating layer Scale25
I1128 19:31:38.242832 30336 net.cpp:122] Setting up Scale25
I1128 19:31:38.242839 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.242842 30336 net.cpp:137] Memory required for data: 405938160
I1128 19:31:38.242847 30336 layer_factory.hpp:77] Creating layer ReLU22
I1128 19:31:38.242853 30336 net.cpp:84] Creating Layer ReLU22
I1128 19:31:38.242857 30336 net.cpp:406] ReLU22 <- Convolution25
I1128 19:31:38.242861 30336 net.cpp:367] ReLU22 -> Convolution25 (in-place)
I1128 19:31:38.243228 30336 net.cpp:122] Setting up ReLU22
I1128 19:31:38.243237 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.243239 30336 net.cpp:137] Memory required for data: 406429680
I1128 19:31:38.243242 30336 layer_factory.hpp:77] Creating layer Convolution26
I1128 19:31:38.243252 30336 net.cpp:84] Creating Layer Convolution26
I1128 19:31:38.243255 30336 net.cpp:406] Convolution26 <- Convolution25
I1128 19:31:38.243263 30336 net.cpp:380] Convolution26 -> Convolution26
I1128 19:31:38.253139 30336 net.cpp:122] Setting up Convolution26
I1128 19:31:38.253155 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.253159 30336 net.cpp:137] Memory required for data: 406921200
I1128 19:31:38.253180 30336 layer_factory.hpp:77] Creating layer BatchNorm26
I1128 19:31:38.253188 30336 net.cpp:84] Creating Layer BatchNorm26
I1128 19:31:38.253193 30336 net.cpp:406] BatchNorm26 <- Convolution26
I1128 19:31:38.253201 30336 net.cpp:367] BatchNorm26 -> Convolution26 (in-place)
I1128 19:31:38.253373 30336 net.cpp:122] Setting up BatchNorm26
I1128 19:31:38.253379 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.253382 30336 net.cpp:137] Memory required for data: 407412720
I1128 19:31:38.253388 30336 layer_factory.hpp:77] Creating layer Scale26
I1128 19:31:38.253396 30336 net.cpp:84] Creating Layer Scale26
I1128 19:31:38.253399 30336 net.cpp:406] Scale26 <- Convolution26
I1128 19:31:38.253404 30336 net.cpp:367] Scale26 -> Convolution26 (in-place)
I1128 19:31:38.253440 30336 layer_factory.hpp:77] Creating layer Scale26
I1128 19:31:38.253538 30336 net.cpp:122] Setting up Scale26
I1128 19:31:38.253545 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.253547 30336 net.cpp:137] Memory required for data: 407904240
I1128 19:31:38.253552 30336 layer_factory.hpp:77] Creating layer Eltwise11
I1128 19:31:38.253559 30336 net.cpp:84] Creating Layer Eltwise11
I1128 19:31:38.253562 30336 net.cpp:406] Eltwise11 <- Convolution26
I1128 19:31:38.253567 30336 net.cpp:406] Eltwise11 <- Eltwise10_ReLU21_0_split_1
I1128 19:31:38.253572 30336 net.cpp:380] Eltwise11 -> Eltwise11
I1128 19:31:38.253595 30336 net.cpp:122] Setting up Eltwise11
I1128 19:31:38.253602 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.253604 30336 net.cpp:137] Memory required for data: 408395760
I1128 19:31:38.253607 30336 layer_factory.hpp:77] Creating layer ReLU23
I1128 19:31:38.253613 30336 net.cpp:84] Creating Layer ReLU23
I1128 19:31:38.253628 30336 net.cpp:406] ReLU23 <- Eltwise11
I1128 19:31:38.253633 30336 net.cpp:367] ReLU23 -> Eltwise11 (in-place)
I1128 19:31:38.254082 30336 net.cpp:122] Setting up ReLU23
I1128 19:31:38.254091 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.254096 30336 net.cpp:137] Memory required for data: 408887280
I1128 19:31:38.254098 30336 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
I1128 19:31:38.254104 30336 net.cpp:84] Creating Layer Eltwise11_ReLU23_0_split
I1128 19:31:38.254108 30336 net.cpp:406] Eltwise11_ReLU23_0_split <- Eltwise11
I1128 19:31:38.254114 30336 net.cpp:380] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
I1128 19:31:38.254122 30336 net.cpp:380] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
I1128 19:31:38.254168 30336 net.cpp:122] Setting up Eltwise11_ReLU23_0_split
I1128 19:31:38.254174 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.254179 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.254182 30336 net.cpp:137] Memory required for data: 409870320
I1128 19:31:38.254185 30336 layer_factory.hpp:77] Creating layer Convolution27
I1128 19:31:38.254195 30336 net.cpp:84] Creating Layer Convolution27
I1128 19:31:38.254199 30336 net.cpp:406] Convolution27 <- Eltwise11_ReLU23_0_split_0
I1128 19:31:38.254204 30336 net.cpp:380] Convolution27 -> Convolution27
I1128 19:31:38.264698 30336 net.cpp:122] Setting up Convolution27
I1128 19:31:38.264717 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.264721 30336 net.cpp:137] Memory required for data: 410361840
I1128 19:31:38.264729 30336 layer_factory.hpp:77] Creating layer BatchNorm27
I1128 19:31:38.264737 30336 net.cpp:84] Creating Layer BatchNorm27
I1128 19:31:38.264741 30336 net.cpp:406] BatchNorm27 <- Convolution27
I1128 19:31:38.264750 30336 net.cpp:367] BatchNorm27 -> Convolution27 (in-place)
I1128 19:31:38.264956 30336 net.cpp:122] Setting up BatchNorm27
I1128 19:31:38.264961 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.264964 30336 net.cpp:137] Memory required for data: 410853360
I1128 19:31:38.264971 30336 layer_factory.hpp:77] Creating layer Scale27
I1128 19:31:38.264988 30336 net.cpp:84] Creating Layer Scale27
I1128 19:31:38.264992 30336 net.cpp:406] Scale27 <- Convolution27
I1128 19:31:38.265012 30336 net.cpp:367] Scale27 -> Convolution27 (in-place)
I1128 19:31:38.265063 30336 layer_factory.hpp:77] Creating layer Scale27
I1128 19:31:38.265193 30336 net.cpp:122] Setting up Scale27
I1128 19:31:38.265198 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.265202 30336 net.cpp:137] Memory required for data: 411344880
I1128 19:31:38.265221 30336 layer_factory.hpp:77] Creating layer ReLU24
I1128 19:31:38.265228 30336 net.cpp:84] Creating Layer ReLU24
I1128 19:31:38.265230 30336 net.cpp:406] ReLU24 <- Convolution27
I1128 19:31:38.265236 30336 net.cpp:367] ReLU24 -> Convolution27 (in-place)
I1128 19:31:38.265375 30336 net.cpp:122] Setting up ReLU24
I1128 19:31:38.265383 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.265384 30336 net.cpp:137] Memory required for data: 411836400
I1128 19:31:38.265388 30336 layer_factory.hpp:77] Creating layer Convolution28
I1128 19:31:38.265398 30336 net.cpp:84] Creating Layer Convolution28
I1128 19:31:38.265401 30336 net.cpp:406] Convolution28 <- Convolution27
I1128 19:31:38.265408 30336 net.cpp:380] Convolution28 -> Convolution28
I1128 19:31:38.275537 30336 net.cpp:122] Setting up Convolution28
I1128 19:31:38.275555 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.275558 30336 net.cpp:137] Memory required for data: 412327920
I1128 19:31:38.275580 30336 layer_factory.hpp:77] Creating layer BatchNorm28
I1128 19:31:38.275589 30336 net.cpp:84] Creating Layer BatchNorm28
I1128 19:31:38.275595 30336 net.cpp:406] BatchNorm28 <- Convolution28
I1128 19:31:38.275604 30336 net.cpp:367] BatchNorm28 -> Convolution28 (in-place)
I1128 19:31:38.275784 30336 net.cpp:122] Setting up BatchNorm28
I1128 19:31:38.275789 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.275802 30336 net.cpp:137] Memory required for data: 412819440
I1128 19:31:38.275810 30336 layer_factory.hpp:77] Creating layer Scale28
I1128 19:31:38.275818 30336 net.cpp:84] Creating Layer Scale28
I1128 19:31:38.275822 30336 net.cpp:406] Scale28 <- Convolution28
I1128 19:31:38.275827 30336 net.cpp:367] Scale28 -> Convolution28 (in-place)
I1128 19:31:38.275864 30336 layer_factory.hpp:77] Creating layer Scale28
I1128 19:31:38.275964 30336 net.cpp:122] Setting up Scale28
I1128 19:31:38.275969 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.275974 30336 net.cpp:137] Memory required for data: 413310960
I1128 19:31:38.275979 30336 layer_factory.hpp:77] Creating layer Eltwise12
I1128 19:31:38.275986 30336 net.cpp:84] Creating Layer Eltwise12
I1128 19:31:38.275990 30336 net.cpp:406] Eltwise12 <- Convolution28
I1128 19:31:38.275995 30336 net.cpp:406] Eltwise12 <- Eltwise11_ReLU23_0_split_1
I1128 19:31:38.276001 30336 net.cpp:380] Eltwise12 -> Eltwise12
I1128 19:31:38.276026 30336 net.cpp:122] Setting up Eltwise12
I1128 19:31:38.276031 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.276034 30336 net.cpp:137] Memory required for data: 413802480
I1128 19:31:38.276038 30336 layer_factory.hpp:77] Creating layer ReLU25
I1128 19:31:38.276043 30336 net.cpp:84] Creating Layer ReLU25
I1128 19:31:38.276046 30336 net.cpp:406] ReLU25 <- Eltwise12
I1128 19:31:38.276052 30336 net.cpp:367] ReLU25 -> Eltwise12 (in-place)
I1128 19:31:38.276176 30336 net.cpp:122] Setting up ReLU25
I1128 19:31:38.276183 30336 net.cpp:129] Top shape: 20 384 4 4 (122880)
I1128 19:31:38.276186 30336 net.cpp:137] Memory required for data: 414294000
I1128 19:31:38.276190 30336 layer_factory.hpp:77] Creating layer Pooling1
I1128 19:31:38.276195 30336 net.cpp:84] Creating Layer Pooling1
I1128 19:31:38.276198 30336 net.cpp:406] Pooling1 <- Eltwise12
I1128 19:31:38.276206 30336 net.cpp:380] Pooling1 -> Pooling1
I1128 19:31:38.276372 30336 net.cpp:122] Setting up Pooling1
I1128 19:31:38.276379 30336 net.cpp:129] Top shape: 20 384 1 1 (7680)
I1128 19:31:38.276382 30336 net.cpp:137] Memory required for data: 414324720
I1128 19:31:38.276386 30336 layer_factory.hpp:77] Creating layer InnerProduct1
I1128 19:31:38.276391 30336 net.cpp:84] Creating Layer InnerProduct1
I1128 19:31:38.276394 30336 net.cpp:406] InnerProduct1 <- Pooling1
I1128 19:31:38.276401 30336 net.cpp:380] InnerProduct1 -> InnerProduct1
I1128 19:31:38.276522 30336 net.cpp:122] Setting up InnerProduct1
I1128 19:31:38.276527 30336 net.cpp:129] Top shape: 20 10 (200)
I1128 19:31:38.276530 30336 net.cpp:137] Memory required for data: 414325520
I1128 19:31:38.276536 30336 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I1128 19:31:38.276541 30336 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
I1128 19:31:38.276545 30336 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I1128 19:31:38.276551 30336 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I1128 19:31:38.276558 30336 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I1128 19:31:38.276595 30336 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
I1128 19:31:38.276600 30336 net.cpp:129] Top shape: 20 10 (200)
I1128 19:31:38.276603 30336 net.cpp:129] Top shape: 20 10 (200)
I1128 19:31:38.276607 30336 net.cpp:137] Memory required for data: 414327120
I1128 19:31:38.276609 30336 layer_factory.hpp:77] Creating layer Accuracy1
I1128 19:31:38.276614 30336 net.cpp:84] Creating Layer Accuracy1
I1128 19:31:38.276618 30336 net.cpp:406] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_0
I1128 19:31:38.276623 30336 net.cpp:406] Accuracy1 <- Data2_Data1_1_split_0
I1128 19:31:38.276629 30336 net.cpp:380] Accuracy1 -> Accuracy1
I1128 19:31:38.276659 30336 net.cpp:122] Setting up Accuracy1
I1128 19:31:38.276664 30336 net.cpp:129] Top shape: (1)
I1128 19:31:38.276669 30336 net.cpp:137] Memory required for data: 414327124
I1128 19:31:38.276671 30336 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1128 19:31:38.276682 30336 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1128 19:31:38.276686 30336 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_1
I1128 19:31:38.276703 30336 net.cpp:406] SoftmaxWithLoss1 <- Data2_Data1_1_split_1
I1128 19:31:38.276710 30336 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1128 19:31:38.276717 30336 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1128 19:31:38.277180 30336 net.cpp:122] Setting up SoftmaxWithLoss1
I1128 19:31:38.277189 30336 net.cpp:129] Top shape: (1)
I1128 19:31:38.277191 30336 net.cpp:132]     with loss weight 1
I1128 19:31:38.277200 30336 net.cpp:137] Memory required for data: 414327128
I1128 19:31:38.277204 30336 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1128 19:31:38.277209 30336 net.cpp:200] Accuracy1 does not need backward computation.
I1128 19:31:38.277215 30336 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
I1128 19:31:38.277217 30336 net.cpp:198] InnerProduct1 needs backward computation.
I1128 19:31:38.277220 30336 net.cpp:198] Pooling1 needs backward computation.
I1128 19:31:38.277225 30336 net.cpp:198] ReLU25 needs backward computation.
I1128 19:31:38.277227 30336 net.cpp:198] Eltwise12 needs backward computation.
I1128 19:31:38.277230 30336 net.cpp:198] Scale28 needs backward computation.
I1128 19:31:38.277256 30336 net.cpp:198] BatchNorm28 needs backward computation.
I1128 19:31:38.277259 30336 net.cpp:198] Convolution28 needs backward computation.
I1128 19:31:38.277276 30336 net.cpp:198] ReLU24 needs backward computation.
I1128 19:31:38.277279 30336 net.cpp:198] Scale27 needs backward computation.
I1128 19:31:38.277282 30336 net.cpp:198] BatchNorm27 needs backward computation.
I1128 19:31:38.277287 30336 net.cpp:198] Convolution27 needs backward computation.
I1128 19:31:38.277290 30336 net.cpp:198] Eltwise11_ReLU23_0_split needs backward computation.
I1128 19:31:38.277294 30336 net.cpp:198] ReLU23 needs backward computation.
I1128 19:31:38.277297 30336 net.cpp:198] Eltwise11 needs backward computation.
I1128 19:31:38.277302 30336 net.cpp:198] Scale26 needs backward computation.
I1128 19:31:38.277305 30336 net.cpp:198] BatchNorm26 needs backward computation.
I1128 19:31:38.277309 30336 net.cpp:198] Convolution26 needs backward computation.
I1128 19:31:38.277312 30336 net.cpp:198] ReLU22 needs backward computation.
I1128 19:31:38.277315 30336 net.cpp:198] Scale25 needs backward computation.
I1128 19:31:38.277318 30336 net.cpp:198] BatchNorm25 needs backward computation.
I1128 19:31:38.277320 30336 net.cpp:198] Convolution25 needs backward computation.
I1128 19:31:38.277323 30336 net.cpp:198] Eltwise10_ReLU21_0_split needs backward computation.
I1128 19:31:38.277328 30336 net.cpp:198] ReLU21 needs backward computation.
I1128 19:31:38.277330 30336 net.cpp:198] Eltwise10 needs backward computation.
I1128 19:31:38.277334 30336 net.cpp:198] Scale24 needs backward computation.
I1128 19:31:38.277336 30336 net.cpp:198] BatchNorm24 needs backward computation.
I1128 19:31:38.277339 30336 net.cpp:198] Convolution24 needs backward computation.
I1128 19:31:38.277344 30336 net.cpp:198] Scale23 needs backward computation.
I1128 19:31:38.277348 30336 net.cpp:198] BatchNorm23 needs backward computation.
I1128 19:31:38.277349 30336 net.cpp:198] Convolution23 needs backward computation.
I1128 19:31:38.277354 30336 net.cpp:198] ReLU20 needs backward computation.
I1128 19:31:38.277355 30336 net.cpp:198] Scale22 needs backward computation.
I1128 19:31:38.277359 30336 net.cpp:198] BatchNorm22 needs backward computation.
I1128 19:31:38.277361 30336 net.cpp:198] Convolution22 needs backward computation.
I1128 19:31:38.277365 30336 net.cpp:198] Eltwise9_ReLU19_0_split needs backward computation.
I1128 19:31:38.277369 30336 net.cpp:198] ReLU19 needs backward computation.
I1128 19:31:38.277371 30336 net.cpp:198] Eltwise9 needs backward computation.
I1128 19:31:38.277374 30336 net.cpp:198] Scale21 needs backward computation.
I1128 19:31:38.277377 30336 net.cpp:198] BatchNorm21 needs backward computation.
I1128 19:31:38.277387 30336 net.cpp:198] Convolution21 needs backward computation.
I1128 19:31:38.277390 30336 net.cpp:198] ReLU18 needs backward computation.
I1128 19:31:38.277393 30336 net.cpp:198] Scale20 needs backward computation.
I1128 19:31:38.277396 30336 net.cpp:198] BatchNorm20 needs backward computation.
I1128 19:31:38.277400 30336 net.cpp:198] Convolution20 needs backward computation.
I1128 19:31:38.277402 30336 net.cpp:198] Eltwise8_ReLU17_0_split needs backward computation.
I1128 19:31:38.277405 30336 net.cpp:198] ReLU17 needs backward computation.
I1128 19:31:38.277410 30336 net.cpp:198] Eltwise8 needs backward computation.
I1128 19:31:38.277412 30336 net.cpp:198] Scale19 needs backward computation.
I1128 19:31:38.277415 30336 net.cpp:198] BatchNorm19 needs backward computation.
I1128 19:31:38.277417 30336 net.cpp:198] Convolution19 needs backward computation.
I1128 19:31:38.277421 30336 net.cpp:198] ReLU16 needs backward computation.
I1128 19:31:38.277425 30336 net.cpp:198] Scale18 needs backward computation.
I1128 19:31:38.277427 30336 net.cpp:198] BatchNorm18 needs backward computation.
I1128 19:31:38.277429 30336 net.cpp:198] Convolution18 needs backward computation.
I1128 19:31:38.277433 30336 net.cpp:198] Eltwise7_ReLU15_0_split needs backward computation.
I1128 19:31:38.277436 30336 net.cpp:198] ReLU15 needs backward computation.
I1128 19:31:38.277439 30336 net.cpp:198] Eltwise7 needs backward computation.
I1128 19:31:38.277443 30336 net.cpp:198] Scale17 needs backward computation.
I1128 19:31:38.277446 30336 net.cpp:198] BatchNorm17 needs backward computation.
I1128 19:31:38.277449 30336 net.cpp:198] Convolution17 needs backward computation.
I1128 19:31:38.277452 30336 net.cpp:198] Scale16 needs backward computation.
I1128 19:31:38.277456 30336 net.cpp:198] BatchNorm16 needs backward computation.
I1128 19:31:38.277458 30336 net.cpp:198] Convolution16 needs backward computation.
I1128 19:31:38.277462 30336 net.cpp:198] ReLU14 needs backward computation.
I1128 19:31:38.277464 30336 net.cpp:198] Scale15 needs backward computation.
I1128 19:31:38.277467 30336 net.cpp:198] BatchNorm15 needs backward computation.
I1128 19:31:38.277470 30336 net.cpp:198] Convolution15 needs backward computation.
I1128 19:31:38.277473 30336 net.cpp:198] Eltwise6_ReLU13_0_split needs backward computation.
I1128 19:31:38.277477 30336 net.cpp:198] ReLU13 needs backward computation.
I1128 19:31:38.277480 30336 net.cpp:198] Eltwise6 needs backward computation.
I1128 19:31:38.277485 30336 net.cpp:198] Scale14 needs backward computation.
I1128 19:31:38.277488 30336 net.cpp:198] BatchNorm14 needs backward computation.
I1128 19:31:38.277492 30336 net.cpp:198] Convolution14 needs backward computation.
I1128 19:31:38.277494 30336 net.cpp:198] ReLU12 needs backward computation.
I1128 19:31:38.277498 30336 net.cpp:198] Scale13 needs backward computation.
I1128 19:31:38.277500 30336 net.cpp:198] BatchNorm13 needs backward computation.
I1128 19:31:38.277503 30336 net.cpp:198] Convolution13 needs backward computation.
I1128 19:31:38.277506 30336 net.cpp:198] Eltwise5_ReLU11_0_split needs backward computation.
I1128 19:31:38.277510 30336 net.cpp:198] ReLU11 needs backward computation.
I1128 19:31:38.277513 30336 net.cpp:198] Eltwise5 needs backward computation.
I1128 19:31:38.277516 30336 net.cpp:198] Scale12 needs backward computation.
I1128 19:31:38.277520 30336 net.cpp:198] BatchNorm12 needs backward computation.
I1128 19:31:38.277524 30336 net.cpp:198] Convolution12 needs backward computation.
I1128 19:31:38.277528 30336 net.cpp:198] ReLU10 needs backward computation.
I1128 19:31:38.277529 30336 net.cpp:198] Scale11 needs backward computation.
I1128 19:31:38.277532 30336 net.cpp:198] BatchNorm11 needs backward computation.
I1128 19:31:38.277536 30336 net.cpp:198] Convolution11 needs backward computation.
I1128 19:31:38.277539 30336 net.cpp:198] Eltwise4_ReLU9_0_split needs backward computation.
I1128 19:31:38.277542 30336 net.cpp:198] ReLU9 needs backward computation.
I1128 19:31:38.277545 30336 net.cpp:198] Eltwise4 needs backward computation.
I1128 19:31:38.277554 30336 net.cpp:198] Scale10 needs backward computation.
I1128 19:31:38.277557 30336 net.cpp:198] BatchNorm10 needs backward computation.
I1128 19:31:38.277560 30336 net.cpp:198] Convolution10 needs backward computation.
I1128 19:31:38.277564 30336 net.cpp:198] Scale9 needs backward computation.
I1128 19:31:38.277567 30336 net.cpp:198] BatchNorm9 needs backward computation.
I1128 19:31:38.277571 30336 net.cpp:198] Convolution9 needs backward computation.
I1128 19:31:38.277575 30336 net.cpp:198] ReLU8 needs backward computation.
I1128 19:31:38.277577 30336 net.cpp:198] Scale8 needs backward computation.
I1128 19:31:38.277581 30336 net.cpp:198] BatchNorm8 needs backward computation.
I1128 19:31:38.277585 30336 net.cpp:198] Convolution8 needs backward computation.
I1128 19:31:38.277587 30336 net.cpp:198] Eltwise3_ReLU7_0_split needs backward computation.
I1128 19:31:38.277590 30336 net.cpp:198] ReLU7 needs backward computation.
I1128 19:31:38.277595 30336 net.cpp:198] Eltwise3 needs backward computation.
I1128 19:31:38.277597 30336 net.cpp:198] Scale7 needs backward computation.
I1128 19:31:38.277601 30336 net.cpp:198] BatchNorm7 needs backward computation.
I1128 19:31:38.277603 30336 net.cpp:198] Convolution7 needs backward computation.
I1128 19:31:38.277606 30336 net.cpp:198] ReLU6 needs backward computation.
I1128 19:31:38.277609 30336 net.cpp:198] Scale6 needs backward computation.
I1128 19:31:38.277612 30336 net.cpp:198] BatchNorm6 needs backward computation.
I1128 19:31:38.277614 30336 net.cpp:198] Convolution6 needs backward computation.
I1128 19:31:38.277618 30336 net.cpp:198] Eltwise2_ReLU5_0_split needs backward computation.
I1128 19:31:38.277622 30336 net.cpp:198] ReLU5 needs backward computation.
I1128 19:31:38.277624 30336 net.cpp:198] Eltwise2 needs backward computation.
I1128 19:31:38.277629 30336 net.cpp:198] Scale5 needs backward computation.
I1128 19:31:38.277634 30336 net.cpp:198] BatchNorm5 needs backward computation.
I1128 19:31:38.277637 30336 net.cpp:198] Convolution5 needs backward computation.
I1128 19:31:38.277640 30336 net.cpp:198] ReLU4 needs backward computation.
I1128 19:31:38.277643 30336 net.cpp:198] Scale4 needs backward computation.
I1128 19:31:38.277647 30336 net.cpp:198] BatchNorm4 needs backward computation.
I1128 19:31:38.277649 30336 net.cpp:198] Convolution4 needs backward computation.
I1128 19:31:38.277652 30336 net.cpp:198] Eltwise1_ReLU3_0_split needs backward computation.
I1128 19:31:38.277655 30336 net.cpp:198] ReLU3 needs backward computation.
I1128 19:31:38.277659 30336 net.cpp:198] Eltwise1 needs backward computation.
I1128 19:31:38.277662 30336 net.cpp:198] Scale3 needs backward computation.
I1128 19:31:38.277665 30336 net.cpp:198] BatchNorm3 needs backward computation.
I1128 19:31:38.277668 30336 net.cpp:198] Convolution3 needs backward computation.
I1128 19:31:38.277671 30336 net.cpp:198] ReLU2 needs backward computation.
I1128 19:31:38.277674 30336 net.cpp:198] Scale2 needs backward computation.
I1128 19:31:38.277678 30336 net.cpp:198] BatchNorm2 needs backward computation.
I1128 19:31:38.277679 30336 net.cpp:198] Convolution2 needs backward computation.
I1128 19:31:38.277683 30336 net.cpp:198] Convolution1_ReLU1_0_split needs backward computation.
I1128 19:31:38.277686 30336 net.cpp:198] ReLU1 needs backward computation.
I1128 19:31:38.277689 30336 net.cpp:198] Scale1 needs backward computation.
I1128 19:31:38.277693 30336 net.cpp:198] BatchNorm1 needs backward computation.
I1128 19:31:38.277695 30336 net.cpp:198] Convolution1 needs backward computation.
I1128 19:31:38.277699 30336 net.cpp:200] Data2_Data1_1_split does not need backward computation.
I1128 19:31:38.277703 30336 net.cpp:200] Data1 does not need backward computation.
I1128 19:31:38.277705 30336 net.cpp:242] This network produces output Accuracy1
I1128 19:31:38.277709 30336 net.cpp:242] This network produces output SoftmaxWithLoss1
I1128 19:31:38.277755 30336 net.cpp:255] Network initialization done.
I1128 19:31:38.277916 30336 solver.cpp:56] Solver scaffolding done.
I1128 19:31:38.283695 30336 caffe.cpp:248] Starting Optimization
I1128 19:31:38.283702 30336 solver.cpp:272] Solving 
I1128 19:31:38.283704 30336 solver.cpp:273] Learning Rate Policy: multistep
I1128 19:31:38.289515 30336 solver.cpp:330] Iteration 0, Testing net (#0)
I1128 19:31:55.852080 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:32:13.546943 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:32:13.689528 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 1
I1128 19:32:13.689548 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 87.3361 (* 1 = 87.3361 loss)
I1128 19:32:14.365252 30336 solver.cpp:218] Iteration 0 (-4.2043e+20 iter/s, 36.0803s/200 iters), loss = 2.55203
I1128 19:32:14.365291 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.55203 (* 1 = 2.55203 loss)
I1128 19:32:14.365309 30336 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1128 19:34:27.374392 30336 solver.cpp:218] Iteration 200 (1.50371 iter/s, 133.004s/200 iters), loss = 1.89414
I1128 19:34:27.374521 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.89414 (* 1 = 1.89414 loss)
I1128 19:34:27.374541 30336 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1128 19:36:31.929846 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:36:41.304965 30336 solver.cpp:218] Iteration 400 (1.49336 iter/s, 133.926s/200 iters), loss = 1.62234
I1128 19:36:41.304991 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.62234 (* 1 = 1.62234 loss)
I1128 19:36:41.304997 30336 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1128 19:37:47.646554 30336 solver.cpp:330] Iteration 500, Testing net (#0)
I1128 19:38:05.494923 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:38:23.486138 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:38:23.631438 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.3378
I1128 19:38:23.631458 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.82536 (* 1 = 1.82536 loss)
I1128 19:39:31.312640 30336 solver.cpp:218] Iteration 600 (1.17646 iter/s, 170.002s/200 iters), loss = 1.55391
I1128 19:39:31.312752 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.55391 (* 1 = 1.55391 loss)
I1128 19:39:31.312758 30336 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1128 19:41:29.926010 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:41:45.338559 30336 solver.cpp:218] Iteration 800 (1.4923 iter/s, 134.021s/200 iters), loss = 1.46284
I1128 19:41:45.338598 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.46284 (* 1 = 1.46284 loss)
I1128 19:41:45.338603 30336 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1128 19:43:58.731467 30336 solver.cpp:330] Iteration 1000, Testing net (#0)
I1128 19:44:16.618759 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:44:34.611938 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:44:34.757732 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.2258
I1128 19:44:34.757757 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.25495 (* 1 = 2.25495 loss)
I1128 19:44:35.426290 30336 solver.cpp:218] Iteration 1000 (1.17585 iter/s, 170.089s/200 iters), loss = 1.32821
I1128 19:44:35.426342 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.32821 (* 1 = 1.32821 loss)
I1128 19:44:35.426349 30336 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1128 19:46:27.465690 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:46:49.606540 30336 solver.cpp:218] Iteration 1200 (1.4905 iter/s, 134.183s/200 iters), loss = 1.12362
I1128 19:46:49.606586 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.12362 (* 1 = 1.12362 loss)
I1128 19:46:49.606592 30336 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1128 19:49:03.724047 30336 solver.cpp:218] Iteration 1400 (1.49121 iter/s, 134.119s/200 iters), loss = 1.13539
I1128 19:49:03.724128 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.13539 (* 1 = 1.13539 loss)
I1128 19:49:03.724133 30336 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1128 19:50:10.060525 30336 solver.cpp:330] Iteration 1500, Testing net (#0)
I1128 19:50:27.885917 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:50:45.849807 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:50:45.995270 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.3515
I1128 19:50:45.995290 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.80137 (* 1 = 1.80137 loss)
I1128 19:51:25.519233 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:51:53.644557 30336 solver.cpp:218] Iteration 1600 (1.17702 iter/s, 169.921s/200 iters), loss = 1.20857
I1128 19:51:53.644598 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.20857 (* 1 = 1.20857 loss)
I1128 19:51:53.644603 30336 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1128 19:54:07.589538 30336 solver.cpp:218] Iteration 1800 (1.49316 iter/s, 133.945s/200 iters), loss = 1.12017
I1128 19:54:07.589663 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.12017 (* 1 = 1.12017 loss)
I1128 19:54:07.589684 30336 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1128 19:55:47.422225 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:56:20.921007 30336 solver.cpp:330] Iteration 2000, Testing net (#0)
I1128 19:56:38.763218 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:56:56.747910 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:56:56.892572 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4493
I1128 19:56:56.892591 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.5864 (* 1 = 1.5864 loss)
I1128 19:56:57.560195 30336 solver.cpp:218] Iteration 2000 (1.17668 iter/s, 169.969s/200 iters), loss = 1.05085
I1128 19:56:57.560222 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.05085 (* 1 = 1.05085 loss)
I1128 19:56:57.560227 30336 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1128 19:59:11.585584 30336 solver.cpp:218] Iteration 2200 (1.49227 iter/s, 134.024s/200 iters), loss = 0.914648
I1128 19:59:11.585690 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.914648 (* 1 = 0.914648 loss)
I1128 19:59:11.585697 30336 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1128 20:00:44.748297 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:01:25.624195 30336 solver.cpp:218] Iteration 2400 (1.49213 iter/s, 134.037s/200 iters), loss = 0.796866
I1128 20:01:25.624322 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.796866 (* 1 = 0.796866 loss)
I1128 20:01:25.624331 30336 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1128 20:02:31.975716 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_2500.caffemodel
I1128 20:02:35.111273 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_2500.solverstate
I1128 20:02:35.180449 30336 solver.cpp:330] Iteration 2500, Testing net (#0)
I1128 20:02:53.054106 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:03:11.010874 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:03:11.155731 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4841
I1128 20:03:11.155750 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.43295 (* 1 = 1.43295 loss)
I1128 20:04:18.855239 30336 solver.cpp:218] Iteration 2600 (1.15455 iter/s, 173.228s/200 iters), loss = 1.06112
I1128 20:04:18.855394 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.06112 (* 1 = 1.06112 loss)
I1128 20:04:18.855401 30336 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1128 20:05:46.013526 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:06:32.944866 30336 solver.cpp:218] Iteration 2800 (1.49157 iter/s, 134.087s/200 iters), loss = 0.786546
I1128 20:06:32.945026 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.786546 (* 1 = 0.786546 loss)
I1128 20:06:32.945034 30336 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1128 20:08:46.380475 30336 solver.cpp:330] Iteration 3000, Testing net (#0)
I1128 20:09:04.225647 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:09:22.216722 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:09:22.362241 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4967
I1128 20:09:22.362262 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.32547 (* 1 = 1.32547 loss)
I1128 20:09:23.030124 30336 solver.cpp:218] Iteration 3000 (1.1759 iter/s, 170.082s/200 iters), loss = 0.777032
I1128 20:09:23.030153 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.777032 (* 1 = 0.777032 loss)
I1128 20:09:23.030172 30336 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1128 20:10:43.436080 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:11:37.028877 30336 solver.cpp:218] Iteration 3200 (1.49258 iter/s, 133.996s/200 iters), loss = 0.656673
I1128 20:11:37.029031 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.656673 (* 1 = 0.656673 loss)
I1128 20:11:37.029038 30336 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1128 20:13:51.058617 30336 solver.cpp:218] Iteration 3400 (1.49224 iter/s, 134.027s/200 iters), loss = 0.86342
I1128 20:13:51.058760 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.86342 (* 1 = 0.86342 loss)
I1128 20:13:51.058768 30336 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1128 20:14:57.479727 30336 solver.cpp:330] Iteration 3500, Testing net (#0)
I1128 20:15:15.307725 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:15:33.264375 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:15:33.409497 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4633
I1128 20:15:33.409519 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.46088 (* 1 = 1.46088 loss)
I1128 20:15:41.459254 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:16:41.095783 30336 solver.cpp:218] Iteration 3600 (1.17624 iter/s, 170.034s/200 iters), loss = 0.89992
I1128 20:16:41.095921 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.89992 (* 1 = 0.89992 loss)
I1128 20:16:41.095927 30336 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1128 20:18:55.152812 30336 solver.cpp:218] Iteration 3800 (1.49202 iter/s, 134.046s/200 iters), loss = 0.748494
I1128 20:18:55.152931 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.748494 (* 1 = 0.748494 loss)
I1128 20:18:55.152952 30336 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1128 20:20:03.526721 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:21:08.556062 30336 solver.cpp:330] Iteration 4000, Testing net (#0)
I1128 20:21:26.397531 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:21:44.376451 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:21:44.521221 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4062
I1128 20:21:44.521241 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.72322 (* 1 = 1.72322 loss)
I1128 20:21:45.188472 30336 solver.cpp:218] Iteration 4000 (1.17631 iter/s, 170.023s/200 iters), loss = 0.456126
I1128 20:21:45.188498 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.456126 (* 1 = 0.456126 loss)
I1128 20:21:45.188503 30336 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1128 20:23:59.178349 30336 solver.cpp:218] Iteration 4200 (1.49274 iter/s, 133.981s/200 iters), loss = 0.535868
I1128 20:23:59.178491 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.535868 (* 1 = 0.535868 loss)
I1128 20:23:59.178498 30336 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1128 20:25:00.812201 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:26:13.160665 30336 solver.cpp:218] Iteration 4400 (1.49282 iter/s, 133.975s/200 iters), loss = 0.653332
I1128 20:26:13.160815 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.653332 (* 1 = 0.653332 loss)
I1128 20:26:13.160822 30336 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1128 20:27:19.504786 30336 solver.cpp:330] Iteration 4500, Testing net (#0)
I1128 20:27:37.326444 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:27:55.285303 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:27:55.430455 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.343
I1128 20:27:55.430476 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.14692 (* 1 = 2.14692 loss)
I1128 20:29:03.117462 30336 solver.cpp:218] Iteration 4600 (1.17683 iter/s, 169.948s/200 iters), loss = 0.427815
I1128 20:29:03.117630 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.427815 (* 1 = 0.427815 loss)
I1128 20:29:03.117638 30336 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1128 20:29:58.760483 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:31:17.161746 30336 solver.cpp:218] Iteration 4800 (1.49211 iter/s, 134.038s/200 iters), loss = 0.476123
I1128 20:31:17.161893 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.476123 (* 1 = 0.476123 loss)
I1128 20:31:17.161900 30336 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1128 20:33:30.560750 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_5000.caffemodel
I1128 20:33:30.705669 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_5000.solverstate
I1128 20:33:30.772827 30336 solver.cpp:330] Iteration 5000, Testing net (#0)
I1128 20:33:48.610395 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:34:06.579252 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:34:06.724786 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.2938
I1128 20:34:06.724807 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.14854 (* 1 = 2.14854 loss)
I1128 20:34:07.393077 30336 solver.cpp:218] Iteration 5000 (1.17492 iter/s, 170.224s/200 iters), loss = 0.531612
I1128 20:34:07.393105 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.531612 (* 1 = 0.531612 loss)
I1128 20:34:07.393124 30336 sgd_solver.cpp:105] Iteration 5000, lr = 0.1
I1128 20:34:56.985817 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:36:21.420651 30336 solver.cpp:218] Iteration 5200 (1.49229 iter/s, 134.023s/200 iters), loss = 0.668061
I1128 20:36:21.420791 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.668061 (* 1 = 0.668061 loss)
I1128 20:36:21.420799 30336 sgd_solver.cpp:105] Iteration 5200, lr = 0.1
I1128 20:38:35.477378 30336 solver.cpp:218] Iteration 5400 (1.49196 iter/s, 134.052s/200 iters), loss = 0.508483
I1128 20:38:35.477509 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.508483 (* 1 = 0.508483 loss)
I1128 20:38:35.477516 30336 sgd_solver.cpp:105] Iteration 5400, lr = 0.1
I1128 20:39:18.403333 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:39:41.858615 30336 solver.cpp:330] Iteration 5500, Testing net (#0)
I1128 20:39:59.678526 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:40:17.643765 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:40:17.788497 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.3635
I1128 20:40:17.788518 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.07209 (* 1 = 2.07209 loss)
I1128 20:41:25.479791 30336 solver.cpp:218] Iteration 5600 (1.17649 iter/s, 169.997s/200 iters), loss = 0.45784
I1128 20:41:25.479933 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.45784 (* 1 = 0.45784 loss)
I1128 20:41:25.479955 30336 sgd_solver.cpp:105] Iteration 5600, lr = 0.1
I1128 20:43:39.705066 30336 solver.cpp:218] Iteration 5800 (1.49008 iter/s, 134.221s/200 iters), loss = 0.373525
I1128 20:43:39.705198 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.373525 (* 1 = 0.373525 loss)
I1128 20:43:39.705204 30336 sgd_solver.cpp:105] Iteration 5800, lr = 0.1
I1128 20:44:16.620281 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:45:53.161801 30336 solver.cpp:330] Iteration 6000, Testing net (#0)
I1128 20:46:11.009398 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:46:28.991410 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:46:29.138345 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4762
I1128 20:46:29.138368 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.57828 (* 1 = 1.57828 loss)
I1128 20:46:29.809681 30336 solver.cpp:218] Iteration 6000 (1.17579 iter/s, 170.099s/200 iters), loss = 0.365364
I1128 20:46:29.809782 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.365364 (* 1 = 0.365364 loss)
I1128 20:46:29.809800 30336 sgd_solver.cpp:105] Iteration 6000, lr = 0.1
I1128 20:48:43.987828 30336 solver.cpp:218] Iteration 6200 (1.4906 iter/s, 134.174s/200 iters), loss = 0.374312
I1128 20:48:43.987921 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.374312 (* 1 = 0.374312 loss)
I1128 20:48:43.987926 30336 sgd_solver.cpp:105] Iteration 6200, lr = 0.1
I1128 20:49:14.174779 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:50:58.036221 30336 solver.cpp:218] Iteration 6400 (1.49205 iter/s, 134.044s/200 iters), loss = 0.353133
I1128 20:50:58.036381 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.353133 (* 1 = 0.353133 loss)
I1128 20:50:58.036388 30336 sgd_solver.cpp:105] Iteration 6400, lr = 0.1
I1128 20:52:04.366053 30336 solver.cpp:330] Iteration 6500, Testing net (#0)
I1128 20:52:22.187525 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:52:40.154510 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:52:40.301898 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4804
I1128 20:52:40.301921 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.59507 (* 1 = 1.59507 loss)
I1128 20:53:47.976070 30336 solver.cpp:218] Iteration 6600 (1.17696 iter/s, 169.929s/200 iters), loss = 0.376647
I1128 20:53:47.976207 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.376647 (* 1 = 0.376647 loss)
I1128 20:53:47.976214 30336 sgd_solver.cpp:105] Iteration 6600, lr = 0.1
I1128 20:54:12.110970 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:56:02.010680 30336 solver.cpp:218] Iteration 6800 (1.49224 iter/s, 134.027s/200 iters), loss = 0.497535
I1128 20:56:02.010830 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.497535 (* 1 = 0.497535 loss)
I1128 20:56:02.010838 30336 sgd_solver.cpp:105] Iteration 6800, lr = 0.1
I1128 20:58:15.420867 30336 solver.cpp:330] Iteration 7000, Testing net (#0)
I1128 20:58:33.263808 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:58:51.256598 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:58:51.402107 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.3863
I1128 20:58:51.402127 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.20279 (* 1 = 2.20279 loss)
I1128 20:58:52.069046 30336 solver.cpp:218] Iteration 7000 (1.17613 iter/s, 170.05s/200 iters), loss = 0.379011
I1128 20:58:52.069072 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.379011 (* 1 = 0.379011 loss)
I1128 20:58:52.069077 30336 sgd_solver.cpp:105] Iteration 7000, lr = 0.1
I1128 20:59:10.172041 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:01:06.069773 30336 solver.cpp:218] Iteration 7200 (1.4926 iter/s, 133.994s/200 iters), loss = 0.319607
I1128 21:01:06.069919 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.319607 (* 1 = 0.319607 loss)
I1128 21:01:06.069927 30336 sgd_solver.cpp:105] Iteration 7200, lr = 0.1
I1128 21:03:20.184779 30336 solver.cpp:218] Iteration 7400 (1.49133 iter/s, 134.109s/200 iters), loss = 0.312008
I1128 21:03:20.185045 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.312008 (* 1 = 0.312008 loss)
I1128 21:03:20.185052 30336 sgd_solver.cpp:105] Iteration 7400, lr = 0.1
I1128 21:03:31.601892 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:04:26.585685 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_7500.caffemodel
I1128 21:04:26.728127 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_7500.solverstate
I1128 21:04:26.794823 30336 solver.cpp:330] Iteration 7500, Testing net (#0)
I1128 21:04:44.634096 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:05:02.610895 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:05:02.756506 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4848
I1128 21:05:02.756526 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.61303 (* 1 = 1.61303 loss)
I1128 21:06:10.420008 30336 solver.cpp:218] Iteration 7600 (1.1749 iter/s, 170.228s/200 iters), loss = 0.364226
I1128 21:06:10.420155 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.364226 (* 1 = 0.364226 loss)
I1128 21:06:10.420176 30336 sgd_solver.cpp:105] Iteration 7600, lr = 0.1
I1128 21:08:24.415132 30336 solver.cpp:218] Iteration 7800 (1.49265 iter/s, 133.99s/200 iters), loss = 0.313992
I1128 21:08:24.415253 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.313992 (* 1 = 0.313992 loss)
I1128 21:08:24.415261 30336 sgd_solver.cpp:105] Iteration 7800, lr = 0.1
I1128 21:08:29.786917 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:10:37.780012 30336 solver.cpp:330] Iteration 8000, Testing net (#0)
I1128 21:10:55.604967 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:11:13.584481 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:11:13.729931 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.3988
I1128 21:11:13.729951 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.19868 (* 1 = 2.19868 loss)
I1128 21:11:14.396348 30336 solver.cpp:218] Iteration 8000 (1.17665 iter/s, 169.975s/200 iters), loss = 0.167926
I1128 21:11:14.396374 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.167926 (* 1 = 0.167926 loss)
I1128 21:11:14.396379 30336 sgd_solver.cpp:105] Iteration 8000, lr = 0.1
I1128 21:13:27.753082 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:13:28.417763 30336 solver.cpp:218] Iteration 8200 (1.49236 iter/s, 134.016s/200 iters), loss = 0.29355
I1128 21:13:28.417789 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.29355 (* 1 = 0.29355 loss)
I1128 21:13:28.417795 30336 sgd_solver.cpp:105] Iteration 8200, lr = 0.1
I1128 21:15:42.460316 30336 solver.cpp:218] Iteration 8400 (1.49212 iter/s, 134.038s/200 iters), loss = 0.359166
I1128 21:15:42.460465 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.359166 (* 1 = 0.359166 loss)
I1128 21:15:42.460472 30336 sgd_solver.cpp:105] Iteration 8400, lr = 0.1
I1128 21:16:48.817548 30336 solver.cpp:330] Iteration 8500, Testing net (#0)
I1128 21:17:06.667815 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:17:24.653607 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:17:24.798249 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5873
I1128 21:17:24.798270 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.24458 (* 1 = 1.24458 loss)
I1128 21:18:25.099345 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:18:32.460443 30336 solver.cpp:218] Iteration 8600 (1.17651 iter/s, 169.994s/200 iters), loss = 0.310937
I1128 21:18:32.460469 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.310937 (* 1 = 0.310937 loss)
I1128 21:18:32.460474 30336 sgd_solver.cpp:105] Iteration 8600, lr = 0.1
I1128 21:20:46.474738 30336 solver.cpp:218] Iteration 8800 (1.49243 iter/s, 134.009s/200 iters), loss = 0.193525
I1128 21:20:46.474874 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.193525 (* 1 = 0.193525 loss)
I1128 21:20:46.474880 30336 sgd_solver.cpp:105] Iteration 8800, lr = 0.1
I1128 21:22:47.107841 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:22:59.838641 30336 solver.cpp:330] Iteration 9000, Testing net (#0)
I1128 21:23:17.673563 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:23:35.638058 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:23:35.782392 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5835
I1128 21:23:35.782411 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.48484 (* 1 = 1.48484 loss)
I1128 21:23:36.449681 30336 solver.cpp:218] Iteration 9000 (1.17669 iter/s, 169.969s/200 iters), loss = 0.289063
I1128 21:23:36.449708 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.289063 (* 1 = 0.289063 loss)
I1128 21:23:36.449713 30336 sgd_solver.cpp:105] Iteration 9000, lr = 0.1
I1128 21:25:50.539496 30336 solver.cpp:218] Iteration 9200 (1.49159 iter/s, 134.085s/200 iters), loss = 0.206889
I1128 21:25:50.539641 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.206889 (* 1 = 0.206889 loss)
I1128 21:25:50.539649 30336 sgd_solver.cpp:105] Iteration 9200, lr = 0.1
I1128 21:27:44.498193 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:28:04.596982 30336 solver.cpp:218] Iteration 9400 (1.49194 iter/s, 134.053s/200 iters), loss = 0.193942
I1128 21:28:04.597008 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.193942 (* 1 = 0.193942 loss)
I1128 21:28:04.597013 30336 sgd_solver.cpp:105] Iteration 9400, lr = 0.1
I1128 21:29:10.957692 30336 solver.cpp:330] Iteration 9500, Testing net (#0)
I1128 21:29:28.791070 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:29:46.757611 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:29:46.902580 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5916
I1128 21:29:46.902601 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.39644 (* 1 = 1.39644 loss)
I1128 21:30:54.588392 30336 solver.cpp:218] Iteration 9600 (1.17657 iter/s, 169.986s/200 iters), loss = 0.204327
I1128 21:30:54.588526 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.204327 (* 1 = 0.204327 loss)
I1128 21:30:54.588549 30336 sgd_solver.cpp:105] Iteration 9600, lr = 0.1
I1128 21:32:42.516821 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:33:08.646428 30336 solver.cpp:218] Iteration 9800 (1.49194 iter/s, 134.054s/200 iters), loss = 0.245288
I1128 21:33:08.646455 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.245288 (* 1 = 0.245288 loss)
I1128 21:33:08.646459 30336 sgd_solver.cpp:105] Iteration 9800, lr = 0.1
I1128 21:35:22.038753 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_10000.caffemodel
I1128 21:35:22.182454 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_10000.solverstate
I1128 21:35:22.251111 30336 solver.cpp:330] Iteration 10000, Testing net (#0)
I1128 21:35:40.070080 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:35:58.035538 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:35:58.181128 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6319
I1128 21:35:58.181149 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.23783 (* 1 = 1.23783 loss)
I1128 21:35:58.847942 30336 solver.cpp:218] Iteration 10000 (1.17512 iter/s, 170.196s/200 iters), loss = 0.252973
I1128 21:35:58.847972 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.252973 (* 1 = 0.252973 loss)
I1128 21:35:58.847991 30336 sgd_solver.cpp:105] Iteration 10000, lr = 0.1
I1128 21:37:40.858269 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:38:13.059375 30336 solver.cpp:218] Iteration 10200 (1.49024 iter/s, 134.207s/200 iters), loss = 0.208082
I1128 21:38:13.059456 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.208082 (* 1 = 0.208082 loss)
I1128 21:38:13.059463 30336 sgd_solver.cpp:105] Iteration 10200, lr = 0.1
I1128 21:40:27.177582 30336 solver.cpp:218] Iteration 10400 (1.49127 iter/s, 134.114s/200 iters), loss = 0.186547
I1128 21:40:27.177711 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.186547 (* 1 = 0.186547 loss)
I1128 21:40:27.177733 30336 sgd_solver.cpp:105] Iteration 10400, lr = 0.1
I1128 21:41:33.586072 30336 solver.cpp:330] Iteration 10500, Testing net (#0)
I1128 21:41:51.425981 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:42:09.417659 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:42:09.564121 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7298
I1128 21:42:09.564172 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.852026 (* 1 = 0.852026 loss)
I1128 21:42:38.419378 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:43:17.309861 30336 solver.cpp:218] Iteration 10600 (1.1756 iter/s, 170.126s/200 iters), loss = 0.204165
I1128 21:43:17.309993 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.204165 (* 1 = 0.204165 loss)
I1128 21:43:17.310001 30336 sgd_solver.cpp:105] Iteration 10600, lr = 0.1
I1128 21:45:31.467561 30336 solver.cpp:218] Iteration 10800 (1.49084 iter/s, 134.153s/200 iters), loss = 0.186317
I1128 21:45:31.467686 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.186317 (* 1 = 0.186317 loss)
I1128 21:45:31.467708 30336 sgd_solver.cpp:105] Iteration 10800, lr = 0.1
I1128 21:47:00.579885 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:47:44.795334 30336 solver.cpp:330] Iteration 11000, Testing net (#0)
I1128 21:48:02.618088 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:48:20.581410 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:48:20.726725 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6605
I1128 21:48:20.726745 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.26712 (* 1 = 1.26712 loss)
I1128 21:48:21.394455 30336 solver.cpp:218] Iteration 11000 (1.17702 iter/s, 169.921s/200 iters), loss = 0.164907
I1128 21:48:21.394482 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.164907 (* 1 = 0.164907 loss)
I1128 21:48:21.394489 30336 sgd_solver.cpp:105] Iteration 11000, lr = 0.1
I1128 21:50:35.396708 30336 solver.cpp:218] Iteration 11200 (1.49256 iter/s, 133.998s/200 iters), loss = 0.253187
I1128 21:50:35.396837 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.253187 (* 1 = 0.253187 loss)
I1128 21:50:35.396858 30336 sgd_solver.cpp:105] Iteration 11200, lr = 0.1
I1128 21:51:58.500524 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:52:49.423404 30336 solver.cpp:218] Iteration 11400 (1.49229 iter/s, 134.022s/200 iters), loss = 0.2107
I1128 21:52:49.423518 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.2107 (* 1 = 0.2107 loss)
I1128 21:52:49.423540 30336 sgd_solver.cpp:105] Iteration 11400, lr = 0.1
I1128 21:53:55.759469 30336 solver.cpp:330] Iteration 11500, Testing net (#0)
I1128 21:54:13.582819 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:54:31.547577 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:54:31.693158 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6297
I1128 21:54:31.693179 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.41501 (* 1 = 1.41501 loss)
I1128 21:55:39.339817 30336 solver.cpp:218] Iteration 11600 (1.17709 iter/s, 169.91s/200 iters), loss = 0.139577
I1128 21:55:39.339952 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.139577 (* 1 = 0.139577 loss)
I1128 21:55:39.339973 30336 sgd_solver.cpp:105] Iteration 11600, lr = 0.1
I1128 21:56:55.698895 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:57:53.295117 30336 solver.cpp:218] Iteration 11800 (1.49309 iter/s, 133.951s/200 iters), loss = 0.212086
I1128 21:57:53.295199 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.212086 (* 1 = 0.212086 loss)
I1128 21:57:53.295204 30336 sgd_solver.cpp:105] Iteration 11800, lr = 0.1
I1128 22:00:06.576438 30336 solver.cpp:330] Iteration 12000, Testing net (#0)
I1128 22:00:24.416942 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:00:42.398372 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:00:42.543215 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.704
I1128 22:00:42.543236 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.08557 (* 1 = 1.08557 loss)
I1128 22:00:43.211690 30336 solver.cpp:218] Iteration 12000 (1.17709 iter/s, 169.91s/200 iters), loss = 0.166283
I1128 22:00:43.211716 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.166283 (* 1 = 0.166283 loss)
I1128 22:00:43.211722 30336 sgd_solver.cpp:105] Iteration 12000, lr = 0.1
I1128 22:01:53.526767 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:02:57.144384 30336 solver.cpp:218] Iteration 12200 (1.49335 iter/s, 133.927s/200 iters), loss = 0.232722
I1128 22:02:57.144526 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.232722 (* 1 = 0.232722 loss)
I1128 22:02:57.144532 30336 sgd_solver.cpp:105] Iteration 12200, lr = 0.1
I1128 22:05:11.071396 30336 solver.cpp:218] Iteration 12400 (1.49341 iter/s, 133.922s/200 iters), loss = 0.181326
I1128 22:05:11.071480 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.181326 (* 1 = 0.181326 loss)
I1128 22:05:11.071486 30336 sgd_solver.cpp:105] Iteration 12400, lr = 0.1
I1128 22:06:14.709812 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:06:17.388911 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_12500.caffemodel
I1128 22:06:17.530885 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_12500.solverstate
I1128 22:06:17.597924 30336 solver.cpp:330] Iteration 12500, Testing net (#0)
I1128 22:06:35.419189 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:06:53.382581 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:06:53.527235 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.663101
I1128 22:06:53.527256 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.44688 (* 1 = 1.44688 loss)
I1128 22:08:01.186223 30336 solver.cpp:218] Iteration 12600 (1.17572 iter/s, 170.108s/200 iters), loss = 0.150957
I1128 22:08:01.186336 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.150957 (* 1 = 0.150957 loss)
I1128 22:08:01.186357 30336 sgd_solver.cpp:105] Iteration 12600, lr = 0.1
I1128 22:10:15.154901 30336 solver.cpp:218] Iteration 12800 (1.49294 iter/s, 133.964s/200 iters), loss = 0.170801
I1128 22:10:15.155045 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.170801 (* 1 = 0.170801 loss)
I1128 22:10:15.155050 30336 sgd_solver.cpp:105] Iteration 12800, lr = 0.1
I1128 22:11:12.770093 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:12:28.457805 30336 solver.cpp:330] Iteration 13000, Testing net (#0)
I1128 22:12:46.273425 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:13:04.236220 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:13:04.381822 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7492
I1128 22:13:04.381844 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.91451 (* 1 = 0.91451 loss)
I1128 22:13:05.050451 30336 solver.cpp:218] Iteration 13000 (1.17724 iter/s, 169.889s/200 iters), loss = 0.20718
I1128 22:13:05.050480 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.20718 (* 1 = 0.20718 loss)
I1128 22:13:05.050500 30336 sgd_solver.cpp:105] Iteration 13000, lr = 0.1
I1128 22:15:19.102444 30336 solver.cpp:218] Iteration 13200 (1.49201 iter/s, 134.047s/200 iters), loss = 0.135012
I1128 22:15:19.102589 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.135012 (* 1 = 0.135012 loss)
I1128 22:15:19.102596 30336 sgd_solver.cpp:105] Iteration 13200, lr = 0.1
I1128 22:16:10.719272 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:17:33.170398 30336 solver.cpp:218] Iteration 13400 (1.49184 iter/s, 134.063s/200 iters), loss = 0.281649
I1128 22:17:33.170547 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.281649 (* 1 = 0.281649 loss)
I1128 22:17:33.170555 30336 sgd_solver.cpp:105] Iteration 13400, lr = 0.1
I1128 22:18:39.537191 30336 solver.cpp:330] Iteration 13500, Testing net (#0)
I1128 22:18:57.364778 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:19:15.336812 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:19:15.482009 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.797099
I1128 22:19:15.482030 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.708586 (* 1 = 0.708586 loss)
I1128 22:20:23.132005 30336 solver.cpp:218] Iteration 13600 (1.17678 iter/s, 169.955s/200 iters), loss = 0.147094
I1128 22:20:23.132145 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.147094 (* 1 = 0.147094 loss)
I1128 22:20:23.132153 30336 sgd_solver.cpp:105] Iteration 13600, lr = 0.1
I1128 22:21:08.033432 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:22:37.134690 30336 solver.cpp:218] Iteration 13800 (1.49256 iter/s, 133.998s/200 iters), loss = 0.17321
I1128 22:22:37.134807 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.173211 (* 1 = 0.173211 loss)
I1128 22:22:37.134814 30336 sgd_solver.cpp:105] Iteration 13800, lr = 0.1
I1128 22:24:50.474684 30336 solver.cpp:330] Iteration 14000, Testing net (#0)
I1128 22:25:08.295261 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:25:26.260772 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:25:26.405042 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7464
I1128 22:25:26.405063 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.931719 (* 1 = 0.931719 loss)
I1128 22:25:27.072202 30336 solver.cpp:218] Iteration 14000 (1.17695 iter/s, 169.931s/200 iters), loss = 0.126155
I1128 22:25:27.072242 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.126155 (* 1 = 0.126155 loss)
I1128 22:25:27.072248 30336 sgd_solver.cpp:105] Iteration 14000, lr = 0.1
I1128 22:26:05.966926 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:27:41.142679 30336 solver.cpp:218] Iteration 14200 (1.49181 iter/s, 134.066s/200 iters), loss = 0.224414
I1128 22:27:41.142838 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.224414 (* 1 = 0.224414 loss)
I1128 22:27:41.142844 30336 sgd_solver.cpp:105] Iteration 14200, lr = 0.1
I1128 22:29:55.215680 30336 solver.cpp:218] Iteration 14400 (1.49178 iter/s, 134.068s/200 iters), loss = 0.163669
I1128 22:29:55.215816 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.163669 (* 1 = 0.163669 loss)
I1128 22:29:55.215822 30336 sgd_solver.cpp:105] Iteration 14400, lr = 0.1
I1128 22:30:28.070613 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:31:01.574259 30336 solver.cpp:330] Iteration 14500, Testing net (#0)
I1128 22:31:19.413157 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:31:37.388321 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:31:37.533597 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7178
I1128 22:31:37.533617 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.12397 (* 1 = 1.12397 loss)
I1128 22:32:45.140558 30336 solver.cpp:218] Iteration 14600 (1.17703 iter/s, 169.919s/200 iters), loss = 0.141952
I1128 22:32:45.140612 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.141952 (* 1 = 0.141952 loss)
I1128 22:32:45.140632 30336 sgd_solver.cpp:105] Iteration 14600, lr = 0.1
I1128 22:34:59.074409 30336 solver.cpp:218] Iteration 14800 (1.49333 iter/s, 133.929s/200 iters), loss = 0.130956
I1128 22:34:59.074506 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.130956 (* 1 = 0.130956 loss)
I1128 22:34:59.074512 30336 sgd_solver.cpp:105] Iteration 14800, lr = 0.1
I1128 22:35:25.200278 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:37:12.344130 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_15000.caffemodel
I1128 22:37:12.483165 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_15000.solverstate
I1128 22:37:12.551419 30336 solver.cpp:330] Iteration 15000, Testing net (#0)
I1128 22:37:30.387198 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:37:48.366813 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:37:48.512037 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7456
I1128 22:37:48.512058 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.892095 (* 1 = 0.892095 loss)
I1128 22:37:49.179229 30336 solver.cpp:218] Iteration 15000 (1.17579 iter/s, 170.099s/200 iters), loss = 0.113302
I1128 22:37:49.179255 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.113302 (* 1 = 0.113302 loss)
I1128 22:37:49.179275 30336 sgd_solver.cpp:105] Iteration 15000, lr = 0.1
I1128 22:40:03.188380 30336 solver.cpp:218] Iteration 15200 (1.49249 iter/s, 134.004s/200 iters), loss = 0.139782
I1128 22:40:03.188499 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.139782 (* 1 = 0.139782 loss)
I1128 22:40:03.188506 30336 sgd_solver.cpp:105] Iteration 15200, lr = 0.1
I1128 22:40:23.311822 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:42:17.237149 30336 solver.cpp:218] Iteration 15400 (1.49205 iter/s, 134.044s/200 iters), loss = 0.159841
I1128 22:42:17.237288 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.159841 (* 1 = 0.159841 loss)
I1128 22:42:17.237295 30336 sgd_solver.cpp:105] Iteration 15400, lr = 0.1
I1128 22:43:23.568795 30336 solver.cpp:330] Iteration 15500, Testing net (#0)
I1128 22:43:41.397125 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:43:59.369449 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:43:59.515259 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7354
I1128 22:43:59.515280 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.99856 (* 1 = 0.99856 loss)
I1128 22:45:07.284360 30336 solver.cpp:218] Iteration 15600 (1.17619 iter/s, 170.041s/200 iters), loss = 0.129865
I1128 22:45:07.284502 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.129866 (* 1 = 0.129866 loss)
I1128 22:45:07.284510 30336 sgd_solver.cpp:105] Iteration 15600, lr = 0.1
I1128 22:45:20.772719 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:47:21.463748 30336 solver.cpp:218] Iteration 15800 (1.4906 iter/s, 134.174s/200 iters), loss = 0.196518
I1128 22:47:21.463876 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.196518 (* 1 = 0.196518 loss)
I1128 22:47:21.463898 30336 sgd_solver.cpp:105] Iteration 15800, lr = 0.1
I1128 22:49:34.903280 30336 solver.cpp:330] Iteration 16000, Testing net (#0)
I1128 22:49:52.781270 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:50:10.779952 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:50:10.927328 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8095
I1128 22:50:10.927353 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.685795 (* 1 = 0.685795 loss)
I1128 22:50:11.600287 30336 solver.cpp:218] Iteration 16000 (1.17557 iter/s, 170.13s/200 iters), loss = 0.119292
I1128 22:50:11.600323 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.119292 (* 1 = 0.119292 loss)
I1128 22:50:11.600343 30336 sgd_solver.cpp:105] Iteration 16000, lr = 0.1
I1128 22:50:18.997406 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:52:25.680619 30336 solver.cpp:218] Iteration 16200 (1.4917 iter/s, 134.075s/200 iters), loss = 0.141135
I1128 22:52:25.680745 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.141135 (* 1 = 0.141135 loss)
I1128 22:52:25.680766 30336 sgd_solver.cpp:105] Iteration 16200, lr = 0.1
I1128 22:54:39.648331 30336 solver.cpp:218] Iteration 16400 (1.49295 iter/s, 133.963s/200 iters), loss = 0.183365
I1128 22:54:39.648440 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.183365 (* 1 = 0.183365 loss)
I1128 22:54:39.648447 30336 sgd_solver.cpp:105] Iteration 16400, lr = 0.1
I1128 22:54:41.001852 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:55:45.980579 30336 solver.cpp:330] Iteration 16500, Testing net (#0)
I1128 22:56:03.806625 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:56:21.776895 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:56:21.921562 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.823999
I1128 22:56:21.921582 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.620748 (* 1 = 0.620748 loss)
I1128 22:57:29.556265 30336 solver.cpp:218] Iteration 16600 (1.17715 iter/s, 169.902s/200 iters), loss = 0.152401
I1128 22:57:29.556421 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.152402 (* 1 = 0.152402 loss)
I1128 22:57:29.556428 30336 sgd_solver.cpp:105] Iteration 16600, lr = 0.1
I1128 22:59:38.166309 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:59:43.519507 30336 solver.cpp:218] Iteration 16800 (1.493 iter/s, 133.958s/200 iters), loss = 0.103424
I1128 22:59:43.519548 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103424 (* 1 = 0.103424 loss)
I1128 22:59:43.519554 30336 sgd_solver.cpp:105] Iteration 16800, lr = 0.1
I1128 23:01:56.832408 30336 solver.cpp:330] Iteration 17000, Testing net (#0)
I1128 23:02:14.664923 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:02:32.638063 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:02:32.783725 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.760099
I1128 23:02:32.783745 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.970936 (* 1 = 0.970936 loss)
I1128 23:02:33.451393 30336 solver.cpp:218] Iteration 17000 (1.17698 iter/s, 169.926s/200 iters), loss = 0.057902
I1128 23:02:33.451421 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0579022 (* 1 = 0.0579022 loss)
I1128 23:02:33.451426 30336 sgd_solver.cpp:105] Iteration 17000, lr = 0.1
I1128 23:04:36.116955 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:04:47.504278 30336 solver.cpp:218] Iteration 17200 (1.492 iter/s, 134.048s/200 iters), loss = 0.121282
I1128 23:04:47.504328 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.121282 (* 1 = 0.121282 loss)
I1128 23:04:47.504334 30336 sgd_solver.cpp:105] Iteration 17200, lr = 0.1
I1128 23:07:01.551092 30336 solver.cpp:218] Iteration 17400 (1.49207 iter/s, 134.042s/200 iters), loss = 0.247989
I1128 23:07:01.551210 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.24799 (* 1 = 0.24799 loss)
I1128 23:07:01.551232 30336 sgd_solver.cpp:105] Iteration 17400, lr = 0.1
I1128 23:08:07.913161 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_17500.caffemodel
I1128 23:08:08.053122 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_17500.solverstate
I1128 23:08:08.120126 30336 solver.cpp:330] Iteration 17500, Testing net (#0)
I1128 23:08:25.944875 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:08:43.916743 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:08:44.062212 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8105
I1128 23:08:44.062233 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.670688 (* 1 = 0.670688 loss)
I1128 23:09:34.321203 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:09:51.733311 30336 solver.cpp:218] Iteration 17600 (1.1753 iter/s, 170.17s/200 iters), loss = 0.12138
I1128 23:09:51.733340 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.121381 (* 1 = 0.121381 loss)
I1128 23:09:51.733345 30336 sgd_solver.cpp:105] Iteration 17600, lr = 0.1
I1128 23:12:05.747150 30336 solver.cpp:218] Iteration 17800 (1.49249 iter/s, 134.004s/200 iters), loss = 0.105659
I1128 23:12:05.747331 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105659 (* 1 = 0.105659 loss)
I1128 23:12:05.747339 30336 sgd_solver.cpp:105] Iteration 17800, lr = 0.1
I1128 23:13:55.638016 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:14:19.087532 30336 solver.cpp:330] Iteration 18000, Testing net (#0)
I1128 23:14:36.930268 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:14:54.911793 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:14:55.057350 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.794399
I1128 23:14:55.057371 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.805716 (* 1 = 0.805716 loss)
I1128 23:14:55.724627 30336 solver.cpp:218] Iteration 18000 (1.17671 iter/s, 169.966s/200 iters), loss = 0.19975
I1128 23:14:55.724666 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.19975 (* 1 = 0.19975 loss)
I1128 23:14:55.724673 30336 sgd_solver.cpp:105] Iteration 18000, lr = 0.1
I1128 23:17:09.681771 30336 solver.cpp:218] Iteration 18200 (1.49311 iter/s, 133.949s/200 iters), loss = 0.182649
I1128 23:17:09.681922 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.182649 (* 1 = 0.182649 loss)
I1128 23:17:09.681929 30336 sgd_solver.cpp:105] Iteration 18200, lr = 0.1
I1128 23:18:53.501981 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:19:23.631706 30336 solver.cpp:218] Iteration 18400 (1.49318 iter/s, 133.942s/200 iters), loss = 0.15445
I1128 23:19:23.631814 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.154451 (* 1 = 0.154451 loss)
I1128 23:19:23.631836 30336 sgd_solver.cpp:105] Iteration 18400, lr = 0.1
I1128 23:20:29.951601 30336 solver.cpp:330] Iteration 18500, Testing net (#0)
I1128 23:20:47.794729 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:21:05.780980 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:21:05.926446 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8183
I1128 23:21:05.926467 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.634903 (* 1 = 0.634903 loss)
I1128 23:22:13.571333 30336 solver.cpp:218] Iteration 18600 (1.17695 iter/s, 169.93s/200 iters), loss = 0.0541297
I1128 23:22:13.571452 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0541299 (* 1 = 0.0541299 loss)
I1128 23:22:13.571475 30336 sgd_solver.cpp:105] Iteration 18600, lr = 0.1
I1128 23:23:50.706691 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:24:27.531014 30336 solver.cpp:218] Iteration 18800 (1.49306 iter/s, 133.953s/200 iters), loss = 0.11029
I1128 23:24:27.531172 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.11029 (* 1 = 0.11029 loss)
I1128 23:24:27.531194 30336 sgd_solver.cpp:105] Iteration 18800, lr = 0.1
I1128 23:26:40.835479 30336 solver.cpp:330] Iteration 19000, Testing net (#0)
I1128 23:26:58.670652 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:27:16.653381 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:27:16.798303 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.820199
I1128 23:27:16.798324 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.641237 (* 1 = 0.641237 loss)
I1128 23:27:17.464762 30336 solver.cpp:218] Iteration 19000 (1.17699 iter/s, 169.925s/200 iters), loss = 0.164183
I1128 23:27:17.464788 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.164184 (* 1 = 0.164184 loss)
I1128 23:27:17.464793 30336 sgd_solver.cpp:105] Iteration 19000, lr = 0.1
I1128 23:28:48.609730 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:29:31.495685 30336 solver.cpp:218] Iteration 19200 (1.49226 iter/s, 134.024s/200 iters), loss = 0.0657059
I1128 23:29:31.495738 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0657061 (* 1 = 0.0657061 loss)
I1128 23:29:31.495757 30336 sgd_solver.cpp:105] Iteration 19200, lr = 0.1
I1128 23:31:45.458930 30336 solver.cpp:218] Iteration 19400 (1.49302 iter/s, 133.957s/200 iters), loss = 0.117686
I1128 23:31:45.459091 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.117686 (* 1 = 0.117686 loss)
I1128 23:31:45.459100 30336 sgd_solver.cpp:105] Iteration 19400, lr = 0.1
I1128 23:32:51.768801 30336 solver.cpp:330] Iteration 19500, Testing net (#0)
I1128 23:33:09.600451 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:33:27.580391 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:33:27.725881 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7819
I1128 23:33:27.725916 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.852091 (* 1 = 0.852091 loss)
I1128 23:33:46.482311 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:34:35.361608 30336 solver.cpp:218] Iteration 19600 (1.1772 iter/s, 169.895s/200 iters), loss = 0.111284
I1128 23:34:35.361737 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.111284 (* 1 = 0.111284 loss)
I1128 23:34:35.361758 30336 sgd_solver.cpp:105] Iteration 19600, lr = 0.1
I1128 23:36:49.311342 30336 solver.cpp:218] Iteration 19800 (1.49317 iter/s, 133.943s/200 iters), loss = 0.193642
I1128 23:36:49.311480 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.193643 (* 1 = 0.193643 loss)
I1128 23:36:49.311501 30336 sgd_solver.cpp:105] Iteration 19800, lr = 0.1
I1128 23:38:07.691509 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:39:02.601478 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_20000.caffemodel
I1128 23:39:02.740715 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_20000.solverstate
I1128 23:39:02.809402 30336 solver.cpp:330] Iteration 20000, Testing net (#0)
I1128 23:39:20.643133 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:39:38.610371 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:39:38.754946 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7508
I1128 23:39:38.754967 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.994519 (* 1 = 0.994519 loss)
I1128 23:39:39.422713 30336 solver.cpp:218] Iteration 20000 (1.17575 iter/s, 170.104s/200 iters), loss = 0.0932709
I1128 23:39:39.422740 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.093271 (* 1 = 0.093271 loss)
I1128 23:39:39.422760 30336 sgd_solver.cpp:105] Iteration 20000, lr = 0.1
I1128 23:41:53.363462 30336 solver.cpp:218] Iteration 20200 (1.49326 iter/s, 133.935s/200 iters), loss = 0.0616123
I1128 23:41:53.363603 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0616124 (* 1 = 0.0616124 loss)
I1128 23:41:53.363631 30336 sgd_solver.cpp:105] Iteration 20200, lr = 0.1
I1128 23:43:05.718417 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:44:07.315943 30336 solver.cpp:218] Iteration 20400 (1.49303 iter/s, 133.956s/200 iters), loss = 0.0929162
I1128 23:44:07.316059 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0929164 (* 1 = 0.0929164 loss)
I1128 23:44:07.316069 30336 sgd_solver.cpp:105] Iteration 20400, lr = 0.1
I1128 23:45:13.629186 30336 solver.cpp:330] Iteration 20500, Testing net (#0)
I1128 23:45:31.456161 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:45:49.423204 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:45:49.569006 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.830699
I1128 23:45:49.569026 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.616349 (* 1 = 0.616349 loss)
I1128 23:46:57.260973 30336 solver.cpp:218] Iteration 20600 (1.17684 iter/s, 169.946s/200 iters), loss = 0.0996404
I1128 23:46:57.261067 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0996406 (* 1 = 0.0996406 loss)
I1128 23:46:57.261073 30336 sgd_solver.cpp:105] Iteration 20600, lr = 0.1
I1128 23:48:03.615995 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:49:11.299171 30336 solver.cpp:218] Iteration 20800 (1.49212 iter/s, 134.038s/200 iters), loss = 0.0710489
I1128 23:49:11.299304 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.071049 (* 1 = 0.071049 loss)
I1128 23:49:11.299327 30336 sgd_solver.cpp:105] Iteration 20800, lr = 0.1
I1128 23:51:24.703462 30336 solver.cpp:330] Iteration 21000, Testing net (#0)
I1128 23:51:42.536214 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:52:00.506652 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:52:00.651739 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8183
I1128 23:52:00.651759 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.646561 (* 1 = 0.646561 loss)
I1128 23:52:01.318286 30336 solver.cpp:218] Iteration 21000 (1.17635 iter/s, 170.017s/200 iters), loss = 0.111892
I1128 23:52:01.318313 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.111892 (* 1 = 0.111892 loss)
I1128 23:52:01.318320 30336 sgd_solver.cpp:105] Iteration 21000, lr = 0.1
I1128 23:53:00.941776 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:54:15.297374 30336 solver.cpp:218] Iteration 21200 (1.4928 iter/s, 133.977s/200 iters), loss = 0.0821111
I1128 23:54:15.297488 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0821112 (* 1 = 0.0821112 loss)
I1128 23:54:15.297495 30336 sgd_solver.cpp:105] Iteration 21200, lr = 0.1
I1128 23:56:29.406117 30336 solver.cpp:218] Iteration 21400 (1.49136 iter/s, 134.106s/200 iters), loss = 0.12807
I1128 23:56:29.406236 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.12807 (* 1 = 0.12807 loss)
I1128 23:56:29.406242 30336 sgd_solver.cpp:105] Iteration 21400, lr = 0.1
I1128 23:57:23.032740 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:57:35.763684 30336 solver.cpp:330] Iteration 21500, Testing net (#0)
I1128 23:57:53.596451 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:58:11.557545 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:58:11.702535 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.814199
I1128 23:58:11.702555 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.679685 (* 1 = 0.679685 loss)
I1128 23:59:19.347306 30336 solver.cpp:218] Iteration 21600 (1.17691 iter/s, 169.937s/200 iters), loss = 0.160972
I1128 23:59:19.347421 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.160972 (* 1 = 0.160972 loss)
I1128 23:59:19.347443 30336 sgd_solver.cpp:105] Iteration 21600, lr = 0.1
I1129 00:01:33.352682 30336 solver.cpp:218] Iteration 21800 (1.49252 iter/s, 134.002s/200 iters), loss = 0.098673
I1129 00:01:33.352862 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0986731 (* 1 = 0.0986731 loss)
I1129 00:01:33.352870 30336 sgd_solver.cpp:105] Iteration 21800, lr = 0.1
I1129 00:02:20.269245 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:03:46.718421 30336 solver.cpp:330] Iteration 22000, Testing net (#0)
I1129 00:04:04.546269 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:04:22.527760 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:04:22.673108 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.818299
I1129 00:04:22.673130 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.644294 (* 1 = 0.644294 loss)
I1129 00:04:23.343132 30336 solver.cpp:218] Iteration 22000 (1.17657 iter/s, 169.985s/200 iters), loss = 0.0599014
I1129 00:04:23.343158 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0599015 (* 1 = 0.0599015 loss)
I1129 00:04:23.343164 30336 sgd_solver.cpp:105] Iteration 22000, lr = 0.1
I1129 00:06:37.401013 30336 solver.cpp:218] Iteration 22200 (1.49194 iter/s, 134.054s/200 iters), loss = 0.100151
I1129 00:06:37.401149 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.100152 (* 1 = 0.100152 loss)
I1129 00:06:37.401155 30336 sgd_solver.cpp:105] Iteration 22200, lr = 0.1
I1129 00:07:18.309798 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:08:51.504814 30336 solver.cpp:218] Iteration 22400 (1.49143 iter/s, 134.099s/200 iters), loss = 0.204255
I1129 00:08:51.505515 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.204255 (* 1 = 0.204255 loss)
I1129 00:08:51.505523 30336 sgd_solver.cpp:105] Iteration 22400, lr = 0.1
I1129 00:09:57.890161 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_22500.caffemodel
I1129 00:09:58.031980 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_22500.solverstate
I1129 00:09:58.100358 30336 solver.cpp:330] Iteration 22500, Testing net (#0)
I1129 00:10:15.929587 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:10:33.894295 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:10:34.039328 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8321
I1129 00:10:34.039348 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.578584 (* 1 = 0.578584 loss)
I1129 00:11:41.691639 30336 solver.cpp:218] Iteration 22600 (1.17522 iter/s, 170.18s/200 iters), loss = 0.14789
I1129 00:11:41.691787 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.14789 (* 1 = 0.14789 loss)
I1129 00:11:41.691793 30336 sgd_solver.cpp:105] Iteration 22600, lr = 0.1
I1129 00:12:16.541276 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:13:55.672442 30336 solver.cpp:218] Iteration 22800 (1.4928 iter/s, 133.976s/200 iters), loss = 0.0898095
I1129 00:13:55.672525 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0898097 (* 1 = 0.0898097 loss)
I1129 00:13:55.672533 30336 sgd_solver.cpp:105] Iteration 22800, lr = 0.1
I1129 00:16:08.992815 30336 solver.cpp:330] Iteration 23000, Testing net (#0)
I1129 00:16:26.808200 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:16:44.771327 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:16:44.915904 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8151
I1129 00:16:44.915926 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.686395 (* 1 = 0.686395 loss)
I1129 00:16:45.582252 30336 solver.cpp:218] Iteration 23000 (1.17714 iter/s, 169.903s/200 iters), loss = 0.05417
I1129 00:16:45.582279 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0541701 (* 1 = 0.0541701 loss)
I1129 00:16:45.582285 30336 sgd_solver.cpp:105] Iteration 23000, lr = 0.1
I1129 00:17:13.731413 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:18:59.579493 30336 solver.cpp:218] Iteration 23200 (1.49264 iter/s, 133.991s/200 iters), loss = 0.111128
I1129 00:18:59.579674 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.111129 (* 1 = 0.111129 loss)
I1129 00:18:59.579682 30336 sgd_solver.cpp:105] Iteration 23200, lr = 0.1
I1129 00:21:13.581540 30336 solver.cpp:218] Iteration 23400 (1.49258 iter/s, 133.996s/200 iters), loss = 0.0760747
I1129 00:21:13.581665 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0760749 (* 1 = 0.0760749 loss)
I1129 00:21:13.581686 30336 sgd_solver.cpp:105] Iteration 23400, lr = 0.1
I1129 00:21:35.705363 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:22:19.912878 30336 solver.cpp:330] Iteration 23500, Testing net (#0)
I1129 00:22:37.749382 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:22:55.723410 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:22:55.868474 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.805699
I1129 00:22:55.868494 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.731544 (* 1 = 0.731544 loss)
I1129 00:24:03.509707 30336 solver.cpp:218] Iteration 23600 (1.17702 iter/s, 169.921s/200 iters), loss = 0.0623273
I1129 00:24:03.509853 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0623274 (* 1 = 0.0623274 loss)
I1129 00:24:03.509861 30336 sgd_solver.cpp:105] Iteration 23600, lr = 0.1
I1129 00:26:17.458910 30336 solver.cpp:218] Iteration 23800 (1.49317 iter/s, 133.943s/200 iters), loss = 0.120707
I1129 00:26:17.458997 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.120707 (* 1 = 0.120707 loss)
I1129 00:26:17.459002 30336 sgd_solver.cpp:105] Iteration 23800, lr = 0.1
I1129 00:26:33.544975 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:28:30.757522 30336 solver.cpp:330] Iteration 24000, Testing net (#0)
I1129 00:28:48.589974 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:29:06.563381 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:29:06.708791 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8325
I1129 00:29:06.708812 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.66461 (* 1 = 0.66461 loss)
I1129 00:29:07.375638 30336 solver.cpp:218] Iteration 24000 (1.1771 iter/s, 169.91s/200 iters), loss = 0.0911391
I1129 00:29:07.375679 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0911393 (* 1 = 0.0911393 loss)
I1129 00:29:07.375684 30336 sgd_solver.cpp:105] Iteration 24000, lr = 0.1
I1129 00:31:21.348753 30336 solver.cpp:218] Iteration 24200 (1.4929 iter/s, 133.968s/200 iters), loss = 0.131844
I1129 00:31:21.348889 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.131844 (* 1 = 0.131844 loss)
I1129 00:31:21.348897 30336 sgd_solver.cpp:105] Iteration 24200, lr = 0.1
I1129 00:31:30.739611 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:33:35.326161 30336 solver.cpp:218] Iteration 24400 (1.49285 iter/s, 133.972s/200 iters), loss = 0.0532099
I1129 00:33:35.326297 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.05321 (* 1 = 0.05321 loss)
I1129 00:33:35.326305 30336 sgd_solver.cpp:105] Iteration 24400, lr = 0.1
I1129 00:34:41.665163 30336 solver.cpp:330] Iteration 24500, Testing net (#0)
I1129 00:34:59.484740 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:35:17.451155 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:35:17.595698 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8377
I1129 00:35:17.595718 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.56347 (* 1 = 0.56347 loss)
I1129 00:36:25.246443 30336 solver.cpp:218] Iteration 24600 (1.17707 iter/s, 169.913s/200 iters), loss = 0.0702139
I1129 00:36:25.246626 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0702141 (* 1 = 0.0702141 loss)
I1129 00:36:25.246634 30336 sgd_solver.cpp:105] Iteration 24600, lr = 0.1
I1129 00:36:28.609576 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:38:39.224462 30336 solver.cpp:218] Iteration 24800 (1.49284 iter/s, 133.973s/200 iters), loss = 0.141198
I1129 00:38:39.224612 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.141198 (* 1 = 0.141198 loss)
I1129 00:38:39.224620 30336 sgd_solver.cpp:105] Iteration 24800, lr = 0.1
I1129 00:40:49.863483 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:40:52.539444 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_25000.caffemodel
I1129 00:40:52.677518 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_25000.solverstate
I1129 00:40:52.745636 30336 solver.cpp:330] Iteration 25000, Testing net (#0)
I1129 00:41:10.576607 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:41:28.549409 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:41:28.694870 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.835
I1129 00:41:28.694891 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.623721 (* 1 = 0.623721 loss)
I1129 00:41:29.363379 30336 solver.cpp:218] Iteration 25000 (1.17556 iter/s, 170.132s/200 iters), loss = 0.151007
I1129 00:41:29.363405 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151007 (* 1 = 0.151007 loss)
I1129 00:41:29.363425 30336 sgd_solver.cpp:105] Iteration 25000, lr = 0.1
I1129 00:43:43.329936 30336 solver.cpp:218] Iteration 25200 (1.49297 iter/s, 133.961s/200 iters), loss = 0.116421
I1129 00:43:43.330065 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.116421 (* 1 = 0.116421 loss)
I1129 00:43:43.330087 30336 sgd_solver.cpp:105] Iteration 25200, lr = 0.1
I1129 00:45:47.999243 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:45:57.375576 30336 solver.cpp:218] Iteration 25400 (1.49209 iter/s, 134.04s/200 iters), loss = 0.078085
I1129 00:45:57.375619 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0780852 (* 1 = 0.0780852 loss)
I1129 00:45:57.375624 30336 sgd_solver.cpp:105] Iteration 25400, lr = 0.1
I1129 00:47:03.709023 30336 solver.cpp:330] Iteration 25500, Testing net (#0)
I1129 00:47:21.552124 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:47:39.539852 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:47:39.685741 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.821899
I1129 00:47:39.685761 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.661576 (* 1 = 0.661576 loss)
I1129 00:48:47.332407 30336 solver.cpp:218] Iteration 25600 (1.17681 iter/s, 169.95s/200 iters), loss = 0.238955
I1129 00:48:47.332541 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.238955 (* 1 = 0.238955 loss)
I1129 00:48:47.332548 30336 sgd_solver.cpp:105] Iteration 25600, lr = 0.1
I1129 00:50:45.916544 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:51:01.309185 30336 solver.cpp:218] Iteration 25800 (1.49288 iter/s, 133.97s/200 iters), loss = 0.131304
I1129 00:51:01.309212 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.131304 (* 1 = 0.131304 loss)
I1129 00:51:01.309217 30336 sgd_solver.cpp:105] Iteration 25800, lr = 0.1
I1129 00:53:14.624886 30336 solver.cpp:330] Iteration 26000, Testing net (#0)
I1129 00:53:32.477073 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:53:50.460088 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:53:50.605157 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8385
I1129 00:53:50.605178 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.604646 (* 1 = 0.604646 loss)
I1129 00:53:51.272768 30336 solver.cpp:218] Iteration 26000 (1.1768 iter/s, 169.953s/200 iters), loss = 0.0750864
I1129 00:53:51.272794 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0750866 (* 1 = 0.0750866 loss)
I1129 00:53:51.272800 30336 sgd_solver.cpp:105] Iteration 26000, lr = 0.1
I1129 00:55:43.141261 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:56:05.248420 30336 solver.cpp:218] Iteration 26200 (1.4929 iter/s, 133.968s/200 iters), loss = 0.085195
I1129 00:56:05.248446 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0851952 (* 1 = 0.0851952 loss)
I1129 00:56:05.248451 30336 sgd_solver.cpp:105] Iteration 26200, lr = 0.1
I1129 00:58:19.217133 30336 solver.cpp:218] Iteration 26400 (1.49297 iter/s, 133.961s/200 iters), loss = 0.140546
I1129 00:58:19.217257 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.140546 (* 1 = 0.140546 loss)
I1129 00:58:19.217277 30336 sgd_solver.cpp:105] Iteration 26400, lr = 0.1
I1129 00:59:25.533162 30336 solver.cpp:330] Iteration 26500, Testing net (#0)
I1129 00:59:43.369412 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:00:01.335119 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:00:01.480947 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7814
I1129 01:00:01.480969 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.905438 (* 1 = 0.905438 loss)
I1129 01:00:41.025025 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:01:09.155603 30336 solver.cpp:218] Iteration 26600 (1.17696 iter/s, 169.93s/200 iters), loss = 0.0713347
I1129 01:01:09.155632 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0713349 (* 1 = 0.0713349 loss)
I1129 01:01:09.155639 30336 sgd_solver.cpp:105] Iteration 26600, lr = 0.1
I1129 01:03:23.153954 30336 solver.cpp:218] Iteration 26800 (1.49263 iter/s, 133.992s/200 iters), loss = 0.0812164
I1129 01:03:23.154088 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0812167 (* 1 = 0.0812167 loss)
I1129 01:03:23.154114 30336 sgd_solver.cpp:105] Iteration 26800, lr = 0.1
I1129 01:05:02.991273 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:05:36.489192 30336 solver.cpp:330] Iteration 27000, Testing net (#0)
I1129 01:05:54.308965 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:06:12.281785 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:06:12.426323 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.797199
I1129 01:06:12.426345 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.784047 (* 1 = 0.784047 loss)
I1129 01:06:13.092639 30336 solver.cpp:218] Iteration 27000 (1.17695 iter/s, 169.931s/200 iters), loss = 0.143538
I1129 01:06:13.092667 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.143538 (* 1 = 0.143538 loss)
I1129 01:06:13.092689 30336 sgd_solver.cpp:105] Iteration 27000, lr = 0.1
I1129 01:08:27.060400 30336 solver.cpp:218] Iteration 27200 (1.49297 iter/s, 133.962s/200 iters), loss = 0.111753
I1129 01:08:27.060492 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.111754 (* 1 = 0.111754 loss)
I1129 01:08:27.060501 30336 sgd_solver.cpp:105] Iteration 27200, lr = 0.1
I1129 01:10:00.167686 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:10:41.030354 30336 solver.cpp:218] Iteration 27400 (1.49294 iter/s, 133.964s/200 iters), loss = 0.0875033
I1129 01:10:41.030477 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0875036 (* 1 = 0.0875036 loss)
I1129 01:10:41.030501 30336 sgd_solver.cpp:105] Iteration 27400, lr = 0.1
I1129 01:11:47.379925 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_27500.caffemodel
I1129 01:11:47.517374 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_27500.solverstate
I1129 01:11:47.584622 30336 solver.cpp:330] Iteration 27500, Testing net (#0)
I1129 01:12:05.424979 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:12:23.396622 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:12:23.541568 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.829
I1129 01:12:23.541589 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.610721 (* 1 = 0.610721 loss)
I1129 01:13:31.229066 30336 solver.cpp:218] Iteration 27600 (1.17515 iter/s, 170.191s/200 iters), loss = 0.15301
I1129 01:13:31.229207 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.15301 (* 1 = 0.15301 loss)
I1129 01:13:31.229214 30336 sgd_solver.cpp:105] Iteration 27600, lr = 0.1
I1129 01:14:58.355657 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:15:45.262612 30336 solver.cpp:218] Iteration 27800 (1.49223 iter/s, 134.028s/200 iters), loss = 0.0623367
I1129 01:15:45.262758 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0623369 (* 1 = 0.0623369 loss)
I1129 01:15:45.262765 30336 sgd_solver.cpp:105] Iteration 27800, lr = 0.1
I1129 01:17:58.659878 30336 solver.cpp:330] Iteration 28000, Testing net (#0)
I1129 01:18:16.503752 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:18:34.489415 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:18:34.636122 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.836999
I1129 01:18:34.636143 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.581004 (* 1 = 0.581004 loss)
I1129 01:18:35.303493 30336 solver.cpp:218] Iteration 28000 (1.17624 iter/s, 170.033s/200 iters), loss = 0.0947897
I1129 01:18:35.303519 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0947899 (* 1 = 0.0947899 loss)
I1129 01:18:35.303524 30336 sgd_solver.cpp:105] Iteration 28000, lr = 0.1
I1129 01:19:55.717370 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:20:49.305613 30336 solver.cpp:218] Iteration 28200 (1.49258 iter/s, 133.996s/200 iters), loss = 0.103504
I1129 01:20:49.305752 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103504 (* 1 = 0.103504 loss)
I1129 01:20:49.305759 30336 sgd_solver.cpp:105] Iteration 28200, lr = 0.1
I1129 01:23:03.287144 30336 solver.cpp:218] Iteration 28400 (1.49281 iter/s, 133.976s/200 iters), loss = 0.0517992
I1129 01:23:03.287287 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0517994 (* 1 = 0.0517994 loss)
I1129 01:23:03.287295 30336 sgd_solver.cpp:105] Iteration 28400, lr = 0.1
I1129 01:24:09.614025 30336 solver.cpp:330] Iteration 28500, Testing net (#0)
I1129 01:24:27.433848 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:24:45.402878 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:24:45.547298 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.812199
I1129 01:24:45.547319 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.741812 (* 1 = 0.741812 loss)
I1129 01:24:53.591331 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:25:53.194895 30336 solver.cpp:218] Iteration 28600 (1.17718 iter/s, 169.897s/200 iters), loss = 0.103645
I1129 01:25:53.195020 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103645 (* 1 = 0.103645 loss)
I1129 01:25:53.195041 30336 sgd_solver.cpp:105] Iteration 28600, lr = 0.1
I1129 01:28:07.154621 30336 solver.cpp:218] Iteration 28800 (1.49309 iter/s, 133.951s/200 iters), loss = 0.0877378
I1129 01:28:07.154665 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.087738 (* 1 = 0.087738 loss)
I1129 01:28:07.154685 30336 sgd_solver.cpp:105] Iteration 28800, lr = 0.1
I1129 01:29:15.484019 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:30:20.450616 30336 solver.cpp:330] Iteration 29000, Testing net (#0)
I1129 01:30:38.279436 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:30:56.242123 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:30:56.387624 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.800499
I1129 01:30:56.387645 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.731384 (* 1 = 0.731384 loss)
I1129 01:30:57.055305 30336 solver.cpp:218] Iteration 29000 (1.17723 iter/s, 169.89s/200 iters), loss = 0.0924292
I1129 01:30:57.055333 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0924294 (* 1 = 0.0924294 loss)
I1129 01:30:57.055341 30336 sgd_solver.cpp:105] Iteration 29000, lr = 0.1
I1129 01:33:11.102929 30336 solver.cpp:218] Iteration 29200 (1.49209 iter/s, 134.04s/200 iters), loss = 0.0384737
I1129 01:33:11.103070 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0384738 (* 1 = 0.0384738 loss)
I1129 01:33:11.103077 30336 sgd_solver.cpp:105] Iteration 29200, lr = 0.1
I1129 01:34:12.732667 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:35:25.081581 30336 solver.cpp:218] Iteration 29400 (1.49286 iter/s, 133.971s/200 iters), loss = 0.124835
I1129 01:35:25.081732 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.124836 (* 1 = 0.124836 loss)
I1129 01:35:25.081739 30336 sgd_solver.cpp:105] Iteration 29400, lr = 0.1
I1129 01:36:31.433346 30336 solver.cpp:330] Iteration 29500, Testing net (#0)
I1129 01:36:49.254029 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:37:07.214785 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:37:07.359467 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8375
I1129 01:37:07.359486 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.602038 (* 1 = 0.602038 loss)
I1129 01:38:15.012622 30336 solver.cpp:218] Iteration 29600 (1.17701 iter/s, 169.922s/200 iters), loss = 0.0769688
I1129 01:38:15.012778 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.076969 (* 1 = 0.076969 loss)
I1129 01:38:15.012786 30336 sgd_solver.cpp:105] Iteration 29600, lr = 0.1
I1129 01:39:10.645992 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:40:29.039896 30336 solver.cpp:218] Iteration 29800 (1.49231 iter/s, 134.02s/200 iters), loss = 0.108747
I1129 01:40:29.040050 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.108747 (* 1 = 0.108747 loss)
I1129 01:40:29.040056 30336 sgd_solver.cpp:105] Iteration 29800, lr = 0.1
I1129 01:42:42.402143 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_30000.caffemodel
I1129 01:42:42.543678 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_30000.solverstate
I1129 01:42:42.611145 30336 solver.cpp:330] Iteration 30000, Testing net (#0)
I1129 01:43:00.438158 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:43:18.415885 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:43:18.561223 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8352
I1129 01:43:18.561242 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.616196 (* 1 = 0.616196 loss)
I1129 01:43:19.228801 30336 solver.cpp:218] Iteration 30000 (1.17523 iter/s, 170.18s/200 iters), loss = 0.106179
I1129 01:43:19.228828 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106179 (* 1 = 0.106179 loss)
I1129 01:43:19.228833 30336 sgd_solver.cpp:105] Iteration 30000, lr = 0.1
I1129 01:44:08.841076 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:45:33.317627 30336 solver.cpp:218] Iteration 30200 (1.49162 iter/s, 134.082s/200 iters), loss = 0.151843
I1129 01:45:33.317744 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.151843 (* 1 = 0.151843 loss)
I1129 01:45:33.317765 30336 sgd_solver.cpp:105] Iteration 30200, lr = 0.1
I1129 01:47:47.376649 30336 solver.cpp:218] Iteration 30400 (1.49195 iter/s, 134.052s/200 iters), loss = 0.0713631
I1129 01:47:47.377094 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0713633 (* 1 = 0.0713633 loss)
I1129 01:47:47.377104 30336 sgd_solver.cpp:105] Iteration 30400, lr = 0.1
I1129 01:48:30.287847 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:48:53.735508 30336 solver.cpp:330] Iteration 30500, Testing net (#0)
I1129 01:49:11.573889 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:49:29.557426 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:49:29.702944 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8362
I1129 01:49:29.702965 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.621121 (* 1 = 0.621121 loss)
I1129 01:50:37.371714 30336 solver.cpp:218] Iteration 30600 (1.17656 iter/s, 169.986s/200 iters), loss = 0.101792
I1129 01:50:37.371847 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.101792 (* 1 = 0.101792 loss)
I1129 01:50:37.371855 30336 sgd_solver.cpp:105] Iteration 30600, lr = 0.1
I1129 01:52:51.350502 30336 solver.cpp:218] Iteration 30800 (1.49285 iter/s, 133.972s/200 iters), loss = 0.097573
I1129 01:52:51.350633 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.097573 (* 1 = 0.097573 loss)
I1129 01:52:51.350641 30336 sgd_solver.cpp:105] Iteration 30800, lr = 0.1
I1129 01:53:28.208988 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:55:04.671557 30336 solver.cpp:330] Iteration 31000, Testing net (#0)
I1129 01:55:22.514888 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:55:40.497576 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:55:40.643318 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8511
I1129 01:55:40.643339 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.522824 (* 1 = 0.522824 loss)
I1129 01:55:41.312132 30336 solver.cpp:218] Iteration 31000 (1.17679 iter/s, 169.953s/200 iters), loss = 0.165389
I1129 01:55:41.312160 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.165389 (* 1 = 0.165389 loss)
I1129 01:55:41.312165 30336 sgd_solver.cpp:105] Iteration 31000, lr = 0.1
I1129 01:57:55.282174 30336 solver.cpp:218] Iteration 31200 (1.49294 iter/s, 133.964s/200 iters), loss = 0.105737
I1129 01:57:55.282323 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105737 (* 1 = 0.105737 loss)
I1129 01:57:55.282330 30336 sgd_solver.cpp:105] Iteration 31200, lr = 0.1
I1129 01:58:25.440579 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:00:09.328908 30336 solver.cpp:218] Iteration 31400 (1.49204 iter/s, 134.045s/200 iters), loss = 0.0833608
I1129 02:00:09.329025 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0833608 (* 1 = 0.0833608 loss)
I1129 02:00:09.329030 30336 sgd_solver.cpp:105] Iteration 31400, lr = 0.1
I1129 02:01:15.684463 30336 solver.cpp:330] Iteration 31500, Testing net (#0)
I1129 02:01:33.501178 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:01:51.468327 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:01:51.613046 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8369
I1129 02:01:51.613068 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.576973 (* 1 = 0.576973 loss)
I1129 02:02:59.286283 30336 solver.cpp:218] Iteration 31600 (1.17678 iter/s, 169.955s/200 iters), loss = 0.0570415
I1129 02:02:59.286424 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0570415 (* 1 = 0.0570415 loss)
I1129 02:02:59.286432 30336 sgd_solver.cpp:105] Iteration 31600, lr = 0.1
I1129 02:03:23.409718 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:05:13.286283 30336 solver.cpp:218] Iteration 31800 (1.49257 iter/s, 133.997s/200 iters), loss = 0.130465
I1129 02:05:13.286407 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.130465 (* 1 = 0.130465 loss)
I1129 02:05:13.286429 30336 sgd_solver.cpp:105] Iteration 31800, lr = 0.1
I1129 02:07:26.631620 30336 solver.cpp:330] Iteration 32000, Testing net (#0)
I1129 02:07:44.478641 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:08:02.441714 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:08:02.587111 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.833599
I1129 02:08:02.587133 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.659948 (* 1 = 0.659948 loss)
I1129 02:08:03.255710 30336 solver.cpp:218] Iteration 32000 (1.17671 iter/s, 169.965s/200 iters), loss = 0.0865056
I1129 02:08:03.255739 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0865057 (* 1 = 0.0865057 loss)
I1129 02:08:03.255745 30336 sgd_solver.cpp:46] MultiStep Status: Iteration 32000, step = 1
I1129 02:08:03.255749 30336 sgd_solver.cpp:105] Iteration 32000, lr = 0.01
I1129 02:08:21.360129 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:10:17.251596 30336 solver.cpp:218] Iteration 32200 (1.49263 iter/s, 133.992s/200 iters), loss = 0.0811267
I1129 02:10:17.251715 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0811268 (* 1 = 0.0811268 loss)
I1129 02:10:17.251724 30336 sgd_solver.cpp:105] Iteration 32200, lr = 0.01
I1129 02:12:31.260216 30336 solver.cpp:218] Iteration 32400 (1.49249 iter/s, 134.004s/200 iters), loss = 0.0564751
I1129 02:12:31.260342 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0564752 (* 1 = 0.0564752 loss)
I1129 02:12:31.260367 30336 sgd_solver.cpp:105] Iteration 32400, lr = 0.01
I1129 02:12:42.667824 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:13:37.619951 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_32500.caffemodel
I1129 02:13:37.760371 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_32500.solverstate
I1129 02:13:37.827730 30336 solver.cpp:330] Iteration 32500, Testing net (#0)
I1129 02:13:55.654439 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:14:13.623065 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:14:13.768522 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.894902
I1129 02:14:13.768543 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.397742 (* 1 = 0.397742 loss)
I1129 02:15:21.426853 30336 solver.cpp:218] Iteration 32600 (1.17536 iter/s, 170.16s/200 iters), loss = 0.0200121
I1129 02:15:21.426998 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0200122 (* 1 = 0.0200122 loss)
I1129 02:15:21.427021 30336 sgd_solver.cpp:105] Iteration 32600, lr = 0.01
I1129 02:17:35.388156 30336 solver.cpp:218] Iteration 32800 (1.49303 iter/s, 133.956s/200 iters), loss = 0.011144
I1129 02:17:35.388283 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0111441 (* 1 = 0.0111441 loss)
I1129 02:17:35.388308 30336 sgd_solver.cpp:105] Iteration 32800, lr = 0.01
I1129 02:17:40.758715 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:19:48.685595 30336 solver.cpp:330] Iteration 33000, Testing net (#0)
I1129 02:20:06.507925 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:20:24.477519 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:20:24.623303 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.905502
I1129 02:20:24.623325 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.339418 (* 1 = 0.339418 loss)
I1129 02:20:25.292913 30336 solver.cpp:218] Iteration 33000 (1.17718 iter/s, 169.898s/200 iters), loss = 0.00473287
I1129 02:20:25.292951 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00473292 (* 1 = 0.00473292 loss)
I1129 02:20:25.292974 30336 sgd_solver.cpp:105] Iteration 33000, lr = 0.01
I1129 02:22:38.622778 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:22:39.285447 30336 solver.cpp:218] Iteration 33200 (1.49268 iter/s, 133.987s/200 iters), loss = 0.00770442
I1129 02:22:39.285476 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00770446 (* 1 = 0.00770446 loss)
I1129 02:22:39.285483 30336 sgd_solver.cpp:105] Iteration 33200, lr = 0.01
I1129 02:24:53.319172 30336 solver.cpp:218] Iteration 33400 (1.49222 iter/s, 134.028s/200 iters), loss = 0.00875059
I1129 02:24:53.319304 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00875065 (* 1 = 0.00875065 loss)
I1129 02:24:53.319329 30336 sgd_solver.cpp:105] Iteration 33400, lr = 0.01
I1129 02:25:59.686036 30336 solver.cpp:330] Iteration 33500, Testing net (#0)
I1129 02:26:17.527274 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:26:35.514636 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:26:35.659981 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.907602
I1129 02:26:35.660003 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.335462 (* 1 = 0.335462 loss)
I1129 02:27:35.958036 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:27:43.323783 30336 solver.cpp:218] Iteration 33600 (1.17649 iter/s, 169.998s/200 iters), loss = 0.0181676
I1129 02:27:43.323809 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0181676 (* 1 = 0.0181676 loss)
I1129 02:27:43.323814 30336 sgd_solver.cpp:105] Iteration 33600, lr = 0.01
I1129 02:29:57.279651 30336 solver.cpp:218] Iteration 33800 (1.49309 iter/s, 133.95s/200 iters), loss = 0.0145436
I1129 02:29:57.279801 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0145436 (* 1 = 0.0145436 loss)
I1129 02:29:57.279808 30336 sgd_solver.cpp:105] Iteration 33800, lr = 0.01
I1129 02:31:57.817721 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:32:10.543177 30336 solver.cpp:330] Iteration 34000, Testing net (#0)
I1129 02:32:28.376598 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:32:46.361150 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:32:46.505592 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.910403
I1129 02:32:46.505614 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.340024 (* 1 = 0.340024 loss)
I1129 02:32:47.173835 30336 solver.cpp:218] Iteration 34000 (1.17725 iter/s, 169.887s/200 iters), loss = 0.0114214
I1129 02:32:47.173861 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0114215 (* 1 = 0.0114215 loss)
I1129 02:32:47.173866 30336 sgd_solver.cpp:105] Iteration 34000, lr = 0.01
I1129 02:35:01.180758 30336 solver.cpp:218] Iteration 34200 (1.4925 iter/s, 134.004s/200 iters), loss = 0.0157324
I1129 02:35:01.180897 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0157324 (* 1 = 0.0157324 loss)
I1129 02:35:01.180919 30336 sgd_solver.cpp:105] Iteration 34200, lr = 0.01
I1129 02:36:55.082237 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:37:15.165535 30336 solver.cpp:218] Iteration 34400 (1.49275 iter/s, 133.981s/200 iters), loss = 0.00329853
I1129 02:37:15.165561 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00329858 (* 1 = 0.00329858 loss)
I1129 02:37:15.165566 30336 sgd_solver.cpp:105] Iteration 34400, lr = 0.01
I1129 02:38:21.486189 30336 solver.cpp:330] Iteration 34500, Testing net (#0)
I1129 02:38:39.310242 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:38:57.282403 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:38:57.427481 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.911603
I1129 02:38:57.427501 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.337498 (* 1 = 0.337498 loss)
I1129 02:40:05.076589 30336 solver.cpp:218] Iteration 34600 (1.17712 iter/s, 169.906s/200 iters), loss = 0.005099
I1129 02:40:05.076741 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00509906 (* 1 = 0.00509906 loss)
I1129 02:40:05.076750 30336 sgd_solver.cpp:105] Iteration 34600, lr = 0.01
I1129 02:41:52.927686 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:42:19.053401 30336 solver.cpp:218] Iteration 34800 (1.49285 iter/s, 133.972s/200 iters), loss = 0.0361027
I1129 02:42:19.053431 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0361028 (* 1 = 0.0361028 loss)
I1129 02:42:19.053436 30336 sgd_solver.cpp:105] Iteration 34800, lr = 0.01
I1129 02:44:32.320999 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_35000.caffemodel
I1129 02:44:32.460857 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_35000.solverstate
I1129 02:44:32.527783 30336 solver.cpp:330] Iteration 35000, Testing net (#0)
I1129 02:44:50.346457 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:45:08.325291 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:45:08.470747 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.912503
I1129 02:45:08.470768 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.339053 (* 1 = 0.339053 loss)
I1129 02:45:09.138669 30336 solver.cpp:218] Iteration 35000 (1.17592 iter/s, 170.079s/200 iters), loss = 0.00256524
I1129 02:45:09.138697 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0025653 (* 1 = 0.0025653 loss)
I1129 02:45:09.138703 30336 sgd_solver.cpp:105] Iteration 35000, lr = 0.01
I1129 02:46:50.972918 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:47:23.118904 30336 solver.cpp:218] Iteration 35200 (1.49281 iter/s, 133.975s/200 iters), loss = 0.0387424
I1129 02:47:23.119043 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0387425 (* 1 = 0.0387425 loss)
I1129 02:47:23.119050 30336 sgd_solver.cpp:105] Iteration 35200, lr = 0.01
I1129 02:49:37.105365 30336 solver.cpp:218] Iteration 35400 (1.49274 iter/s, 133.982s/200 iters), loss = 0.00445101
I1129 02:49:37.105489 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00445109 (* 1 = 0.00445109 loss)
I1129 02:49:37.105511 30336 sgd_solver.cpp:105] Iteration 35400, lr = 0.01
I1129 02:50:43.456562 30336 solver.cpp:330] Iteration 35500, Testing net (#0)
I1129 02:51:01.274999 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:51:19.261878 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:51:19.407742 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.914203
I1129 02:51:19.407763 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.338942 (* 1 = 0.338942 loss)
I1129 02:51:48.235066 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:52:27.088639 30336 solver.cpp:218] Iteration 35600 (1.17663 iter/s, 169.977s/200 iters), loss = 0.00146609
I1129 02:52:27.088779 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00146618 (* 1 = 0.00146618 loss)
I1129 02:52:27.088788 30336 sgd_solver.cpp:105] Iteration 35600, lr = 0.01
I1129 02:54:41.115500 30336 solver.cpp:218] Iteration 35800 (1.4923 iter/s, 134.022s/200 iters), loss = 0.0088235
I1129 02:54:41.115627 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00882358 (* 1 = 0.00882358 loss)
I1129 02:54:41.115633 30336 sgd_solver.cpp:105] Iteration 35800, lr = 0.01
I1129 02:56:10.275600 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:56:54.498333 30336 solver.cpp:330] Iteration 36000, Testing net (#0)
I1129 02:57:12.345080 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:57:30.332716 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:57:30.478307 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.912503
I1129 02:57:30.478328 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.353515 (* 1 = 0.353515 loss)
I1129 02:57:31.144426 30336 solver.cpp:218] Iteration 36000 (1.17632 iter/s, 170.022s/200 iters), loss = 0.00144356
I1129 02:57:31.144454 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00144365 (* 1 = 0.00144365 loss)
I1129 02:57:31.144459 30336 sgd_solver.cpp:105] Iteration 36000, lr = 0.01
I1129 02:59:45.163390 30336 solver.cpp:218] Iteration 36200 (1.49238 iter/s, 134.014s/200 iters), loss = 0.0393074
I1129 02:59:45.163566 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0393075 (* 1 = 0.0393075 loss)
I1129 02:59:45.163574 30336 sgd_solver.cpp:105] Iteration 36200, lr = 0.01
I1129 03:01:08.268929 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:01:59.172133 30336 solver.cpp:218] Iteration 36400 (1.4925 iter/s, 134.003s/200 iters), loss = 0.0056283
I1129 03:01:59.172281 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00562839 (* 1 = 0.00562839 loss)
I1129 03:01:59.172289 30336 sgd_solver.cpp:105] Iteration 36400, lr = 0.01
I1129 03:03:05.503826 30336 solver.cpp:330] Iteration 36500, Testing net (#0)
I1129 03:03:23.345803 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:03:41.326835 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:03:41.472024 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.914403
I1129 03:03:41.472044 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.342359 (* 1 = 0.342359 loss)
I1129 03:04:49.123531 30336 solver.cpp:218] Iteration 36600 (1.17685 iter/s, 169.945s/200 iters), loss = 0.0093761
I1129 03:04:49.123643 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00937618 (* 1 = 0.00937618 loss)
I1129 03:04:49.123651 30336 sgd_solver.cpp:105] Iteration 36600, lr = 0.01
I1129 03:06:05.510980 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:07:03.124218 30336 solver.cpp:218] Iteration 36800 (1.49258 iter/s, 133.996s/200 iters), loss = 0.00599641
I1129 03:07:03.124358 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0059965 (* 1 = 0.0059965 loss)
I1129 03:07:03.124382 30336 sgd_solver.cpp:105] Iteration 36800, lr = 0.01
I1129 03:09:16.427508 30336 solver.cpp:330] Iteration 37000, Testing net (#0)
I1129 03:09:34.254086 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:09:52.236598 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:09:52.381870 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917103
I1129 03:09:52.381892 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.342121 (* 1 = 0.342121 loss)
I1129 03:09:53.049017 30336 solver.cpp:218] Iteration 37000 (1.17702 iter/s, 169.921s/200 iters), loss = 0.0202506
I1129 03:09:53.049046 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0202507 (* 1 = 0.0202507 loss)
I1129 03:09:53.049052 30336 sgd_solver.cpp:105] Iteration 37000, lr = 0.01
I1129 03:11:03.398854 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:12:07.011495 30336 solver.cpp:218] Iteration 37200 (1.49299 iter/s, 133.959s/200 iters), loss = 0.00350194
I1129 03:12:07.011654 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00350203 (* 1 = 0.00350203 loss)
I1129 03:12:07.011662 30336 sgd_solver.cpp:105] Iteration 37200, lr = 0.01
I1129 03:14:20.952215 30336 solver.cpp:218] Iteration 37400 (1.49324 iter/s, 133.937s/200 iters), loss = 0.00708877
I1129 03:14:20.952354 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00708886 (* 1 = 0.00708886 loss)
I1129 03:14:20.952363 30336 sgd_solver.cpp:105] Iteration 37400, lr = 0.01
I1129 03:15:24.566321 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:15:27.243371 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_37500.caffemodel
I1129 03:15:27.382014 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_37500.solverstate
I1129 03:15:27.450204 30336 solver.cpp:330] Iteration 37500, Testing net (#0)
I1129 03:15:45.288560 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:16:03.268815 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:16:03.414186 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.911903
I1129 03:16:03.414207 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.358721 (* 1 = 0.358721 loss)
I1129 03:17:11.096660 30336 solver.cpp:218] Iteration 37600 (1.17551 iter/s, 170.139s/200 iters), loss = 0.00165687
I1129 03:17:11.096812 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00165697 (* 1 = 0.00165697 loss)
I1129 03:17:11.096820 30336 sgd_solver.cpp:105] Iteration 37600, lr = 0.01
I1129 03:19:25.119343 30336 solver.cpp:218] Iteration 37800 (1.49233 iter/s, 134.018s/200 iters), loss = 0.0206621
I1129 03:19:25.119482 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0206622 (* 1 = 0.0206622 loss)
I1129 03:19:25.119489 30336 sgd_solver.cpp:105] Iteration 37800, lr = 0.01
I1129 03:20:22.734453 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:21:38.417783 30336 solver.cpp:330] Iteration 38000, Testing net (#0)
I1129 03:21:56.239346 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:22:14.205807 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:22:14.351737 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.916703
I1129 03:22:14.351758 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355573 (* 1 = 0.355573 loss)
I1129 03:22:15.018359 30336 solver.cpp:218] Iteration 38000 (1.17721 iter/s, 169.893s/200 iters), loss = 0.00408928
I1129 03:22:15.018388 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00408937 (* 1 = 0.00408937 loss)
I1129 03:22:15.018407 30336 sgd_solver.cpp:105] Iteration 38000, lr = 0.01
I1129 03:24:28.993202 30336 solver.cpp:218] Iteration 38200 (1.49287 iter/s, 133.97s/200 iters), loss = 0.00294207
I1129 03:24:28.993342 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00294216 (* 1 = 0.00294216 loss)
I1129 03:24:28.993350 30336 sgd_solver.cpp:105] Iteration 38200, lr = 0.01
I1129 03:25:20.587258 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:26:42.948895 30336 solver.cpp:218] Iteration 38400 (1.49308 iter/s, 133.951s/200 iters), loss = 0.00574232
I1129 03:26:42.949033 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00574241 (* 1 = 0.00574241 loss)
I1129 03:26:42.949041 30336 sgd_solver.cpp:105] Iteration 38400, lr = 0.01
I1129 03:27:49.273064 30336 solver.cpp:330] Iteration 38500, Testing net (#0)
I1129 03:28:07.109374 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:28:25.088986 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:28:25.234243 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915103
I1129 03:28:25.234266 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.359341 (* 1 = 0.359341 loss)
I1129 03:29:32.872375 30336 solver.cpp:218] Iteration 38600 (1.17704 iter/s, 169.917s/200 iters), loss = 0.0123672
I1129 03:29:32.872526 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0123673 (* 1 = 0.0123673 loss)
I1129 03:29:32.872534 30336 sgd_solver.cpp:105] Iteration 38600, lr = 0.01
I1129 03:30:17.739372 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:31:46.830410 30336 solver.cpp:218] Iteration 38800 (1.49306 iter/s, 133.953s/200 iters), loss = 0.00246065
I1129 03:31:46.830560 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00246073 (* 1 = 0.00246073 loss)
I1129 03:31:46.830569 30336 sgd_solver.cpp:105] Iteration 38800, lr = 0.01
I1129 03:34:00.235765 30336 solver.cpp:330] Iteration 39000, Testing net (#0)
I1129 03:34:18.076655 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:34:36.059957 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:34:36.205400 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.913903
I1129 03:34:36.205422 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.361313 (* 1 = 0.361313 loss)
I1129 03:34:36.872862 30336 solver.cpp:218] Iteration 39000 (1.17622 iter/s, 170.036s/200 iters), loss = 0.000849164
I1129 03:34:36.872891 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000849246 (* 1 = 0.000849246 loss)
I1129 03:34:36.872896 30336 sgd_solver.cpp:105] Iteration 39000, lr = 0.01
I1129 03:35:15.741973 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:36:50.875540 30336 solver.cpp:218] Iteration 39200 (1.49256 iter/s, 133.998s/200 iters), loss = 0.00766049
I1129 03:36:50.875681 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00766057 (* 1 = 0.00766057 loss)
I1129 03:36:50.875689 30336 sgd_solver.cpp:105] Iteration 39200, lr = 0.01
I1129 03:39:04.890627 30336 solver.cpp:218] Iteration 39400 (1.49242 iter/s, 134.01s/200 iters), loss = 0.000746387
I1129 03:39:04.890760 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000746465 (* 1 = 0.000746465 loss)
I1129 03:39:04.890767 30336 sgd_solver.cpp:105] Iteration 39400, lr = 0.01
I1129 03:39:37.740183 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:40:11.241080 30336 solver.cpp:330] Iteration 39500, Testing net (#0)
I1129 03:40:29.070091 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:40:47.022549 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:40:47.166908 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.914603
I1129 03:40:47.166929 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.361684 (* 1 = 0.361684 loss)
I1129 03:41:54.831329 30336 solver.cpp:218] Iteration 39600 (1.17692 iter/s, 169.934s/200 iters), loss = 0.00182449
I1129 03:41:54.831468 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00182457 (* 1 = 0.00182457 loss)
I1129 03:41:54.831476 30336 sgd_solver.cpp:105] Iteration 39600, lr = 0.01
I1129 03:44:08.873777 30336 solver.cpp:218] Iteration 39800 (1.49212 iter/s, 134.038s/200 iters), loss = 0.00156571
I1129 03:44:08.873934 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00156579 (* 1 = 0.00156579 loss)
I1129 03:44:08.873940 30336 sgd_solver.cpp:105] Iteration 39800, lr = 0.01
I1129 03:44:35.027114 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:46:22.221328 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_40000.caffemodel
I1129 03:46:22.362603 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_40000.solverstate
I1129 03:46:22.431059 30336 solver.cpp:330] Iteration 40000, Testing net (#0)
I1129 03:46:40.258500 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:46:58.229188 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:46:58.374804 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915603
I1129 03:46:58.374824 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.364761 (* 1 = 0.364761 loss)
I1129 03:46:59.042070 30336 solver.cpp:218] Iteration 40000 (1.17535 iter/s, 170.162s/200 iters), loss = 0.00376558
I1129 03:46:59.042099 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00376566 (* 1 = 0.00376566 loss)
I1129 03:46:59.042120 30336 sgd_solver.cpp:105] Iteration 40000, lr = 0.01
I1129 03:49:13.043355 30336 solver.cpp:218] Iteration 40200 (1.49258 iter/s, 133.996s/200 iters), loss = 0.00171693
I1129 03:49:13.043416 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.001717 (* 1 = 0.001717 loss)
I1129 03:49:13.043421 30336 sgd_solver.cpp:105] Iteration 40200, lr = 0.01
I1129 03:49:33.163583 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:51:27.104668 30336 solver.cpp:218] Iteration 40400 (1.49191 iter/s, 134.056s/200 iters), loss = 0.00343854
I1129 03:51:27.104811 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00343862 (* 1 = 0.00343862 loss)
I1129 03:51:27.104833 30336 sgd_solver.cpp:105] Iteration 40400, lr = 0.01
I1129 03:52:33.444036 30336 solver.cpp:330] Iteration 40500, Testing net (#0)
I1129 03:52:51.286967 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:53:09.256731 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:53:09.402386 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.914803
I1129 03:53:09.402407 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.361826 (* 1 = 0.361826 loss)
I1129 03:54:17.046133 30336 solver.cpp:218] Iteration 40600 (1.17692 iter/s, 169.935s/200 iters), loss = 0.00184802
I1129 03:54:17.046264 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00184809 (* 1 = 0.00184809 loss)
I1129 03:54:17.046272 30336 sgd_solver.cpp:105] Iteration 40600, lr = 0.01
I1129 03:54:30.458006 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:56:31.046876 30336 solver.cpp:218] Iteration 40800 (1.49258 iter/s, 133.996s/200 iters), loss = 0.0105561
I1129 03:56:31.046995 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0105562 (* 1 = 0.0105562 loss)
I1129 03:56:31.047003 30336 sgd_solver.cpp:105] Iteration 40800, lr = 0.01
I1129 03:58:44.372154 30336 solver.cpp:330] Iteration 41000, Testing net (#0)
I1129 03:59:02.196655 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:59:20.175628 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 03:59:20.321246 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.916103
I1129 03:59:20.321267 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.361877 (* 1 = 0.361877 loss)
I1129 03:59:20.987648 30336 solver.cpp:218] Iteration 41000 (1.17692 iter/s, 169.935s/200 iters), loss = 0.00153787
I1129 03:59:20.987677 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00153794 (* 1 = 0.00153794 loss)
I1129 03:59:20.987684 30336 sgd_solver.cpp:105] Iteration 41000, lr = 0.01
I1129 03:59:28.362432 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:01:34.923740 30336 solver.cpp:218] Iteration 41200 (1.4933 iter/s, 133.931s/200 iters), loss = 0.0012966
I1129 04:01:34.923835 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00129668 (* 1 = 0.00129668 loss)
I1129 04:01:34.923842 30336 sgd_solver.cpp:105] Iteration 41200, lr = 0.01
I1129 04:03:48.946753 30336 solver.cpp:218] Iteration 41400 (1.49234 iter/s, 134.018s/200 iters), loss = 0.00255382
I1129 04:03:48.947278 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00255389 (* 1 = 0.00255389 loss)
I1129 04:03:48.947285 30336 sgd_solver.cpp:105] Iteration 41400, lr = 0.01
I1129 04:03:50.302136 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:04:55.286914 30336 solver.cpp:330] Iteration 41500, Testing net (#0)
I1129 04:05:13.129582 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:05:31.103487 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:05:31.248747 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915303
I1129 04:05:31.248767 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.377151 (* 1 = 0.377151 loss)
I1129 04:06:38.897516 30336 solver.cpp:218] Iteration 41600 (1.17686 iter/s, 169.944s/200 iters), loss = 0.000633556
I1129 04:06:38.897653 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000633628 (* 1 = 0.000633628 loss)
I1129 04:06:38.897675 30336 sgd_solver.cpp:105] Iteration 41600, lr = 0.01
I1129 04:08:47.567652 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:08:52.922499 30336 solver.cpp:218] Iteration 41800 (1.49231 iter/s, 134.02s/200 iters), loss = 0.00164402
I1129 04:08:52.922526 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00164409 (* 1 = 0.00164409 loss)
I1129 04:08:52.922533 30336 sgd_solver.cpp:105] Iteration 41800, lr = 0.01
I1129 04:11:06.215145 30336 solver.cpp:330] Iteration 42000, Testing net (#0)
I1129 04:11:24.055965 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:11:42.044226 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:11:42.189981 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.914703
I1129 04:11:42.190002 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.371 (* 1 = 0.371 loss)
I1129 04:11:42.859019 30336 solver.cpp:218] Iteration 42000 (1.17695 iter/s, 169.93s/200 iters), loss = 0.00149368
I1129 04:11:42.859047 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00149375 (* 1 = 0.00149375 loss)
I1129 04:11:42.859052 30336 sgd_solver.cpp:105] Iteration 42000, lr = 0.01
I1129 04:13:45.519690 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:13:56.902603 30336 solver.cpp:218] Iteration 42200 (1.49211 iter/s, 134.039s/200 iters), loss = 0.00822354
I1129 04:13:56.902631 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00822361 (* 1 = 0.00822361 loss)
I1129 04:13:56.902634 30336 sgd_solver.cpp:105] Iteration 42200, lr = 0.01
I1129 04:16:10.903142 30336 solver.cpp:218] Iteration 42400 (1.49259 iter/s, 133.995s/200 iters), loss = 0.00145695
I1129 04:16:10.903270 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00145702 (* 1 = 0.00145702 loss)
I1129 04:16:10.903278 30336 sgd_solver.cpp:105] Iteration 42400, lr = 0.01
I1129 04:17:17.243504 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_42500.caffemodel
I1129 04:17:17.382342 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_42500.solverstate
I1129 04:17:17.453018 30336 solver.cpp:330] Iteration 42500, Testing net (#0)
I1129 04:17:35.297160 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:17:53.282033 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:17:53.427844 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915103
I1129 04:17:53.427863 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.368965 (* 1 = 0.368965 loss)
I1129 04:18:43.655967 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:19:01.061858 30336 solver.cpp:218] Iteration 42600 (1.17542 iter/s, 170.152s/200 iters), loss = 0.000981491
I1129 04:19:01.061887 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000981557 (* 1 = 0.000981557 loss)
I1129 04:19:01.061892 30336 sgd_solver.cpp:105] Iteration 42600, lr = 0.01
I1129 04:21:14.980517 30336 solver.cpp:218] Iteration 42800 (1.4935 iter/s, 133.914s/200 iters), loss = 0.00236058
I1129 04:21:14.980669 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00236065 (* 1 = 0.00236065 loss)
I1129 04:21:14.980676 30336 sgd_solver.cpp:105] Iteration 42800, lr = 0.01
I1129 04:23:04.833935 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:23:28.272863 30336 solver.cpp:330] Iteration 43000, Testing net (#0)
I1129 04:23:46.114384 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:24:04.098042 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:24:04.243432 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.914303
I1129 04:24:04.243453 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.3719 (* 1 = 0.3719 loss)
I1129 04:24:04.911161 30336 solver.cpp:218] Iteration 43000 (1.177 iter/s, 169.924s/200 iters), loss = 0.000905434
I1129 04:24:04.911190 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000905496 (* 1 = 0.000905496 loss)
I1129 04:24:04.911195 30336 sgd_solver.cpp:105] Iteration 43000, lr = 0.01
I1129 04:26:18.858975 30336 solver.cpp:218] Iteration 43200 (1.49317 iter/s, 133.943s/200 iters), loss = 0.00120728
I1129 04:26:18.859146 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00120734 (* 1 = 0.00120734 loss)
I1129 04:26:18.859153 30336 sgd_solver.cpp:105] Iteration 43200, lr = 0.01
I1129 04:28:02.687765 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:28:32.823930 30336 solver.cpp:218] Iteration 43400 (1.49298 iter/s, 133.96s/200 iters), loss = 0.000829612
I1129 04:28:32.824060 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000829671 (* 1 = 0.000829671 loss)
I1129 04:28:32.824069 30336 sgd_solver.cpp:105] Iteration 43400, lr = 0.01
I1129 04:29:39.141623 30336 solver.cpp:330] Iteration 43500, Testing net (#0)
I1129 04:29:56.968822 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:30:14.937355 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:30:15.081890 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.914203
I1129 04:30:15.081910 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.378684 (* 1 = 0.378684 loss)
I1129 04:31:22.747046 30336 solver.cpp:218] Iteration 43600 (1.17705 iter/s, 169.917s/200 iters), loss = 0.00148066
I1129 04:31:22.747176 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00148072 (* 1 = 0.00148072 loss)
I1129 04:31:22.747185 30336 sgd_solver.cpp:105] Iteration 43600, lr = 0.01
I1129 04:32:59.899893 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:33:36.747412 30336 solver.cpp:218] Iteration 43800 (1.49259 iter/s, 133.995s/200 iters), loss = 0.0140661
I1129 04:33:36.747539 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0140661 (* 1 = 0.0140661 loss)
I1129 04:33:36.747547 30336 sgd_solver.cpp:105] Iteration 43800, lr = 0.01
I1129 04:35:50.094641 30336 solver.cpp:330] Iteration 44000, Testing net (#0)
I1129 04:36:07.916618 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:36:25.871181 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:36:26.015826 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.914803
I1129 04:36:26.015847 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.372701 (* 1 = 0.372701 loss)
I1129 04:36:26.682510 30336 solver.cpp:218] Iteration 44000 (1.17696 iter/s, 169.929s/200 iters), loss = 0.000412156
I1129 04:36:26.682538 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000412218 (* 1 = 0.000412218 loss)
I1129 04:36:26.682557 30336 sgd_solver.cpp:105] Iteration 44000, lr = 0.01
I1129 04:37:57.772853 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:38:40.632998 30336 solver.cpp:218] Iteration 44200 (1.49314 iter/s, 133.946s/200 iters), loss = 0.00525311
I1129 04:38:40.633126 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00525317 (* 1 = 0.00525317 loss)
I1129 04:38:40.633134 30336 sgd_solver.cpp:105] Iteration 44200, lr = 0.01
I1129 04:40:54.621009 30336 solver.cpp:218] Iteration 44400 (1.49273 iter/s, 133.983s/200 iters), loss = 0.00115979
I1129 04:40:54.621147 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00115985 (* 1 = 0.00115985 loss)
I1129 04:40:54.621155 30336 sgd_solver.cpp:105] Iteration 44400, lr = 0.01
I1129 04:42:00.952929 30336 solver.cpp:330] Iteration 44500, Testing net (#0)
I1129 04:42:18.777276 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:42:36.740938 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:42:36.886060 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.916303
I1129 04:42:36.886081 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.370335 (* 1 = 0.370335 loss)
I1129 04:42:55.641855 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:43:44.525282 30336 solver.cpp:218] Iteration 44600 (1.17718 iter/s, 169.898s/200 iters), loss = 0.000623541
I1129 04:43:44.525421 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000623597 (* 1 = 0.000623597 loss)
I1129 04:43:44.525429 30336 sgd_solver.cpp:105] Iteration 44600, lr = 0.01
I1129 04:45:58.454568 30336 solver.cpp:218] Iteration 44800 (1.49338 iter/s, 133.924s/200 iters), loss = 0.00183398
I1129 04:45:58.454710 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00183404 (* 1 = 0.00183404 loss)
I1129 04:45:58.454732 30336 sgd_solver.cpp:105] Iteration 44800, lr = 0.01
I1129 04:47:16.828775 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:48:11.735437 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_45000.caffemodel
I1129 04:48:11.875037 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_45000.solverstate
I1129 04:48:11.943080 30336 solver.cpp:330] Iteration 45000, Testing net (#0)
I1129 04:48:29.764855 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:48:47.732810 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:48:47.876348 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915203
I1129 04:48:47.876369 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.373741 (* 1 = 0.373741 loss)
I1129 04:48:48.543572 30336 solver.cpp:218] Iteration 45000 (1.1759 iter/s, 170.083s/200 iters), loss = 0.000770153
I1129 04:48:48.543601 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000770208 (* 1 = 0.000770208 loss)
I1129 04:48:48.543623 30336 sgd_solver.cpp:105] Iteration 45000, lr = 0.01
I1129 04:51:02.528287 30336 solver.cpp:218] Iteration 45200 (1.49276 iter/s, 133.98s/200 iters), loss = 0.000275167
I1129 04:51:02.528429 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000275225 (* 1 = 0.000275225 loss)
I1129 04:51:02.528436 30336 sgd_solver.cpp:105] Iteration 45200, lr = 0.01
I1129 04:52:14.897109 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:53:16.532997 30336 solver.cpp:218] Iteration 45400 (1.49254 iter/s, 134s/200 iters), loss = 0.00302084
I1129 04:53:16.533125 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00302089 (* 1 = 0.00302089 loss)
I1129 04:53:16.533133 30336 sgd_solver.cpp:105] Iteration 45400, lr = 0.01
I1129 04:54:22.847057 30336 solver.cpp:330] Iteration 45500, Testing net (#0)
I1129 04:54:40.672751 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:54:58.636490 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:54:58.781334 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915703
I1129 04:54:58.781355 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.369162 (* 1 = 0.369162 loss)
I1129 04:56:06.442399 30336 solver.cpp:218] Iteration 45600 (1.17714 iter/s, 169.903s/200 iters), loss = 0.00124839
I1129 04:56:06.442555 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00124845 (* 1 = 0.00124845 loss)
I1129 04:56:06.442564 30336 sgd_solver.cpp:105] Iteration 45600, lr = 0.01
I1129 04:57:12.777675 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 04:58:20.438051 30336 solver.cpp:218] Iteration 45800 (1.49264 iter/s, 133.991s/200 iters), loss = 0.00603991
I1129 04:58:20.438172 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00603997 (* 1 = 0.00603997 loss)
I1129 04:58:20.438179 30336 sgd_solver.cpp:105] Iteration 45800, lr = 0.01
I1129 05:00:33.763303 30336 solver.cpp:330] Iteration 46000, Testing net (#0)
I1129 05:00:51.589620 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:01:09.568639 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:01:09.714200 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915903
I1129 05:01:09.714221 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.371366 (* 1 = 0.371366 loss)
I1129 05:01:10.384471 30336 solver.cpp:218] Iteration 46000 (1.17689 iter/s, 169.94s/200 iters), loss = 0.000636196
I1129 05:01:10.384511 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000636252 (* 1 = 0.000636252 loss)
I1129 05:01:10.384516 30336 sgd_solver.cpp:105] Iteration 46000, lr = 0.01
I1129 05:02:09.998944 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:03:24.321239 30336 solver.cpp:218] Iteration 46200 (1.4933 iter/s, 133.932s/200 iters), loss = 0.000790249
I1129 05:03:24.321382 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000790304 (* 1 = 0.000790304 loss)
I1129 05:03:24.321389 30336 sgd_solver.cpp:105] Iteration 46200, lr = 0.01
I1129 05:05:38.276892 30336 solver.cpp:218] Iteration 46400 (1.49309 iter/s, 133.951s/200 iters), loss = 0.000321869
I1129 05:05:38.277004 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000321926 (* 1 = 0.000321926 loss)
I1129 05:05:38.277009 30336 sgd_solver.cpp:105] Iteration 46400, lr = 0.01
I1129 05:06:31.869441 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:06:44.589048 30336 solver.cpp:330] Iteration 46500, Testing net (#0)
I1129 05:07:02.418606 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:07:20.379842 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:07:20.526525 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915703
I1129 05:07:20.526547 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.371652 (* 1 = 0.371652 loss)
I1129 05:08:28.190975 30336 solver.cpp:218] Iteration 46600 (1.17711 iter/s, 169.908s/200 iters), loss = 0.00124388
I1129 05:08:28.191067 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00124393 (* 1 = 0.00124393 loss)
I1129 05:08:28.191074 30336 sgd_solver.cpp:105] Iteration 46600, lr = 0.01
I1129 05:10:42.159314 30336 solver.cpp:218] Iteration 46800 (1.49295 iter/s, 133.963s/200 iters), loss = 0.000675714
I1129 05:10:42.159401 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000675774 (* 1 = 0.000675774 loss)
I1129 05:10:42.159407 30336 sgd_solver.cpp:105] Iteration 46800, lr = 0.01
I1129 05:11:29.065197 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:12:55.487761 30336 solver.cpp:330] Iteration 47000, Testing net (#0)
I1129 05:13:13.336108 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:13:31.313571 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:13:31.458559 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.916903
I1129 05:13:31.458578 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.375422 (* 1 = 0.375422 loss)
I1129 05:13:32.125618 30336 solver.cpp:218] Iteration 47000 (1.17675 iter/s, 169.96s/200 iters), loss = 0.00056292
I1129 05:13:32.125646 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000562982 (* 1 = 0.000562982 loss)
I1129 05:13:32.125653 30336 sgd_solver.cpp:105] Iteration 47000, lr = 0.01
I1129 05:15:46.091934 30336 solver.cpp:218] Iteration 47200 (1.49297 iter/s, 133.961s/200 iters), loss = 0.00146261
I1129 05:15:46.092075 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00146267 (* 1 = 0.00146267 loss)
I1129 05:15:46.092084 30336 sgd_solver.cpp:105] Iteration 47200, lr = 0.01
I1129 05:16:26.968984 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:18:00.039368 30336 solver.cpp:218] Iteration 47400 (1.49318 iter/s, 133.942s/200 iters), loss = 0.00164116
I1129 05:18:00.039525 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00164122 (* 1 = 0.00164122 loss)
I1129 05:18:00.039548 30336 sgd_solver.cpp:105] Iteration 47400, lr = 0.01
I1129 05:19:06.351716 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_47500.caffemodel
I1129 05:19:06.491039 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_47500.solverstate
I1129 05:19:06.559686 30336 solver.cpp:330] Iteration 47500, Testing net (#0)
I1129 05:19:24.377704 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:19:42.347084 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:19:42.493098 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917003
I1129 05:19:42.493119 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.367789 (* 1 = 0.367789 loss)
I1129 05:20:50.145305 30336 solver.cpp:218] Iteration 47600 (1.17578 iter/s, 170.1s/200 iters), loss = 0.000655508
I1129 05:20:50.145431 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000655569 (* 1 = 0.000655569 loss)
I1129 05:20:50.145437 30336 sgd_solver.cpp:105] Iteration 47600, lr = 0.01
I1129 05:21:24.983016 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:23:04.129901 30336 solver.cpp:218] Iteration 47800 (1.49277 iter/s, 133.98s/200 iters), loss = 0.00097685
I1129 05:23:04.130039 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000976911 (* 1 = 0.000976911 loss)
I1129 05:23:04.130048 30336 sgd_solver.cpp:105] Iteration 47800, lr = 0.01
I1129 05:25:17.419579 30336 solver.cpp:330] Iteration 48000, Testing net (#0)
I1129 05:25:35.257182 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:25:53.210464 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:25:53.355931 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917703
I1129 05:25:53.355967 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.37124 (* 1 = 0.37124 loss)
I1129 05:25:54.023591 30336 solver.cpp:218] Iteration 48000 (1.17725 iter/s, 169.888s/200 iters), loss = 0.000927724
I1129 05:25:54.023622 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000927783 (* 1 = 0.000927783 loss)
I1129 05:25:54.023640 30336 sgd_solver.cpp:46] MultiStep Status: Iteration 48000, step = 2
I1129 05:25:54.023644 30336 sgd_solver.cpp:105] Iteration 48000, lr = 0.001
I1129 05:26:22.169005 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:28:07.979014 30336 solver.cpp:218] Iteration 48200 (1.49309 iter/s, 133.951s/200 iters), loss = 0.00085271
I1129 05:28:07.979156 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00085277 (* 1 = 0.00085277 loss)
I1129 05:28:07.979179 30336 sgd_solver.cpp:105] Iteration 48200, lr = 0.001
I1129 05:30:21.863514 30336 solver.cpp:218] Iteration 48400 (1.49388 iter/s, 133.88s/200 iters), loss = 0.000821866
I1129 05:30:21.863634 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000821926 (* 1 = 0.000821926 loss)
I1129 05:30:21.863641 30336 sgd_solver.cpp:105] Iteration 48400, lr = 0.001
I1129 05:30:43.976258 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:31:28.159090 30336 solver.cpp:330] Iteration 48500, Testing net (#0)
I1129 05:31:45.993959 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:32:03.975878 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:32:04.121063 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.920203
I1129 05:32:04.121083 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.362422 (* 1 = 0.362422 loss)
I1129 05:33:11.798806 30336 solver.cpp:218] Iteration 48600 (1.17696 iter/s, 169.929s/200 iters), loss = 0.00062345
I1129 05:33:11.798943 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00062351 (* 1 = 0.00062351 loss)
I1129 05:33:11.798950 30336 sgd_solver.cpp:105] Iteration 48600, lr = 0.001
I1129 05:35:25.808161 30336 solver.cpp:218] Iteration 48800 (1.49249 iter/s, 134.004s/200 iters), loss = 0.00287493
I1129 05:35:25.808277 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00287499 (* 1 = 0.00287499 loss)
I1129 05:35:25.808284 30336 sgd_solver.cpp:105] Iteration 48800, lr = 0.001
I1129 05:35:41.900059 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:37:39.151540 30336 solver.cpp:330] Iteration 49000, Testing net (#0)
I1129 05:37:56.979049 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:38:14.955288 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:38:15.100777 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.920503
I1129 05:38:15.100798 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.358665 (* 1 = 0.358665 loss)
I1129 05:38:15.769059 30336 solver.cpp:218] Iteration 49000 (1.17678 iter/s, 169.955s/200 iters), loss = 0.00313757
I1129 05:38:15.769088 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00313763 (* 1 = 0.00313763 loss)
I1129 05:38:15.769093 30336 sgd_solver.cpp:105] Iteration 49000, lr = 0.001
I1129 05:40:29.755336 30336 solver.cpp:218] Iteration 49200 (1.49274 iter/s, 133.981s/200 iters), loss = 0.00110741
I1129 05:40:29.755496 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00110747 (* 1 = 0.00110747 loss)
I1129 05:40:29.755502 30336 sgd_solver.cpp:105] Iteration 49200, lr = 0.001
I1129 05:40:39.145828 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:42:43.756048 30336 solver.cpp:218] Iteration 49400 (1.49259 iter/s, 133.996s/200 iters), loss = 0.00103782
I1129 05:42:43.756208 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00103788 (* 1 = 0.00103788 loss)
I1129 05:42:43.756217 30336 sgd_solver.cpp:105] Iteration 49400, lr = 0.001
I1129 05:43:50.091781 30336 solver.cpp:330] Iteration 49500, Testing net (#0)
I1129 05:44:07.916908 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:44:25.882688 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:44:26.027506 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.920403
I1129 05:44:26.027526 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.357667 (* 1 = 0.357667 loss)
I1129 05:45:33.670760 30336 solver.cpp:218] Iteration 49600 (1.1771 iter/s, 169.908s/200 iters), loss = 0.000610815
I1129 05:45:33.670884 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000610874 (* 1 = 0.000610874 loss)
I1129 05:45:33.670892 30336 sgd_solver.cpp:105] Iteration 49600, lr = 0.001
I1129 05:45:37.032711 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:47:47.625252 30336 solver.cpp:218] Iteration 49800 (1.4931 iter/s, 133.949s/200 iters), loss = 0.00182674
I1129 05:47:47.625389 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0018268 (* 1 = 0.0018268 loss)
I1129 05:47:47.625396 30336 sgd_solver.cpp:105] Iteration 49800, lr = 0.001
I1129 05:49:58.242955 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:50:00.918637 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_50000.caffemodel
I1129 05:50:01.057910 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_50000.solverstate
I1129 05:50:01.126353 30336 solver.cpp:330] Iteration 50000, Testing net (#0)
I1129 05:50:18.952656 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:50:36.922341 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:50:37.067847 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921403
I1129 05:50:37.067867 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.357722 (* 1 = 0.357722 loss)
I1129 05:50:37.735961 30336 solver.cpp:218] Iteration 50000 (1.17575 iter/s, 170.104s/200 iters), loss = 0.000354532
I1129 05:50:37.736006 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000354592 (* 1 = 0.000354592 loss)
I1129 05:50:37.736011 30336 sgd_solver.cpp:105] Iteration 50000, lr = 0.001
I1129 05:52:51.759654 30336 solver.cpp:218] Iteration 50200 (1.49233 iter/s, 134.019s/200 iters), loss = 0.00263987
I1129 05:52:51.759754 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00263993 (* 1 = 0.00263993 loss)
I1129 05:52:51.759760 30336 sgd_solver.cpp:105] Iteration 50200, lr = 0.001
I1129 05:54:56.334466 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:55:05.706461 30336 solver.cpp:218] Iteration 50400 (1.49319 iter/s, 133.942s/200 iters), loss = 0.000557876
I1129 05:55:05.706487 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000557934 (* 1 = 0.000557934 loss)
I1129 05:55:05.706507 30336 sgd_solver.cpp:105] Iteration 50400, lr = 0.001
I1129 05:56:12.002638 30336 solver.cpp:330] Iteration 50500, Testing net (#0)
I1129 05:56:29.840189 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:56:47.824221 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 05:56:47.969595 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921103
I1129 05:56:47.969617 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.357662 (* 1 = 0.357662 loss)
I1129 05:57:55.605103 30336 solver.cpp:218] Iteration 50600 (1.17721 iter/s, 169.893s/200 iters), loss = 0.00159776
I1129 05:57:55.605227 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00159782 (* 1 = 0.00159782 loss)
I1129 05:57:55.605234 30336 sgd_solver.cpp:105] Iteration 50600, lr = 0.001
I1129 05:59:54.161749 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:00:09.554548 30336 solver.cpp:218] Iteration 50800 (1.49315 iter/s, 133.945s/200 iters), loss = 0.00367383
I1129 06:00:09.554576 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00367389 (* 1 = 0.00367389 loss)
I1129 06:00:09.554580 30336 sgd_solver.cpp:105] Iteration 50800, lr = 0.001
I1129 06:02:22.835170 30336 solver.cpp:330] Iteration 51000, Testing net (#0)
I1129 06:02:40.675796 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:02:58.652861 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:02:58.798133 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.920703
I1129 06:02:58.798154 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.357331 (* 1 = 0.357331 loss)
I1129 06:02:59.464303 30336 solver.cpp:218] Iteration 51000 (1.17713 iter/s, 169.904s/200 iters), loss = 0.000782552
I1129 06:02:59.464329 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000782612 (* 1 = 0.000782612 loss)
I1129 06:02:59.464334 30336 sgd_solver.cpp:105] Iteration 51000, lr = 0.001
I1129 06:04:51.331838 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:05:13.427278 30336 solver.cpp:218] Iteration 51200 (1.493 iter/s, 133.958s/200 iters), loss = 0.000955031
I1129 06:05:13.427304 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000955089 (* 1 = 0.000955089 loss)
I1129 06:05:13.427309 30336 sgd_solver.cpp:105] Iteration 51200, lr = 0.001
I1129 06:07:27.393903 30336 solver.cpp:218] Iteration 51400 (1.49296 iter/s, 133.962s/200 iters), loss = 0.00132872
I1129 06:07:27.394042 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00132878 (* 1 = 0.00132878 loss)
I1129 06:07:27.394049 30336 sgd_solver.cpp:105] Iteration 51400, lr = 0.001
I1129 06:08:33.706949 30336 solver.cpp:330] Iteration 51500, Testing net (#0)
I1129 06:08:51.525511 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:09:09.494009 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:09:09.638803 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.920603
I1129 06:09:09.638823 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.357086 (* 1 = 0.357086 loss)
I1129 06:09:49.163430 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:10:17.295241 30336 solver.cpp:218] Iteration 51600 (1.1772 iter/s, 169.895s/200 iters), loss = 0.000746828
I1129 06:10:17.295267 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000746885 (* 1 = 0.000746885 loss)
I1129 06:10:17.295272 30336 sgd_solver.cpp:105] Iteration 51600, lr = 0.001
I1129 06:12:31.280828 30336 solver.cpp:218] Iteration 51800 (1.49275 iter/s, 133.981s/200 iters), loss = 0.000581736
I1129 06:12:31.280953 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000581796 (* 1 = 0.000581796 loss)
I1129 06:12:31.280975 30336 sgd_solver.cpp:105] Iteration 51800, lr = 0.001
I1129 06:14:11.131093 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:14:44.635517 30336 solver.cpp:330] Iteration 52000, Testing net (#0)
I1129 06:15:02.458811 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:15:20.431944 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:15:20.577253 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921103
I1129 06:15:20.577272 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.357077 (* 1 = 0.357077 loss)
I1129 06:15:21.243957 30336 solver.cpp:218] Iteration 52000 (1.17677 iter/s, 169.957s/200 iters), loss = 0.0123648
I1129 06:15:21.243995 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0123648 (* 1 = 0.0123648 loss)
I1129 06:15:21.244001 30336 sgd_solver.cpp:105] Iteration 52000, lr = 0.001
I1129 06:17:35.181262 30336 solver.cpp:218] Iteration 52200 (1.49329 iter/s, 133.933s/200 iters), loss = 0.00166533
I1129 06:17:35.181385 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00166539 (* 1 = 0.00166539 loss)
I1129 06:17:35.181391 30336 sgd_solver.cpp:105] Iteration 52200, lr = 0.001
I1129 06:19:08.267444 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:19:49.103679 30336 solver.cpp:218] Iteration 52400 (1.49346 iter/s, 133.918s/200 iters), loss = 0.000904462
I1129 06:19:49.103811 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000904519 (* 1 = 0.000904519 loss)
I1129 06:19:49.103833 30336 sgd_solver.cpp:105] Iteration 52400, lr = 0.001
I1129 06:20:55.410765 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_52500.caffemodel
I1129 06:20:55.552660 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_52500.solverstate
I1129 06:20:55.620597 30336 solver.cpp:330] Iteration 52500, Testing net (#0)
I1129 06:21:13.457484 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:21:31.423728 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:21:31.569013 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.920803
I1129 06:21:31.569033 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.357058 (* 1 = 0.357058 loss)
I1129 06:22:39.191944 30336 solver.cpp:218] Iteration 52600 (1.1759 iter/s, 170.082s/200 iters), loss = 0.00067891
I1129 06:22:39.192072 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000678965 (* 1 = 0.000678965 loss)
I1129 06:22:39.192095 30336 sgd_solver.cpp:105] Iteration 52600, lr = 0.001
I1129 06:24:06.252053 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:24:53.125121 30336 solver.cpp:218] Iteration 52800 (1.49334 iter/s, 133.928s/200 iters), loss = 0.00289971
I1129 06:24:53.125247 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00289977 (* 1 = 0.00289977 loss)
I1129 06:24:53.125268 30336 sgd_solver.cpp:105] Iteration 52800, lr = 0.001
I1129 06:27:06.438778 30336 solver.cpp:330] Iteration 53000, Testing net (#0)
I1129 06:27:24.272195 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:27:42.241997 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:27:42.387363 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921303
I1129 06:27:42.387385 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356754 (* 1 = 0.356754 loss)
I1129 06:27:43.056473 30336 solver.cpp:218] Iteration 53000 (1.17699 iter/s, 169.925s/200 iters), loss = 0.00168326
I1129 06:27:43.056501 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00168331 (* 1 = 0.00168331 loss)
I1129 06:27:43.056507 30336 sgd_solver.cpp:105] Iteration 53000, lr = 0.001
I1129 06:29:03.466408 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:29:57.051422 30336 solver.cpp:218] Iteration 53200 (1.49265 iter/s, 133.99s/200 iters), loss = 0.000649976
I1129 06:29:57.051539 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00065003 (* 1 = 0.00065003 loss)
I1129 06:29:57.051563 30336 sgd_solver.cpp:105] Iteration 53200, lr = 0.001
I1129 06:32:11.027380 30336 solver.cpp:218] Iteration 53400 (1.49286 iter/s, 133.971s/200 iters), loss = 0.000242277
I1129 06:32:11.027520 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000242331 (* 1 = 0.000242331 loss)
I1129 06:32:11.027542 30336 sgd_solver.cpp:105] Iteration 53400, lr = 0.001
I1129 06:33:17.337069 30336 solver.cpp:330] Iteration 53500, Testing net (#0)
I1129 06:33:35.155828 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:33:53.125141 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:33:53.270823 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921003
I1129 06:33:53.270843 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356742 (* 1 = 0.356742 loss)
I1129 06:34:01.318917 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:35:00.914218 30336 solver.cpp:218] Iteration 53600 (1.1773 iter/s, 169.881s/200 iters), loss = 0.000937911
I1129 06:35:00.914355 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000937965 (* 1 = 0.000937965 loss)
I1129 06:35:00.914363 30336 sgd_solver.cpp:105] Iteration 53600, lr = 0.001
I1129 06:37:14.877748 30336 solver.cpp:218] Iteration 53800 (1.493 iter/s, 133.959s/200 iters), loss = 0.000658152
I1129 06:37:14.877840 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000658206 (* 1 = 0.000658206 loss)
I1129 06:37:14.877846 30336 sgd_solver.cpp:105] Iteration 53800, lr = 0.001
I1129 06:38:23.204311 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:39:28.171990 30336 solver.cpp:330] Iteration 54000, Testing net (#0)
I1129 06:39:46.009652 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:40:03.995175 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:40:04.140522 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921203
I1129 06:40:04.140542 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356949 (* 1 = 0.356949 loss)
I1129 06:40:04.806800 30336 solver.cpp:218] Iteration 54000 (1.177 iter/s, 169.923s/200 iters), loss = 0.000530815
I1129 06:40:04.806828 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000530869 (* 1 = 0.000530869 loss)
I1129 06:40:04.806834 30336 sgd_solver.cpp:105] Iteration 54000, lr = 0.001
I1129 06:42:18.719300 30336 solver.cpp:218] Iteration 54200 (1.49357 iter/s, 133.908s/200 iters), loss = 0.0010945
I1129 06:42:18.719388 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00109455 (* 1 = 0.00109455 loss)
I1129 06:42:18.719393 30336 sgd_solver.cpp:105] Iteration 54200, lr = 0.001
I1129 06:43:20.334185 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:44:32.632812 30336 solver.cpp:218] Iteration 54400 (1.49355 iter/s, 133.909s/200 iters), loss = 0.000874127
I1129 06:44:32.632972 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000874182 (* 1 = 0.000874182 loss)
I1129 06:44:32.632978 30336 sgd_solver.cpp:105] Iteration 54400, lr = 0.001
I1129 06:45:38.939905 30336 solver.cpp:330] Iteration 54500, Testing net (#0)
I1129 06:45:56.767711 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:46:14.734866 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:46:14.879854 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921203
I1129 06:46:14.879874 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.35647 (* 1 = 0.35647 loss)
I1129 06:47:22.532224 30336 solver.cpp:218] Iteration 54600 (1.17721 iter/s, 169.893s/200 iters), loss = 0.000561776
I1129 06:47:22.532368 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000561832 (* 1 = 0.000561832 loss)
I1129 06:47:22.532377 30336 sgd_solver.cpp:105] Iteration 54600, lr = 0.001
I1129 06:48:18.174968 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:49:36.583890 30336 solver.cpp:218] Iteration 54800 (1.49202 iter/s, 134.047s/200 iters), loss = 0.000995242
I1129 06:49:36.584002 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000995298 (* 1 = 0.000995298 loss)
I1129 06:49:36.584020 30336 sgd_solver.cpp:105] Iteration 54800, lr = 0.001
I1129 06:51:49.963414 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_55000.caffemodel
I1129 06:51:50.102110 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_55000.solverstate
I1129 06:51:50.169637 30336 solver.cpp:330] Iteration 55000, Testing net (#0)
I1129 06:52:08.006114 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:52:25.987783 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:52:26.133368 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921203
I1129 06:52:26.133389 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356386 (* 1 = 0.356386 loss)
I1129 06:52:26.800130 30336 solver.cpp:218] Iteration 55000 (1.17502 iter/s, 170.21s/200 iters), loss = 0.00647514
I1129 06:52:26.800159 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0064752 (* 1 = 0.0064752 loss)
I1129 06:52:26.800164 30336 sgd_solver.cpp:105] Iteration 55000, lr = 0.001
I1129 06:53:16.381389 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:54:40.822378 30336 solver.cpp:218] Iteration 55200 (1.49234 iter/s, 134.017s/200 iters), loss = 0.00244806
I1129 06:54:40.822474 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00244812 (* 1 = 0.00244812 loss)
I1129 06:54:40.822480 30336 sgd_solver.cpp:105] Iteration 55200, lr = 0.001
I1129 06:56:54.840101 30336 solver.cpp:218] Iteration 55400 (1.49239 iter/s, 134.013s/200 iters), loss = 0.00420641
I1129 06:56:54.840246 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00420647 (* 1 = 0.00420647 loss)
I1129 06:56:54.840255 30336 sgd_solver.cpp:105] Iteration 55400, lr = 0.001
I1129 06:57:37.747275 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:58:01.196244 30336 solver.cpp:330] Iteration 55500, Testing net (#0)
I1129 06:58:19.014381 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:58:36.978081 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 06:58:37.123486 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921103
I1129 06:58:37.123507 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356566 (* 1 = 0.356566 loss)
I1129 06:59:44.725781 30336 solver.cpp:218] Iteration 55600 (1.1773 iter/s, 169.88s/200 iters), loss = 0.000977695
I1129 06:59:44.725875 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000977753 (* 1 = 0.000977753 loss)
I1129 06:59:44.725894 30336 sgd_solver.cpp:105] Iteration 55600, lr = 0.001
I1129 07:01:58.619112 30336 solver.cpp:218] Iteration 55800 (1.49378 iter/s, 133.888s/200 iters), loss = 0.00060038
I1129 07:01:58.619221 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00060044 (* 1 = 0.00060044 loss)
I1129 07:01:58.619230 30336 sgd_solver.cpp:105] Iteration 55800, lr = 0.001
I1129 07:02:35.463856 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:04:11.887307 30336 solver.cpp:330] Iteration 56000, Testing net (#0)
I1129 07:04:29.710152 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:04:47.691087 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:04:47.835991 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921003
I1129 07:04:47.836012 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356376 (* 1 = 0.356376 loss)
I1129 07:04:48.503391 30336 solver.cpp:218] Iteration 56000 (1.17731 iter/s, 169.878s/200 iters), loss = 0.00141633
I1129 07:04:48.503432 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00141639 (* 1 = 0.00141639 loss)
I1129 07:04:48.503438 30336 sgd_solver.cpp:105] Iteration 56000, lr = 0.001
I1129 07:07:02.517968 30336 solver.cpp:218] Iteration 56200 (1.49243 iter/s, 134.01s/200 iters), loss = 0.000506962
I1129 07:07:02.518142 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000507023 (* 1 = 0.000507023 loss)
I1129 07:07:02.518151 30336 sgd_solver.cpp:105] Iteration 56200, lr = 0.001
I1129 07:07:32.684449 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:09:16.560079 30336 solver.cpp:218] Iteration 56400 (1.49212 iter/s, 134.037s/200 iters), loss = 0.00106953
I1129 07:09:16.560238 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00106959 (* 1 = 0.00106959 loss)
I1129 07:09:16.560246 30336 sgd_solver.cpp:105] Iteration 56400, lr = 0.001
I1129 07:10:22.913830 30336 solver.cpp:330] Iteration 56500, Testing net (#0)
I1129 07:10:40.745405 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:10:58.719812 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:10:58.864995 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921303
I1129 07:10:58.865016 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356424 (* 1 = 0.356424 loss)
I1129 07:12:06.483599 30336 solver.cpp:218] Iteration 56600 (1.17704 iter/s, 169.917s/200 iters), loss = 0.00117206
I1129 07:12:06.483722 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00117212 (* 1 = 0.00117212 loss)
I1129 07:12:06.483729 30336 sgd_solver.cpp:105] Iteration 56600, lr = 0.001
I1129 07:12:30.593719 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:14:20.404939 30336 solver.cpp:218] Iteration 56800 (1.49347 iter/s, 133.917s/200 iters), loss = 0.00090794
I1129 07:14:20.405056 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000908 (* 1 = 0.000908 loss)
I1129 07:14:20.405078 30336 sgd_solver.cpp:105] Iteration 56800, lr = 0.001
I1129 07:16:33.677251 30336 solver.cpp:330] Iteration 57000, Testing net (#0)
I1129 07:16:51.506230 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:17:09.496237 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:17:09.641822 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.920803
I1129 07:17:09.641844 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356322 (* 1 = 0.356322 loss)
I1129 07:17:10.309300 30336 solver.cpp:218] Iteration 57000 (1.17718 iter/s, 169.898s/200 iters), loss = 0.00282243
I1129 07:17:10.309326 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00282249 (* 1 = 0.00282249 loss)
I1129 07:17:10.309331 30336 sgd_solver.cpp:105] Iteration 57000, lr = 0.001
I1129 07:17:28.393833 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:19:24.195363 30336 solver.cpp:218] Iteration 57200 (1.49386 iter/s, 133.881s/200 iters), loss = 0.000591313
I1129 07:19:24.195489 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000591375 (* 1 = 0.000591375 loss)
I1129 07:19:24.195495 30336 sgd_solver.cpp:105] Iteration 57200, lr = 0.001
I1129 07:21:38.116196 30336 solver.cpp:218] Iteration 57400 (1.49347 iter/s, 133.916s/200 iters), loss = 0.000998802
I1129 07:21:38.116341 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000998863 (* 1 = 0.000998863 loss)
I1129 07:21:38.116348 30336 sgd_solver.cpp:105] Iteration 57400, lr = 0.001
I1129 07:21:49.505141 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:22:44.396632 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_57500.caffemodel
I1129 07:22:44.536042 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_57500.solverstate
I1129 07:22:44.604883 30336 solver.cpp:330] Iteration 57500, Testing net (#0)
I1129 07:23:02.432687 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:23:20.384109 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:23:20.529836 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921103
I1129 07:23:20.529857 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356346 (* 1 = 0.356346 loss)
I1129 07:24:28.144927 30336 solver.cpp:218] Iteration 57600 (1.17631 iter/s, 170.023s/200 iters), loss = 0.000494762
I1129 07:24:28.145047 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000494823 (* 1 = 0.000494823 loss)
I1129 07:24:28.145054 30336 sgd_solver.cpp:105] Iteration 57600, lr = 0.001
I1129 07:26:42.112062 30336 solver.cpp:218] Iteration 57800 (1.49296 iter/s, 133.962s/200 iters), loss = 0.00266808
I1129 07:26:42.112174 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00266814 (* 1 = 0.00266814 loss)
I1129 07:26:42.112180 30336 sgd_solver.cpp:105] Iteration 57800, lr = 0.001
I1129 07:26:47.487234 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:28:55.423681 30336 solver.cpp:330] Iteration 58000, Testing net (#0)
I1129 07:29:13.251775 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:29:31.219414 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:29:31.363514 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921203
I1129 07:29:31.363535 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356307 (* 1 = 0.356307 loss)
I1129 07:29:32.030364 30336 solver.cpp:218] Iteration 58000 (1.17708 iter/s, 169.912s/200 iters), loss = 0.000624225
I1129 07:29:32.030390 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000624285 (* 1 = 0.000624285 loss)
I1129 07:29:32.030396 30336 sgd_solver.cpp:105] Iteration 58000, lr = 0.001
I1129 07:31:45.317912 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:31:45.979944 30336 solver.cpp:218] Iteration 58200 (1.49315 iter/s, 133.945s/200 iters), loss = 0.000613939
I1129 07:31:45.979974 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000613999 (* 1 = 0.000613999 loss)
I1129 07:31:45.979979 30336 sgd_solver.cpp:105] Iteration 58200, lr = 0.001
I1129 07:33:59.930294 30336 solver.cpp:218] Iteration 58400 (1.49314 iter/s, 133.946s/200 iters), loss = 0.00193518
I1129 07:33:59.930388 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00193524 (* 1 = 0.00193524 loss)
I1129 07:33:59.930394 30336 sgd_solver.cpp:105] Iteration 58400, lr = 0.001
I1129 07:35:06.234338 30336 solver.cpp:330] Iteration 58500, Testing net (#0)
I1129 07:35:24.076414 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:35:42.044765 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:35:42.190243 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921903
I1129 07:35:42.190264 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356453 (* 1 = 0.356453 loss)
I1129 07:36:42.497496 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:36:49.859557 30336 solver.cpp:218] Iteration 58600 (1.177 iter/s, 169.923s/200 iters), loss = 0.000638331
I1129 07:36:49.859586 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00063839 (* 1 = 0.00063839 loss)
I1129 07:36:49.859591 30336 sgd_solver.cpp:105] Iteration 58600, lr = 0.001
I1129 07:39:03.889703 30336 solver.cpp:218] Iteration 58800 (1.49225 iter/s, 134.025s/200 iters), loss = 0.000563788
I1129 07:39:03.889858 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000563847 (* 1 = 0.000563847 loss)
I1129 07:39:03.889865 30336 sgd_solver.cpp:105] Iteration 58800, lr = 0.001
I1129 07:41:04.524580 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:41:17.253131 30336 solver.cpp:330] Iteration 59000, Testing net (#0)
I1129 07:41:35.086660 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:41:53.066792 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:41:53.212314 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.922003
I1129 07:41:53.212335 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356824 (* 1 = 0.356824 loss)
I1129 07:41:53.878082 30336 solver.cpp:218] Iteration 59000 (1.1766 iter/s, 169.982s/200 iters), loss = 0.00049643
I1129 07:41:53.878111 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000496491 (* 1 = 0.000496491 loss)
I1129 07:41:53.878131 30336 sgd_solver.cpp:105] Iteration 59000, lr = 0.001
I1129 07:44:07.860016 30336 solver.cpp:218] Iteration 59200 (1.4928 iter/s, 133.977s/200 iters), loss = 0.00302912
I1129 07:44:07.860152 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00302918 (* 1 = 0.00302918 loss)
I1129 07:44:07.860159 30336 sgd_solver.cpp:105] Iteration 59200, lr = 0.001
I1129 07:46:02.051318 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:46:22.175362 30336 solver.cpp:218] Iteration 59400 (1.48909 iter/s, 134.31s/200 iters), loss = 0.000424877
I1129 07:46:22.175402 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000424936 (* 1 = 0.000424936 loss)
I1129 07:46:22.175408 30336 sgd_solver.cpp:105] Iteration 59400, lr = 0.001
I1129 07:47:28.598816 30336 solver.cpp:330] Iteration 59500, Testing net (#0)
I1129 07:47:46.430279 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:48:04.406271 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:48:04.549914 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921303
I1129 07:48:04.549935 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356578 (* 1 = 0.356578 loss)
I1129 07:49:12.201984 30336 solver.cpp:218] Iteration 59600 (1.17633 iter/s, 170.02s/200 iters), loss = 0.00125284
I1129 07:49:12.202097 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0012529 (* 1 = 0.0012529 loss)
I1129 07:49:12.202102 30336 sgd_solver.cpp:105] Iteration 59600, lr = 0.001
I1129 07:51:00.070636 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:51:26.198177 30336 solver.cpp:218] Iteration 59800 (1.49264 iter/s, 133.991s/200 iters), loss = 0.000620859
I1129 07:51:26.198202 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000620919 (* 1 = 0.000620919 loss)
I1129 07:51:26.198207 30336 sgd_solver.cpp:105] Iteration 59800, lr = 0.001
I1129 07:53:39.543985 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_60000.caffemodel
I1129 07:53:39.689640 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_60000.solverstate
I1129 07:53:39.758522 30336 solver.cpp:330] Iteration 60000, Testing net (#0)
I1129 07:53:57.593358 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:54:15.552088 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:54:15.698356 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921003
I1129 07:54:15.698381 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356479 (* 1 = 0.356479 loss)
I1129 07:54:16.365974 30336 solver.cpp:218] Iteration 60000 (1.17535 iter/s, 170.161s/200 iters), loss = 0.000610544
I1129 07:54:16.366001 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000610604 (* 1 = 0.000610604 loss)
I1129 07:54:16.366021 30336 sgd_solver.cpp:105] Iteration 60000, lr = 0.001
I1129 07:55:58.197773 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 07:56:30.346896 30336 solver.cpp:218] Iteration 60200 (1.49281 iter/s, 133.976s/200 iters), loss = 0.00138369
I1129 07:56:30.347456 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00138375 (* 1 = 0.00138375 loss)
I1129 07:56:30.347476 30336 sgd_solver.cpp:105] Iteration 60200, lr = 0.001
I1129 07:58:44.342070 30336 solver.cpp:218] Iteration 60400 (1.49265 iter/s, 133.99s/200 iters), loss = 0.000482171
I1129 07:58:44.342214 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000482231 (* 1 = 0.000482231 loss)
I1129 07:58:44.342236 30336 sgd_solver.cpp:105] Iteration 60400, lr = 0.001
I1129 07:59:50.668663 30336 solver.cpp:330] Iteration 60500, Testing net (#0)
I1129 08:00:08.506819 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:00:26.484200 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:00:26.629570 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921203
I1129 08:00:26.629590 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355955 (* 1 = 0.355955 loss)
I1129 08:00:55.444309 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:01:34.293195 30336 solver.cpp:218] Iteration 60600 (1.17685 iter/s, 169.945s/200 iters), loss = 0.000816657
I1129 08:01:34.293337 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000816717 (* 1 = 0.000816717 loss)
I1129 08:01:34.293345 30336 sgd_solver.cpp:105] Iteration 60600, lr = 0.001
I1129 08:03:48.289881 30336 solver.cpp:218] Iteration 60800 (1.49263 iter/s, 133.992s/200 iters), loss = 0.000831665
I1129 08:03:48.290002 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000831724 (* 1 = 0.000831724 loss)
I1129 08:03:48.290009 30336 sgd_solver.cpp:105] Iteration 60800, lr = 0.001
I1129 08:05:17.424825 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:06:01.650710 30336 solver.cpp:330] Iteration 61000, Testing net (#0)
I1129 08:06:19.489907 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:06:37.453466 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:06:37.599200 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.920903
I1129 08:06:37.599220 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356144 (* 1 = 0.356144 loss)
I1129 08:06:38.265730 30336 solver.cpp:218] Iteration 61000 (1.17668 iter/s, 169.97s/200 iters), loss = 0.00113521
I1129 08:06:38.265758 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00113527 (* 1 = 0.00113527 loss)
I1129 08:06:38.265763 30336 sgd_solver.cpp:105] Iteration 61000, lr = 0.001
I1129 08:08:52.294584 30336 solver.cpp:218] Iteration 61200 (1.49227 iter/s, 134.024s/200 iters), loss = 0.00329117
I1129 08:08:52.294708 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00329123 (* 1 = 0.00329123 loss)
I1129 08:08:52.294726 30336 sgd_solver.cpp:105] Iteration 61200, lr = 0.001
I1129 08:10:15.393601 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:11:06.314862 30336 solver.cpp:218] Iteration 61400 (1.49237 iter/s, 134.015s/200 iters), loss = 0.000959621
I1129 08:11:06.314944 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00095968 (* 1 = 0.00095968 loss)
I1129 08:11:06.314949 30336 sgd_solver.cpp:105] Iteration 61400, lr = 0.001
I1129 08:12:12.666275 30336 solver.cpp:330] Iteration 61500, Testing net (#0)
I1129 08:12:30.482677 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:12:48.459089 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:12:48.604460 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921203
I1129 08:12:48.604480 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356195 (* 1 = 0.356195 loss)
I1129 08:13:56.255858 30336 solver.cpp:218] Iteration 61600 (1.17692 iter/s, 169.935s/200 iters), loss = 0.000723662
I1129 08:13:56.255975 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000723721 (* 1 = 0.000723721 loss)
I1129 08:13:56.255997 30336 sgd_solver.cpp:105] Iteration 61600, lr = 0.001
I1129 08:15:12.630087 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:16:10.242200 30336 solver.cpp:218] Iteration 61800 (1.49275 iter/s, 133.981s/200 iters), loss = 0.00152785
I1129 08:16:10.242331 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00152791 (* 1 = 0.00152791 loss)
I1129 08:16:10.242337 30336 sgd_solver.cpp:105] Iteration 61800, lr = 0.001
I1129 08:18:23.561300 30336 solver.cpp:330] Iteration 62000, Testing net (#0)
I1129 08:18:41.381254 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:18:59.343555 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:18:59.488829 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921003
I1129 08:18:59.488850 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355993 (* 1 = 0.355993 loss)
I1129 08:19:00.157927 30336 solver.cpp:218] Iteration 62000 (1.1771 iter/s, 169.909s/200 iters), loss = 0.000741263
I1129 08:19:00.157960 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000741321 (* 1 = 0.000741321 loss)
I1129 08:19:00.157966 30336 sgd_solver.cpp:105] Iteration 62000, lr = 0.001
I1129 08:20:10.512601 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:21:14.194720 30336 solver.cpp:218] Iteration 62200 (1.49218 iter/s, 134.032s/200 iters), loss = 0.00103367
I1129 08:21:14.194855 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00103373 (* 1 = 0.00103373 loss)
I1129 08:21:14.194877 30336 sgd_solver.cpp:105] Iteration 62200, lr = 0.001
I1129 08:23:28.288283 30336 solver.cpp:218] Iteration 62400 (1.49155 iter/s, 134.089s/200 iters), loss = 0.000375239
I1129 08:23:28.288426 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000375299 (* 1 = 0.000375299 loss)
I1129 08:23:28.288434 30336 sgd_solver.cpp:105] Iteration 62400, lr = 0.001
I1129 08:24:31.991334 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:24:34.670585 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_62500.caffemodel
I1129 08:24:34.811434 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_62500.solverstate
I1129 08:24:34.879508 30336 solver.cpp:330] Iteration 62500, Testing net (#0)
I1129 08:24:52.697099 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:25:10.657212 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:25:10.801585 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.920803
I1129 08:25:10.801606 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355682 (* 1 = 0.355682 loss)
I1129 08:26:18.452433 30336 solver.cpp:218] Iteration 62600 (1.17538 iter/s, 170.158s/200 iters), loss = 0.00114866
I1129 08:26:18.452556 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00114872 (* 1 = 0.00114872 loss)
I1129 08:26:18.452564 30336 sgd_solver.cpp:105] Iteration 62600, lr = 0.001
I1129 08:28:32.402096 30336 solver.cpp:218] Iteration 62800 (1.49315 iter/s, 133.945s/200 iters), loss = 0.000556202
I1129 08:28:32.402235 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000556261 (* 1 = 0.000556261 loss)
I1129 08:28:32.402243 30336 sgd_solver.cpp:105] Iteration 62800, lr = 0.001
I1129 08:29:30.016199 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:30:45.692122 30336 solver.cpp:330] Iteration 63000, Testing net (#0)
I1129 08:31:03.536267 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:31:21.516034 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:31:21.661867 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921303
I1129 08:31:21.661888 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356129 (* 1 = 0.356129 loss)
I1129 08:31:22.328382 30336 solver.cpp:218] Iteration 63000 (1.17702 iter/s, 169.92s/200 iters), loss = 0.00107386
I1129 08:31:22.328423 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00107392 (* 1 = 0.00107392 loss)
I1129 08:31:22.328429 30336 sgd_solver.cpp:105] Iteration 63000, lr = 0.001
I1129 08:33:36.321226 30336 solver.cpp:218] Iteration 63200 (1.49267 iter/s, 133.988s/200 iters), loss = 0.000681313
I1129 08:33:36.321398 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000681371 (* 1 = 0.000681371 loss)
I1129 08:33:36.321405 30336 sgd_solver.cpp:105] Iteration 63200, lr = 0.001
I1129 08:34:27.931231 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:35:50.326316 30336 solver.cpp:218] Iteration 63400 (1.49254 iter/s, 134s/200 iters), loss = 0.000449315
I1129 08:35:50.326443 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000449372 (* 1 = 0.000449372 loss)
I1129 08:35:50.326465 30336 sgd_solver.cpp:105] Iteration 63400, lr = 0.001
I1129 08:36:56.660799 30336 solver.cpp:330] Iteration 63500, Testing net (#0)
I1129 08:37:14.490008 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:37:32.468616 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:37:32.613020 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921403
I1129 08:37:32.613055 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356068 (* 1 = 0.356068 loss)
I1129 08:38:40.275985 30336 solver.cpp:218] Iteration 63600 (1.17686 iter/s, 169.943s/200 iters), loss = 0.000878293
I1129 08:38:40.276127 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000878351 (* 1 = 0.000878351 loss)
I1129 08:38:40.276135 30336 sgd_solver.cpp:105] Iteration 63600, lr = 0.001
I1129 08:39:25.183104 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:40:54.315294 30336 solver.cpp:218] Iteration 63800 (1.49215 iter/s, 134.034s/200 iters), loss = 0.000797533
I1129 08:40:54.315378 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000797591 (* 1 = 0.000797591 loss)
I1129 08:40:54.315384 30336 sgd_solver.cpp:105] Iteration 63800, lr = 0.001
I1129 08:43:07.689087 30336 solver.cpp:330] Iteration 64000, Testing net (#0)
I1129 08:43:25.527896 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:43:43.504806 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:43:43.650151 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921303
I1129 08:43:43.650172 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355875 (* 1 = 0.355875 loss)
I1129 08:43:44.316923 30336 solver.cpp:218] Iteration 64000 (1.1765 iter/s, 169.995s/200 iters), loss = 0.00760405
I1129 08:43:44.316951 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0076041 (* 1 = 0.0076041 loss)
I1129 08:43:44.316956 30336 sgd_solver.cpp:105] Iteration 64000, lr = 0.001
I1129 08:44:23.178709 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:45:58.289460 30336 solver.cpp:218] Iteration 64200 (1.4929 iter/s, 133.968s/200 iters), loss = 0.00084212
I1129 08:45:58.290071 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000842178 (* 1 = 0.000842178 loss)
I1129 08:45:58.290079 30336 sgd_solver.cpp:105] Iteration 64200, lr = 0.001
I1129 08:48:12.235518 30336 solver.cpp:218] Iteration 64400 (1.4932 iter/s, 133.941s/200 iters), loss = 0.000852432
I1129 08:48:12.235635 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00085249 (* 1 = 0.00085249 loss)
I1129 08:48:12.235643 30336 sgd_solver.cpp:105] Iteration 64400, lr = 0.001
I1129 08:48:45.062557 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:49:18.540828 30336 solver.cpp:330] Iteration 64500, Testing net (#0)
I1129 08:49:36.359413 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:49:54.322732 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:49:54.468194 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921103
I1129 08:49:54.468215 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355754 (* 1 = 0.355754 loss)
I1129 08:51:02.085577 30336 solver.cpp:218] Iteration 64600 (1.17756 iter/s, 169.843s/200 iters), loss = 0.000983996
I1129 08:51:02.085688 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000984054 (* 1 = 0.000984054 loss)
I1129 08:51:02.085695 30336 sgd_solver.cpp:105] Iteration 64600, lr = 0.001
I1129 08:53:16.100322 30336 solver.cpp:218] Iteration 64800 (1.49243 iter/s, 134.009s/200 iters), loss = 0.00813311
I1129 08:53:16.100463 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00813317 (* 1 = 0.00813317 loss)
I1129 08:53:16.100471 30336 sgd_solver.cpp:105] Iteration 64800, lr = 0.001
I1129 08:53:42.248807 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:55:29.468556 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_65000.caffemodel
I1129 08:55:29.609792 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_65000.solverstate
I1129 08:55:29.678194 30336 solver.cpp:330] Iteration 65000, Testing net (#0)
I1129 08:55:47.494195 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:56:05.469519 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 08:56:05.615372 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921403
I1129 08:56:05.615394 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355968 (* 1 = 0.355968 loss)
I1129 08:56:06.283758 30336 solver.cpp:218] Iteration 65000 (1.17525 iter/s, 170.177s/200 iters), loss = 0.000865022
I1129 08:56:06.283784 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000865081 (* 1 = 0.000865081 loss)
I1129 08:56:06.283789 30336 sgd_solver.cpp:105] Iteration 65000, lr = 0.001
I1129 08:58:20.335155 30336 solver.cpp:218] Iteration 65200 (1.49202 iter/s, 134.046s/200 iters), loss = 0.0014849
I1129 08:58:20.335292 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00148496 (* 1 = 0.00148496 loss)
I1129 08:58:20.335299 30336 sgd_solver.cpp:105] Iteration 65200, lr = 0.001
I1129 08:58:40.445543 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:00:34.323196 30336 solver.cpp:218] Iteration 65400 (1.49273 iter/s, 133.983s/200 iters), loss = 0.00243688
I1129 09:00:34.323320 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00243694 (* 1 = 0.00243694 loss)
I1129 09:00:34.323343 30336 sgd_solver.cpp:105] Iteration 65400, lr = 0.001
I1129 09:01:40.664964 30336 solver.cpp:330] Iteration 65500, Testing net (#0)
I1129 09:01:58.509006 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:02:16.459586 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:02:16.604945 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921103
I1129 09:02:16.604966 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355965 (* 1 = 0.355965 loss)
I1129 09:03:24.236923 30336 solver.cpp:218] Iteration 65600 (1.17711 iter/s, 169.907s/200 iters), loss = 0.0013507
I1129 09:03:24.237051 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00135076 (* 1 = 0.00135076 loss)
I1129 09:03:24.237073 30336 sgd_solver.cpp:105] Iteration 65600, lr = 0.001
I1129 09:03:37.647413 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:05:38.189532 30336 solver.cpp:218] Iteration 65800 (1.49312 iter/s, 133.947s/200 iters), loss = 0.00128273
I1129 09:05:38.189687 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00128279 (* 1 = 0.00128279 loss)
I1129 09:05:38.189694 30336 sgd_solver.cpp:105] Iteration 65800, lr = 0.001
I1129 09:07:51.479873 30336 solver.cpp:330] Iteration 66000, Testing net (#0)
I1129 09:08:09.318521 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:08:27.299943 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:08:27.447070 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921003
I1129 09:08:27.447093 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356018 (* 1 = 0.356018 loss)
I1129 09:08:28.114053 30336 solver.cpp:218] Iteration 66000 (1.17704 iter/s, 169.918s/200 iters), loss = 0.000739012
I1129 09:08:28.114079 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000739072 (* 1 = 0.000739072 loss)
I1129 09:08:28.114084 30336 sgd_solver.cpp:105] Iteration 66000, lr = 0.001
I1129 09:08:35.497059 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:10:42.155030 30336 solver.cpp:218] Iteration 66200 (1.49214 iter/s, 134.036s/200 iters), loss = 0.00101209
I1129 09:10:42.155148 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00101215 (* 1 = 0.00101215 loss)
I1129 09:10:42.155155 30336 sgd_solver.cpp:105] Iteration 66200, lr = 0.001
I1129 09:12:56.229490 30336 solver.cpp:218] Iteration 66400 (1.49176 iter/s, 134.069s/200 iters), loss = 0.0025314
I1129 09:12:56.229630 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00253146 (* 1 = 0.00253146 loss)
I1129 09:12:56.229637 30336 sgd_solver.cpp:105] Iteration 66400, lr = 0.001
I1129 09:12:57.585368 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:14:02.605489 30336 solver.cpp:330] Iteration 66500, Testing net (#0)
I1129 09:14:20.448429 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:14:38.428745 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:14:38.574476 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921103
I1129 09:14:38.574497 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356217 (* 1 = 0.356217 loss)
I1129 09:15:46.240219 30336 solver.cpp:218] Iteration 66600 (1.17644 iter/s, 170.004s/200 iters), loss = 0.0010572
I1129 09:15:46.240355 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00105727 (* 1 = 0.00105727 loss)
I1129 09:15:46.240362 30336 sgd_solver.cpp:105] Iteration 66600, lr = 0.001
I1129 09:17:54.881047 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:18:00.234797 30336 solver.cpp:218] Iteration 66800 (1.49265 iter/s, 133.99s/200 iters), loss = 0.00039781
I1129 09:18:00.234827 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000397871 (* 1 = 0.000397871 loss)
I1129 09:18:00.234848 30336 sgd_solver.cpp:105] Iteration 66800, lr = 0.001
I1129 09:20:13.671277 30336 solver.cpp:330] Iteration 67000, Testing net (#0)
I1129 09:20:31.521291 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:20:49.515465 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:20:49.660791 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921103
I1129 09:20:49.660812 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356099 (* 1 = 0.356099 loss)
I1129 09:20:50.328964 30336 solver.cpp:218] Iteration 67000 (1.17586 iter/s, 170.088s/200 iters), loss = 0.000460434
I1129 09:20:50.328992 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000460495 (* 1 = 0.000460495 loss)
I1129 09:20:50.328999 30336 sgd_solver.cpp:105] Iteration 67000, lr = 0.001
I1129 09:22:52.975919 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:23:04.375046 30336 solver.cpp:218] Iteration 67200 (1.49208 iter/s, 134.041s/200 iters), loss = 0.000796283
I1129 09:23:04.375073 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000796344 (* 1 = 0.000796344 loss)
I1129 09:23:04.375093 30336 sgd_solver.cpp:105] Iteration 67200, lr = 0.001
I1129 09:25:18.394783 30336 solver.cpp:218] Iteration 67400 (1.49237 iter/s, 134.015s/200 iters), loss = 0.000607251
I1129 09:25:18.394906 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000607313 (* 1 = 0.000607313 loss)
I1129 09:25:18.394927 30336 sgd_solver.cpp:105] Iteration 67400, lr = 0.001
I1129 09:26:24.775431 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_67500.caffemodel
I1129 09:26:24.914955 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_67500.solverstate
I1129 09:26:24.983070 30336 solver.cpp:330] Iteration 67500, Testing net (#0)
I1129 09:26:42.834208 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:27:00.816318 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:27:00.961066 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921003
I1129 09:27:00.961086 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356144 (* 1 = 0.356144 loss)
I1129 09:27:51.219491 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:28:08.645107 30336 solver.cpp:218] Iteration 67600 (1.17478 iter/s, 170.244s/200 iters), loss = 0.000718883
I1129 09:28:08.645133 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000718945 (* 1 = 0.000718945 loss)
I1129 09:28:08.645153 30336 sgd_solver.cpp:105] Iteration 67600, lr = 0.001
I1129 09:30:22.619174 30336 solver.cpp:218] Iteration 67800 (1.49288 iter/s, 133.969s/200 iters), loss = 0.00154334
I1129 09:30:22.619333 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0015434 (* 1 = 0.0015434 loss)
I1129 09:30:22.619340 30336 sgd_solver.cpp:105] Iteration 67800, lr = 0.001
I1129 09:32:12.500452 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:32:35.953546 30336 solver.cpp:330] Iteration 68000, Testing net (#0)
I1129 09:32:53.791891 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:33:11.781594 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:33:11.927507 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921503
I1129 09:33:11.927528 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355854 (* 1 = 0.355854 loss)
I1129 09:33:12.593968 30336 solver.cpp:218] Iteration 68000 (1.17669 iter/s, 169.969s/200 iters), loss = 0.000907245
I1129 09:33:12.593998 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000907306 (* 1 = 0.000907306 loss)
I1129 09:33:12.594019 30336 sgd_solver.cpp:105] Iteration 68000, lr = 0.001
I1129 09:35:26.569149 30336 solver.cpp:218] Iteration 68200 (1.49287 iter/s, 133.97s/200 iters), loss = 0.00147662
I1129 09:35:26.569265 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00147669 (* 1 = 0.00147669 loss)
I1129 09:35:26.569284 30336 sgd_solver.cpp:105] Iteration 68200, lr = 0.001
I1129 09:37:10.403591 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:37:40.543818 30336 solver.cpp:218] Iteration 68400 (1.49287 iter/s, 133.97s/200 iters), loss = 0.00124937
I1129 09:37:40.543876 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00124943 (* 1 = 0.00124943 loss)
I1129 09:37:40.543882 30336 sgd_solver.cpp:105] Iteration 68400, lr = 0.001
I1129 09:38:46.874014 30336 solver.cpp:330] Iteration 68500, Testing net (#0)
I1129 09:39:04.707177 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:39:22.684823 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:39:22.829946 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921103
I1129 09:39:22.829967 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.35618 (* 1 = 0.35618 loss)
I1129 09:40:30.505520 30336 solver.cpp:218] Iteration 68600 (1.17678 iter/s, 169.956s/200 iters), loss = 0.000284436
I1129 09:40:30.505666 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000284496 (* 1 = 0.000284496 loss)
I1129 09:40:30.505673 30336 sgd_solver.cpp:105] Iteration 68600, lr = 0.001
I1129 09:42:07.683935 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:42:44.545120 30336 solver.cpp:218] Iteration 68800 (1.49215 iter/s, 134.035s/200 iters), loss = 0.00049498
I1129 09:42:44.545258 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000495041 (* 1 = 0.000495041 loss)
I1129 09:42:44.545279 30336 sgd_solver.cpp:105] Iteration 68800, lr = 0.001
I1129 09:44:57.931728 30336 solver.cpp:330] Iteration 69000, Testing net (#0)
I1129 09:45:15.788115 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:45:33.786834 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:45:33.932178 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921203
I1129 09:45:33.932199 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356446 (* 1 = 0.356446 loss)
I1129 09:45:34.600208 30336 solver.cpp:218] Iteration 69000 (1.17613 iter/s, 170.049s/200 iters), loss = 0.000436773
I1129 09:45:34.600239 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000436832 (* 1 = 0.000436832 loss)
I1129 09:45:34.600244 30336 sgd_solver.cpp:105] Iteration 69000, lr = 0.001
I1129 09:47:05.766793 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:47:48.663874 30336 solver.cpp:218] Iteration 69200 (1.49188 iter/s, 134.059s/200 iters), loss = 0.000934146
I1129 09:47:48.664002 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000934204 (* 1 = 0.000934204 loss)
I1129 09:47:48.664026 30336 sgd_solver.cpp:105] Iteration 69200, lr = 0.001
I1129 09:50:02.707463 30336 solver.cpp:218] Iteration 69400 (1.49211 iter/s, 134.039s/200 iters), loss = 0.000457348
I1129 09:50:02.707598 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000457407 (* 1 = 0.000457407 loss)
I1129 09:50:02.707628 30336 sgd_solver.cpp:105] Iteration 69400, lr = 0.001
I1129 09:51:09.080585 30336 solver.cpp:330] Iteration 69500, Testing net (#0)
I1129 09:51:26.924238 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:51:44.901556 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:51:45.046752 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921303
I1129 09:51:45.046787 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356092 (* 1 = 0.356092 loss)
I1129 09:52:03.822131 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:52:52.716212 30336 solver.cpp:218] Iteration 69600 (1.17645 iter/s, 170.002s/200 iters), loss = 0.000714834
I1129 09:52:52.716356 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000714894 (* 1 = 0.000714894 loss)
I1129 09:52:52.716362 30336 sgd_solver.cpp:105] Iteration 69600, lr = 0.001
I1129 09:55:06.727850 30336 solver.cpp:218] Iteration 69800 (1.49246 iter/s, 134.007s/200 iters), loss = 0.000460909
I1129 09:55:06.727998 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000460969 (* 1 = 0.000460969 loss)
I1129 09:55:06.728006 30336 sgd_solver.cpp:105] Iteration 69800, lr = 0.001
I1129 09:56:25.136610 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:57:20.091681 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_70000.caffemodel
I1129 09:57:20.229744 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_70000.solverstate
I1129 09:57:20.297343 30336 solver.cpp:330] Iteration 70000, Testing net (#0)
I1129 09:57:38.171195 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:57:56.150990 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 09:57:56.295570 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921403
I1129 09:57:56.295591 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.356394 (* 1 = 0.356394 loss)
I1129 09:57:56.965728 30336 solver.cpp:218] Iteration 70000 (1.17487 iter/s, 170.232s/200 iters), loss = 0.00193171
I1129 09:57:56.965778 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00193177 (* 1 = 0.00193177 loss)
I1129 09:57:56.965785 30336 sgd_solver.cpp:105] Iteration 70000, lr = 0.001
I1129 10:00:12.148645 30336 solver.cpp:218] Iteration 70200 (1.47953 iter/s, 135.178s/200 iters), loss = 0.000551809
I1129 10:00:12.148737 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000551869 (* 1 = 0.000551869 loss)
I1129 10:00:12.148746 30336 sgd_solver.cpp:105] Iteration 70200, lr = 0.001
I1129 10:01:24.833629 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:02:26.496089 30336 solver.cpp:218] Iteration 70400 (1.48872 iter/s, 134.344s/200 iters), loss = 0.000517666
I1129 10:02:26.496232 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000517725 (* 1 = 0.000517725 loss)
I1129 10:02:26.496254 30336 sgd_solver.cpp:105] Iteration 70400, lr = 0.001
I1129 10:03:32.860028 30336 solver.cpp:330] Iteration 70500, Testing net (#0)
I1129 10:03:50.693301 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:04:08.657176 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:04:08.802415 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921503
I1129 10:04:08.802435 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355733 (* 1 = 0.355733 loss)
I1129 10:05:16.462236 30336 solver.cpp:218] Iteration 70600 (1.17674 iter/s, 169.961s/200 iters), loss = 0.00101373
I1129 10:05:16.462378 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00101379 (* 1 = 0.00101379 loss)
I1129 10:05:16.462386 30336 sgd_solver.cpp:105] Iteration 70600, lr = 0.001
I1129 10:06:22.788030 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:07:30.440240 30336 solver.cpp:218] Iteration 70800 (1.49283 iter/s, 133.974s/200 iters), loss = 0.00170194
I1129 10:07:30.440392 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.001702 (* 1 = 0.001702 loss)
I1129 10:07:30.440399 30336 sgd_solver.cpp:105] Iteration 70800, lr = 0.001
I1129 10:09:44.143343 30336 solver.cpp:330] Iteration 71000, Testing net (#0)
I1129 10:10:01.978626 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:10:19.963094 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:10:20.108273 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921403
I1129 10:10:20.108292 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355535 (* 1 = 0.355535 loss)
I1129 10:10:20.778133 30336 solver.cpp:218] Iteration 71000 (1.17417 iter/s, 170.332s/200 iters), loss = 0.00121605
I1129 10:10:20.778163 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00121611 (* 1 = 0.00121611 loss)
I1129 10:10:20.778169 30336 sgd_solver.cpp:105] Iteration 71000, lr = 0.001
I1129 10:11:20.424871 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:12:34.787914 30336 solver.cpp:218] Iteration 71200 (1.49248 iter/s, 134.005s/200 iters), loss = 0.00117448
I1129 10:12:34.788040 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00117454 (* 1 = 0.00117454 loss)
I1129 10:12:34.788063 30336 sgd_solver.cpp:105] Iteration 71200, lr = 0.001
I1129 10:14:48.800688 30336 solver.cpp:218] Iteration 71400 (1.49244 iter/s, 134.008s/200 iters), loss = 0.00146321
I1129 10:14:48.800839 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00146327 (* 1 = 0.00146327 loss)
I1129 10:14:48.800848 30336 sgd_solver.cpp:105] Iteration 71400, lr = 0.001
I1129 10:15:42.424756 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:15:55.163548 30336 solver.cpp:330] Iteration 71500, Testing net (#0)
I1129 10:16:13.015734 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:16:31.011203 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:16:31.156796 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921403
I1129 10:16:31.156816 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355486 (* 1 = 0.355486 loss)
I1129 10:17:38.825242 30336 solver.cpp:218] Iteration 71600 (1.17634 iter/s, 170.019s/200 iters), loss = 0.000816254
I1129 10:17:38.825379 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000816315 (* 1 = 0.000816315 loss)
I1129 10:17:38.825387 30336 sgd_solver.cpp:105] Iteration 71600, lr = 0.001
I1129 10:19:54.380725 30336 solver.cpp:218] Iteration 71800 (1.47546 iter/s, 135.551s/200 iters), loss = 0.0010228
I1129 10:19:54.380798 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00102286 (* 1 = 0.00102286 loss)
I1129 10:19:54.380820 30336 sgd_solver.cpp:105] Iteration 71800, lr = 0.001
I1129 10:20:41.320919 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:22:07.802713 30336 solver.cpp:330] Iteration 72000, Testing net (#0)
I1129 10:22:25.655432 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:22:43.635035 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:22:43.779404 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921203
I1129 10:22:43.779425 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355409 (* 1 = 0.355409 loss)
I1129 10:22:44.448745 30336 solver.cpp:218] Iteration 72000 (1.17604 iter/s, 170.062s/200 iters), loss = 0.000930292
I1129 10:22:44.448777 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000930353 (* 1 = 0.000930353 loss)
I1129 10:22:44.448783 30336 sgd_solver.cpp:46] MultiStep Status: Iteration 72000, step = 3
I1129 10:22:44.448786 30336 sgd_solver.cpp:105] Iteration 72000, lr = 0.0001
I1129 10:24:58.447584 30336 solver.cpp:218] Iteration 72200 (1.4926 iter/s, 133.994s/200 iters), loss = 0.00134074
I1129 10:24:58.447655 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0013408 (* 1 = 0.0013408 loss)
I1129 10:24:58.447665 30336 sgd_solver.cpp:105] Iteration 72200, lr = 0.0001
I1129 10:25:39.345805 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:27:12.502079 30336 solver.cpp:218] Iteration 72400 (1.49198 iter/s, 134.05s/200 iters), loss = 0.000276301
I1129 10:27:12.502128 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000276362 (* 1 = 0.000276362 loss)
I1129 10:27:12.502136 30336 sgd_solver.cpp:105] Iteration 72400, lr = 0.0001
I1129 10:28:18.860421 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_72500.caffemodel
I1129 10:28:19.008941 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_72500.solverstate
I1129 10:28:19.079120 30336 solver.cpp:330] Iteration 72500, Testing net (#0)
I1129 10:28:36.920469 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:28:54.895328 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:28:55.040066 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921603
I1129 10:28:55.040087 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355395 (* 1 = 0.355395 loss)
I1129 10:30:02.729555 30336 solver.cpp:218] Iteration 72600 (1.17494 iter/s, 170.222s/200 iters), loss = 0.000509794
I1129 10:30:02.729605 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000509855 (* 1 = 0.000509855 loss)
I1129 10:30:02.729614 30336 sgd_solver.cpp:105] Iteration 72600, lr = 0.0001
I1129 10:30:37.596277 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:32:16.804553 30336 solver.cpp:218] Iteration 72800 (1.49175 iter/s, 134.07s/200 iters), loss = 0.000979774
I1129 10:32:16.804605 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000979835 (* 1 = 0.000979835 loss)
I1129 10:32:16.804611 30336 sgd_solver.cpp:105] Iteration 72800, lr = 0.0001
I1129 10:34:30.556457 30336 solver.cpp:330] Iteration 73000, Testing net (#0)
I1129 10:34:48.418186 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:35:06.491724 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:35:06.638984 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921703
I1129 10:35:06.639012 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355346 (* 1 = 0.355346 loss)
I1129 10:35:07.308236 30336 solver.cpp:218] Iteration 73000 (1.17304 iter/s, 170.498s/200 iters), loss = 0.00101053
I1129 10:35:07.308288 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00101059 (* 1 = 0.00101059 loss)
I1129 10:35:07.308298 30336 sgd_solver.cpp:105] Iteration 73000, lr = 0.0001
I1129 10:35:35.573237 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:37:21.761715 30336 solver.cpp:218] Iteration 73200 (1.48755 iter/s, 134.449s/200 iters), loss = 0.000424137
I1129 10:37:21.761792 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000424196 (* 1 = 0.000424196 loss)
I1129 10:37:21.761798 30336 sgd_solver.cpp:105] Iteration 73200, lr = 0.0001
I1129 10:39:36.163455 30336 solver.cpp:218] Iteration 73400 (1.48813 iter/s, 134.397s/200 iters), loss = 0.00102339
I1129 10:39:36.163702 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00102345 (* 1 = 0.00102345 loss)
I1129 10:39:36.163713 30336 sgd_solver.cpp:105] Iteration 73400, lr = 0.0001
I1129 10:39:58.330153 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:40:42.649730 30336 solver.cpp:330] Iteration 73500, Testing net (#0)
I1129 10:41:00.508613 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:41:18.519734 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:41:18.666262 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921703
I1129 10:41:18.666301 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.35532 (* 1 = 0.35532 loss)
I1129 10:42:26.438119 30336 solver.cpp:218] Iteration 73600 (1.17461 iter/s, 170.269s/200 iters), loss = 0.000215768
I1129 10:42:26.438269 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000215829 (* 1 = 0.000215829 loss)
I1129 10:42:26.438277 30336 sgd_solver.cpp:105] Iteration 73600, lr = 0.0001
I1129 10:44:40.668640 30336 solver.cpp:218] Iteration 73800 (1.49003 iter/s, 134.226s/200 iters), loss = 0.00152095
I1129 10:44:40.668767 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00152101 (* 1 = 0.00152101 loss)
I1129 10:44:40.668774 30336 sgd_solver.cpp:105] Iteration 73800, lr = 0.0001
I1129 10:44:56.789088 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:46:54.295963 30336 solver.cpp:330] Iteration 74000, Testing net (#0)
I1129 10:47:12.181064 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:47:30.195596 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:47:30.341289 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921703
I1129 10:47:30.341315 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355307 (* 1 = 0.355307 loss)
I1129 10:47:31.010052 30336 solver.cpp:218] Iteration 74000 (1.17415 iter/s, 170.335s/200 iters), loss = 0.000846703
I1129 10:47:31.010084 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000846763 (* 1 = 0.000846763 loss)
I1129 10:47:31.010092 30336 sgd_solver.cpp:105] Iteration 74000, lr = 0.0001
I1129 10:49:45.284852 30336 solver.cpp:218] Iteration 74200 (1.48953 iter/s, 134.27s/200 iters), loss = 0.000310907
I1129 10:49:45.284950 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000310968 (* 1 = 0.000310968 loss)
I1129 10:49:45.284981 30336 sgd_solver.cpp:105] Iteration 74200, lr = 0.0001
I1129 10:49:54.699468 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:51:59.612784 30336 solver.cpp:218] Iteration 74400 (1.48895 iter/s, 134.323s/200 iters), loss = 0.000413451
I1129 10:51:59.612903 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000413512 (* 1 = 0.000413512 loss)
I1129 10:51:59.612924 30336 sgd_solver.cpp:105] Iteration 74400, lr = 0.0001
I1129 10:53:06.102990 30336 solver.cpp:330] Iteration 74500, Testing net (#0)
I1129 10:53:23.970855 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:53:41.989802 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:53:42.136303 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921803
I1129 10:53:42.136358 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355384 (* 1 = 0.355384 loss)
I1129 10:54:49.923610 30336 solver.cpp:218] Iteration 74600 (1.17436 iter/s, 170.305s/200 iters), loss = 0.00131736
I1129 10:54:49.923807 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00131742 (* 1 = 0.00131742 loss)
I1129 10:54:49.923816 30336 sgd_solver.cpp:105] Iteration 74600, lr = 0.0001
I1129 10:54:53.295368 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:57:04.210646 30336 solver.cpp:218] Iteration 74800 (1.4894 iter/s, 134.282s/200 iters), loss = 0.000746962
I1129 10:57:04.210836 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000747023 (* 1 = 0.000747023 loss)
I1129 10:57:04.210850 30336 sgd_solver.cpp:105] Iteration 74800, lr = 0.0001
I1129 10:59:15.153935 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:59:17.837044 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_75000.caffemodel
I1129 10:59:17.985955 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_75000.solverstate
I1129 10:59:18.058385 30336 solver.cpp:330] Iteration 75000, Testing net (#0)
I1129 10:59:35.920619 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:59:53.930135 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 10:59:54.076032 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921803
I1129 10:59:54.076071 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355359 (* 1 = 0.355359 loss)
I1129 10:59:54.748041 30336 solver.cpp:218] Iteration 75000 (1.1728 iter/s, 170.531s/200 iters), loss = 0.00163598
I1129 10:59:54.748077 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00163604 (* 1 = 0.00163604 loss)
I1129 10:59:54.748085 30336 sgd_solver.cpp:105] Iteration 75000, lr = 0.0001
I1129 11:02:09.065524 30336 solver.cpp:218] Iteration 75200 (1.48906 iter/s, 134.313s/200 iters), loss = 0.000941133
I1129 11:02:09.065659 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000941194 (* 1 = 0.000941194 loss)
I1129 11:02:09.065666 30336 sgd_solver.cpp:105] Iteration 75200, lr = 0.0001
I1129 11:04:14.008853 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:04:23.405853 30336 solver.cpp:218] Iteration 75400 (1.48881 iter/s, 134.336s/200 iters), loss = 0.000756337
I1129 11:04:23.405931 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000756396 (* 1 = 0.000756396 loss)
I1129 11:04:23.405944 30336 sgd_solver.cpp:105] Iteration 75400, lr = 0.0001
I1129 11:05:29.920644 30336 solver.cpp:330] Iteration 75500, Testing net (#0)
I1129 11:05:47.779965 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:06:05.779132 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:06:05.924823 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921803
I1129 11:06:05.924872 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355311 (* 1 = 0.355311 loss)
I1129 11:07:13.720397 30336 solver.cpp:218] Iteration 75600 (1.17434 iter/s, 170.309s/200 iters), loss = 0.000418893
I1129 11:07:13.720568 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000418954 (* 1 = 0.000418954 loss)
I1129 11:07:13.720577 30336 sgd_solver.cpp:105] Iteration 75600, lr = 0.0001
I1129 11:09:12.560510 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:09:27.997917 30336 solver.cpp:218] Iteration 75800 (1.48952 iter/s, 134.272s/200 iters), loss = 0.000939585
I1129 11:09:27.997963 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000939646 (* 1 = 0.000939646 loss)
I1129 11:09:27.997969 30336 sgd_solver.cpp:105] Iteration 75800, lr = 0.0001
I1129 11:11:44.162997 30336 solver.cpp:330] Iteration 76000, Testing net (#0)
I1129 11:12:02.097657 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:12:20.208899 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:12:20.354214 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921803
I1129 11:12:20.354240 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355296 (* 1 = 0.355296 loss)
I1129 11:12:21.022310 30336 solver.cpp:218] Iteration 76000 (1.15596 iter/s, 173.016s/200 iters), loss = 0.000277881
I1129 11:12:21.022344 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000277942 (* 1 = 0.000277942 loss)
I1129 11:12:21.022354 30336 sgd_solver.cpp:105] Iteration 76000, lr = 0.0001
I1129 11:14:15.962126 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:14:38.440587 30336 solver.cpp:218] Iteration 76200 (1.45548 iter/s, 137.412s/200 iters), loss = 0.00108106
I1129 11:14:38.440627 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00108112 (* 1 = 0.00108112 loss)
I1129 11:14:38.440635 30336 sgd_solver.cpp:105] Iteration 76200, lr = 0.0001
I1129 11:16:55.743772 30336 solver.cpp:218] Iteration 76400 (1.45669 iter/s, 137.297s/200 iters), loss = 0.000837404
I1129 11:16:55.743836 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000837465 (* 1 = 0.000837465 loss)
I1129 11:16:55.743844 30336 sgd_solver.cpp:105] Iteration 76400, lr = 0.0001
I1129 11:18:02.663475 30336 solver.cpp:330] Iteration 76500, Testing net (#0)
I1129 11:18:20.859068 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:18:39.508076 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:18:39.658175 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921803
I1129 11:18:39.658201 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355274 (* 1 = 0.355274 loss)
I1129 11:19:19.870203 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:19:48.312149 30336 solver.cpp:218] Iteration 76600 (1.15901 iter/s, 172.561s/200 iters), loss = 0.00119256
I1129 11:19:48.312180 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00119262 (* 1 = 0.00119262 loss)
I1129 11:19:48.312201 30336 sgd_solver.cpp:105] Iteration 76600, lr = 0.0001
I1129 11:22:02.506742 30336 solver.cpp:218] Iteration 76800 (1.49043 iter/s, 134.189s/200 iters), loss = 0.000784645
I1129 11:22:02.506793 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000784707 (* 1 = 0.000784707 loss)
I1129 11:22:02.506800 30336 sgd_solver.cpp:105] Iteration 76800, lr = 0.0001
I1129 11:23:42.397572 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:24:15.904042 30336 solver.cpp:330] Iteration 77000, Testing net (#0)
I1129 11:24:33.740707 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:24:52.053683 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:24:52.198788 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921803
I1129 11:24:52.198809 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355286 (* 1 = 0.355286 loss)
I1129 11:24:52.866197 30336 solver.cpp:218] Iteration 77000 (1.17404 iter/s, 170.353s/200 iters), loss = 0.0008393
I1129 11:24:52.866233 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000839362 (* 1 = 0.000839362 loss)
I1129 11:24:52.866241 30336 sgd_solver.cpp:105] Iteration 77000, lr = 0.0001
I1129 11:27:08.828856 30336 solver.cpp:218] Iteration 77200 (1.47105 iter/s, 135.957s/200 iters), loss = 0.00214102
I1129 11:27:08.828917 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00214108 (* 1 = 0.00214108 loss)
I1129 11:27:08.828924 30336 sgd_solver.cpp:105] Iteration 77200, lr = 0.0001
I1129 11:28:41.982419 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:29:22.846364 30336 solver.cpp:218] Iteration 77400 (1.4924 iter/s, 134.012s/200 iters), loss = 0.000822313
I1129 11:29:22.846415 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000822375 (* 1 = 0.000822375 loss)
I1129 11:29:22.846422 30336 sgd_solver.cpp:105] Iteration 77400, lr = 0.0001
I1129 11:30:29.191186 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_77500.caffemodel
I1129 11:30:29.351812 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_77500.solverstate
I1129 11:30:29.421900 30336 solver.cpp:330] Iteration 77500, Testing net (#0)
I1129 11:30:47.233515 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:31:05.191994 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:31:05.337249 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921803
I1129 11:31:05.337271 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355278 (* 1 = 0.355278 loss)
I1129 11:32:13.033310 30336 solver.cpp:218] Iteration 77600 (1.17522 iter/s, 170.18s/200 iters), loss = 0.000415366
I1129 11:32:13.033360 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000415428 (* 1 = 0.000415428 loss)
I1129 11:32:13.033367 30336 sgd_solver.cpp:105] Iteration 77600, lr = 0.0001
I1129 11:33:40.154266 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:34:27.041872 30336 solver.cpp:218] Iteration 77800 (1.4925 iter/s, 134.003s/200 iters), loss = 0.00058141
I1129 11:34:27.041920 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000581471 (* 1 = 0.000581471 loss)
I1129 11:34:27.041927 30336 sgd_solver.cpp:105] Iteration 77800, lr = 0.0001
I1129 11:36:40.387382 30336 solver.cpp:330] Iteration 78000, Testing net (#0)
I1129 11:36:58.206439 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:37:16.165794 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:37:16.310508 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921803
I1129 11:37:16.310530 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355312 (* 1 = 0.355312 loss)
I1129 11:37:16.978337 30336 solver.cpp:218] Iteration 78000 (1.17695 iter/s, 169.93s/200 iters), loss = 0.00121421
I1129 11:37:16.978384 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00121427 (* 1 = 0.00121427 loss)
I1129 11:37:16.978391 30336 sgd_solver.cpp:105] Iteration 78000, lr = 0.0001
I1129 11:38:37.437147 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:39:31.028415 30336 solver.cpp:218] Iteration 78200 (1.49204 iter/s, 134.045s/200 iters), loss = 0.000550372
I1129 11:39:31.028465 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000550434 (* 1 = 0.000550434 loss)
I1129 11:39:31.028472 30336 sgd_solver.cpp:105] Iteration 78200, lr = 0.0001
I1129 11:41:45.058490 30336 solver.cpp:218] Iteration 78400 (1.49226 iter/s, 134.025s/200 iters), loss = 0.000650776
I1129 11:41:45.058632 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000650836 (* 1 = 0.000650836 loss)
I1129 11:41:45.058653 30336 sgd_solver.cpp:105] Iteration 78400, lr = 0.0001
I1129 11:42:51.429006 30336 solver.cpp:330] Iteration 78500, Testing net (#0)
I1129 11:43:09.258491 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:43:27.223817 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:43:27.368970 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921703
I1129 11:43:27.368990 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355305 (* 1 = 0.355305 loss)
I1129 11:43:35.412168 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:44:35.041129 30336 solver.cpp:218] Iteration 78600 (1.17663 iter/s, 169.977s/200 iters), loss = 0.00161951
I1129 11:44:35.041182 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00161957 (* 1 = 0.00161957 loss)
I1129 11:44:35.041189 30336 sgd_solver.cpp:105] Iteration 78600, lr = 0.0001
I1129 11:46:49.025637 30336 solver.cpp:218] Iteration 78800 (1.49276 iter/s, 133.98s/200 iters), loss = 0.000604221
I1129 11:46:49.025699 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000604282 (* 1 = 0.000604282 loss)
I1129 11:46:49.025707 30336 sgd_solver.cpp:105] Iteration 78800, lr = 0.0001
I1129 11:47:57.364884 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:49:02.379721 30336 solver.cpp:330] Iteration 79000, Testing net (#0)
I1129 11:49:20.210355 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:49:38.173101 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:49:38.318020 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921903
I1129 11:49:38.318042 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355236 (* 1 = 0.355236 loss)
I1129 11:49:38.985409 30336 solver.cpp:218] Iteration 79000 (1.17679 iter/s, 169.954s/200 iters), loss = 0.000452654
I1129 11:49:38.985441 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000452714 (* 1 = 0.000452714 loss)
I1129 11:49:38.985450 30336 sgd_solver.cpp:105] Iteration 79000, lr = 0.0001
I1129 11:51:53.023332 30336 solver.cpp:218] Iteration 79200 (1.49217 iter/s, 134.033s/200 iters), loss = 0.000401631
I1129 11:51:53.023396 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000401691 (* 1 = 0.000401691 loss)
I1129 11:51:53.023403 30336 sgd_solver.cpp:105] Iteration 79200, lr = 0.0001
I1129 11:52:54.724522 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:54:07.113875 30336 solver.cpp:218] Iteration 79400 (1.49158 iter/s, 134.086s/200 iters), loss = 0.000706152
I1129 11:54:07.113924 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000706213 (* 1 = 0.000706213 loss)
I1129 11:54:07.113932 30336 sgd_solver.cpp:105] Iteration 79400, lr = 0.0001
I1129 11:55:13.469487 30336 solver.cpp:330] Iteration 79500, Testing net (#0)
I1129 11:55:31.288187 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:55:49.249155 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:55:49.393965 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921903
I1129 11:55:49.393986 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355175 (* 1 = 0.355175 loss)
I1129 11:56:57.075775 30336 solver.cpp:218] Iteration 79600 (1.17678 iter/s, 169.956s/200 iters), loss = 0.000816391
I1129 11:56:57.075824 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000816451 (* 1 = 0.000816451 loss)
I1129 11:56:57.075834 30336 sgd_solver.cpp:105] Iteration 79600, lr = 0.0001
I1129 11:57:52.719152 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 11:59:11.098431 30336 solver.cpp:218] Iteration 79800 (1.49234 iter/s, 134.018s/200 iters), loss = 0.00119994
I1129 11:59:11.098493 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0012 (* 1 = 0.0012 loss)
I1129 11:59:11.098503 30336 sgd_solver.cpp:105] Iteration 79800, lr = 0.0001
I1129 12:01:24.458782 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_80000.caffemodel
I1129 12:01:24.610263 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_80000.solverstate
I1129 12:01:24.681366 30336 solver.cpp:330] Iteration 80000, Testing net (#0)
I1129 12:01:42.506844 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:02:00.470152 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:02:00.615392 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921803
I1129 12:02:00.615416 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355226 (* 1 = 0.355226 loss)
I1129 12:02:01.283044 30336 solver.cpp:218] Iteration 80000 (1.17524 iter/s, 170.178s/200 iters), loss = 0.00037977
I1129 12:02:01.283079 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000379833 (* 1 = 0.000379833 loss)
I1129 12:02:01.283088 30336 sgd_solver.cpp:105] Iteration 80000, lr = 0.0001
I1129 12:02:50.882359 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:04:15.344761 30336 solver.cpp:218] Iteration 80200 (1.4919 iter/s, 134.057s/200 iters), loss = 0.000504169
I1129 12:04:15.344826 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000504232 (* 1 = 0.000504232 loss)
I1129 12:04:15.344835 30336 sgd_solver.cpp:105] Iteration 80200, lr = 0.0001
I1129 12:06:29.923151 30336 solver.cpp:218] Iteration 80400 (1.48618 iter/s, 134.573s/200 iters), loss = 0.000689853
I1129 12:06:29.923202 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000689916 (* 1 = 0.000689916 loss)
I1129 12:06:29.923210 30336 sgd_solver.cpp:105] Iteration 80400, lr = 0.0001
I1129 12:07:12.858346 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:07:36.337990 30336 solver.cpp:330] Iteration 80500, Testing net (#0)
I1129 12:07:54.169173 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:08:12.147486 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:08:12.292592 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921803
I1129 12:08:12.292613 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355242 (* 1 = 0.355242 loss)
I1129 12:09:19.978811 30336 solver.cpp:218] Iteration 80600 (1.17613 iter/s, 170.049s/200 iters), loss = 0.000590499
I1129 12:09:19.978865 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000590562 (* 1 = 0.000590562 loss)
I1129 12:09:19.978873 30336 sgd_solver.cpp:105] Iteration 80600, lr = 0.0001
I1129 12:11:34.029309 30336 solver.cpp:218] Iteration 80800 (1.49203 iter/s, 134.046s/200 iters), loss = 0.000212067
I1129 12:11:34.029357 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000212129 (* 1 = 0.000212129 loss)
I1129 12:11:34.029366 30336 sgd_solver.cpp:105] Iteration 80800, lr = 0.0001
I1129 12:12:10.920369 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:13:47.417165 30336 solver.cpp:330] Iteration 81000, Testing net (#0)
I1129 12:14:05.257488 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:14:23.228169 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:14:23.374445 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921903
I1129 12:14:23.374471 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355257 (* 1 = 0.355257 loss)
I1129 12:14:24.042332 30336 solver.cpp:218] Iteration 81000 (1.17642 iter/s, 170.007s/200 iters), loss = 0.000825436
I1129 12:14:24.042366 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000825497 (* 1 = 0.000825497 loss)
I1129 12:14:24.042372 30336 sgd_solver.cpp:105] Iteration 81000, lr = 0.0001
I1129 12:16:38.085403 30336 solver.cpp:218] Iteration 81200 (1.49211 iter/s, 134.038s/200 iters), loss = 0.000797638
I1129 12:16:38.085451 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000797699 (* 1 = 0.000797699 loss)
I1129 12:16:38.085459 30336 sgd_solver.cpp:105] Iteration 81200, lr = 0.0001
I1129 12:17:08.261364 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:18:52.156797 30336 solver.cpp:218] Iteration 81400 (1.49177 iter/s, 134.069s/200 iters), loss = 0.000450462
I1129 12:18:52.156844 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000450523 (* 1 = 0.000450523 loss)
I1129 12:18:52.156852 30336 sgd_solver.cpp:105] Iteration 81400, lr = 0.0001
I1129 12:19:58.521744 30336 solver.cpp:330] Iteration 81500, Testing net (#0)
I1129 12:20:16.353682 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:20:34.334954 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:20:34.481276 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921803
I1129 12:20:34.481303 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355339 (* 1 = 0.355339 loss)
I1129 12:21:42.149456 30336 solver.cpp:218] Iteration 81600 (1.17654 iter/s, 169.99s/200 iters), loss = 0.000660695
I1129 12:21:42.149504 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000660757 (* 1 = 0.000660757 loss)
I1129 12:21:42.149513 30336 sgd_solver.cpp:105] Iteration 81600, lr = 0.0001
I1129 12:22:06.280263 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:23:56.183974 30336 solver.cpp:218] Iteration 81800 (1.49218 iter/s, 134.032s/200 iters), loss = 0.00162413
I1129 12:23:56.184038 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00162419 (* 1 = 0.00162419 loss)
I1129 12:23:56.184047 30336 sgd_solver.cpp:105] Iteration 81800, lr = 0.0001
I1129 12:26:09.544148 30336 solver.cpp:330] Iteration 82000, Testing net (#0)
I1129 12:26:27.377496 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:26:45.352174 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:26:45.497072 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921803
I1129 12:26:45.497093 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355228 (* 1 = 0.355228 loss)
I1129 12:26:46.165489 30336 solver.cpp:218] Iteration 82000 (1.17662 iter/s, 169.978s/200 iters), loss = 0.00185692
I1129 12:26:46.165524 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00185698 (* 1 = 0.00185698 loss)
I1129 12:26:46.165531 30336 sgd_solver.cpp:105] Iteration 82000, lr = 0.0001
I1129 12:27:04.281550 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:29:00.200561 30336 solver.cpp:218] Iteration 82200 (1.49218 iter/s, 134.032s/200 iters), loss = 0.000294132
I1129 12:29:00.200616 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000294194 (* 1 = 0.000294194 loss)
I1129 12:29:00.200639 30336 sgd_solver.cpp:105] Iteration 82200, lr = 0.0001
I1129 12:31:14.230875 30336 solver.cpp:218] Iteration 82400 (1.49224 iter/s, 134.027s/200 iters), loss = 0.000301294
I1129 12:31:14.230922 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000301354 (* 1 = 0.000301354 loss)
I1129 12:31:14.230931 30336 sgd_solver.cpp:105] Iteration 82400, lr = 0.0001
I1129 12:31:25.641191 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:32:20.590787 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_82500.caffemodel
I1129 12:32:20.739944 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_82500.solverstate
I1129 12:32:20.810456 30336 solver.cpp:330] Iteration 82500, Testing net (#0)
I1129 12:32:38.643620 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:32:56.611914 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:32:56.756803 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.922003
I1129 12:32:56.756826 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355208 (* 1 = 0.355208 loss)
I1129 12:34:04.432457 30336 solver.cpp:218] Iteration 82600 (1.17511 iter/s, 170.197s/200 iters), loss = 0.000571028
I1129 12:34:04.432514 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000571089 (* 1 = 0.000571089 loss)
I1129 12:34:04.432536 30336 sgd_solver.cpp:105] Iteration 82600, lr = 0.0001
I1129 12:36:18.465809 30336 solver.cpp:218] Iteration 82800 (1.49221 iter/s, 134.029s/200 iters), loss = 0.00220886
I1129 12:36:18.465869 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00220892 (* 1 = 0.00220892 loss)
I1129 12:36:18.465876 30336 sgd_solver.cpp:105] Iteration 82800, lr = 0.0001
I1129 12:36:23.839526 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:38:31.810367 30336 solver.cpp:330] Iteration 83000, Testing net (#0)
I1129 12:38:49.646641 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:39:07.620780 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:39:07.766168 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.922103
I1129 12:39:07.766189 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355323 (* 1 = 0.355323 loss)
I1129 12:39:08.434836 30336 solver.cpp:218] Iteration 83000 (1.17672 iter/s, 169.964s/200 iters), loss = 0.000445564
I1129 12:39:08.434870 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000445627 (* 1 = 0.000445627 loss)
I1129 12:39:08.434878 30336 sgd_solver.cpp:105] Iteration 83000, lr = 0.0001
I1129 12:41:21.793917 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:41:22.460049 30336 solver.cpp:218] Iteration 83200 (1.4923 iter/s, 134.021s/200 iters), loss = 0.00287325
I1129 12:41:22.460088 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00287332 (* 1 = 0.00287332 loss)
I1129 12:41:22.460095 30336 sgd_solver.cpp:105] Iteration 83200, lr = 0.0001
I1129 12:43:37.817209 30336 solver.cpp:218] Iteration 83400 (1.47762 iter/s, 135.353s/200 iters), loss = 0.000797323
I1129 12:43:37.817261 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000797388 (* 1 = 0.000797388 loss)
I1129 12:43:37.817268 30336 sgd_solver.cpp:105] Iteration 83400, lr = 0.0001
I1129 12:44:45.525279 30336 solver.cpp:330] Iteration 83500, Testing net (#0)
I1129 12:45:03.712028 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:45:21.706609 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:45:21.851835 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921903
I1129 12:45:21.851858 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355358 (* 1 = 0.355358 loss)
I1129 12:46:22.593241 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:46:29.962702 30336 solver.cpp:218] Iteration 83600 (1.16184 iter/s, 172.14s/200 iters), loss = 0.000605683
I1129 12:46:29.962738 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00060575 (* 1 = 0.00060575 loss)
I1129 12:46:29.962745 30336 sgd_solver.cpp:105] Iteration 83600, lr = 0.0001
I1129 12:48:44.538246 30336 solver.cpp:218] Iteration 83800 (1.4862 iter/s, 134.571s/200 iters), loss = 0.0005623
I1129 12:48:44.538314 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000562367 (* 1 = 0.000562367 loss)
I1129 12:48:44.538324 30336 sgd_solver.cpp:105] Iteration 83800, lr = 0.0001
I1129 12:50:45.292035 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:50:58.034296 30336 solver.cpp:330] Iteration 84000, Testing net (#0)
I1129 12:51:15.875047 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:51:33.852123 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:51:33.997884 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921903
I1129 12:51:33.997905 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355194 (* 1 = 0.355194 loss)
I1129 12:51:34.667798 30336 solver.cpp:218] Iteration 84000 (1.17561 iter/s, 170.124s/200 iters), loss = 0.000660334
I1129 12:51:34.667840 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000660402 (* 1 = 0.000660402 loss)
I1129 12:51:34.667851 30336 sgd_solver.cpp:105] Iteration 84000, lr = 0.0001
I1129 12:53:49.291304 30336 solver.cpp:218] Iteration 84200 (1.48567 iter/s, 134.619s/200 iters), loss = 0.000844779
I1129 12:53:49.291353 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000844846 (* 1 = 0.000844846 loss)
I1129 12:53:49.291375 30336 sgd_solver.cpp:105] Iteration 84200, lr = 0.0001
I1129 12:55:43.186928 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:56:03.272994 30336 solver.cpp:218] Iteration 84400 (1.49279 iter/s, 133.977s/200 iters), loss = 0.00067411
I1129 12:56:03.273026 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000674178 (* 1 = 0.000674178 loss)
I1129 12:56:03.273032 30336 sgd_solver.cpp:105] Iteration 84400, lr = 0.0001
I1129 12:57:09.629079 30336 solver.cpp:330] Iteration 84500, Testing net (#0)
I1129 12:57:27.454881 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:57:45.407474 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 12:57:45.552546 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921903
I1129 12:57:45.552568 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355153 (* 1 = 0.355153 loss)
I1129 12:58:53.215502 30336 solver.cpp:218] Iteration 84600 (1.17691 iter/s, 169.937s/200 iters), loss = 0.000894081
I1129 12:58:53.215556 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000894148 (* 1 = 0.000894148 loss)
I1129 12:58:53.215566 30336 sgd_solver.cpp:105] Iteration 84600, lr = 0.0001
I1129 13:00:41.107168 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:01:07.234689 30336 solver.cpp:218] Iteration 84800 (1.49237 iter/s, 134.015s/200 iters), loss = 0.00043552
I1129 13:01:07.234725 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000435587 (* 1 = 0.000435587 loss)
I1129 13:01:07.234733 30336 sgd_solver.cpp:105] Iteration 84800, lr = 0.0001
I1129 13:03:20.599689 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_85000.caffemodel
I1129 13:03:20.747709 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_85000.solverstate
I1129 13:03:20.817958 30336 solver.cpp:330] Iteration 85000, Testing net (#0)
I1129 13:03:38.629866 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:03:56.590384 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:03:56.735090 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921903
I1129 13:03:56.735111 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.3553 (* 1 = 0.3553 loss)
I1129 13:03:57.402462 30336 solver.cpp:218] Iteration 85000 (1.17535 iter/s, 170.162s/200 iters), loss = 0.000598941
I1129 13:03:57.402496 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000599008 (* 1 = 0.000599008 loss)
I1129 13:03:57.402505 30336 sgd_solver.cpp:105] Iteration 85000, lr = 0.0001
I1129 13:05:39.233048 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:06:11.727433 30336 solver.cpp:218] Iteration 85200 (1.48897 iter/s, 134.321s/200 iters), loss = 0.00122826
I1129 13:06:11.727483 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00122832 (* 1 = 0.00122832 loss)
I1129 13:06:11.727494 30336 sgd_solver.cpp:105] Iteration 85200, lr = 0.0001
I1129 13:08:26.398560 30336 solver.cpp:218] Iteration 85400 (1.48515 iter/s, 134.667s/200 iters), loss = 0.000284604
I1129 13:08:26.398617 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000284671 (* 1 = 0.000284671 loss)
I1129 13:08:26.398624 30336 sgd_solver.cpp:105] Iteration 85400, lr = 0.0001
I1129 13:09:33.739578 30336 solver.cpp:330] Iteration 85500, Testing net (#0)
I1129 13:09:51.749467 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:10:09.776115 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:10:09.922561 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921803
I1129 13:10:09.922585 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355252 (* 1 = 0.355252 loss)
I1129 13:10:38.806186 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:11:17.696961 30336 solver.cpp:218] Iteration 85600 (1.16759 iter/s, 171.293s/200 iters), loss = 0.000508174
I1129 13:11:17.697024 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000508242 (* 1 = 0.000508242 loss)
I1129 13:11:17.697033 30336 sgd_solver.cpp:105] Iteration 85600, lr = 0.0001
I1129 13:13:36.789510 30336 solver.cpp:218] Iteration 85800 (1.43794 iter/s, 139.088s/200 iters), loss = 0.00116025
I1129 13:13:36.789580 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00116032 (* 1 = 0.00116032 loss)
I1129 13:13:36.789592 30336 sgd_solver.cpp:105] Iteration 85800, lr = 0.0001
I1129 13:15:06.993294 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:15:51.529631 30336 solver.cpp:330] Iteration 86000, Testing net (#0)
I1129 13:16:09.488106 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:16:27.529474 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:16:27.684643 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921903
I1129 13:16:27.684689 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355202 (* 1 = 0.355202 loss)
I1129 13:16:28.377579 30336 solver.cpp:218] Iteration 86000 (1.16562 iter/s, 171.583s/200 iters), loss = 0.0009224
I1129 13:16:28.377621 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000922468 (* 1 = 0.000922468 loss)
I1129 13:16:28.377629 30336 sgd_solver.cpp:105] Iteration 86000, lr = 0.0001
I1129 13:18:48.236892 30336 solver.cpp:218] Iteration 86200 (1.43005 iter/s, 139.855s/200 iters), loss = 0.0013342
I1129 13:18:48.236953 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00133426 (* 1 = 0.00133426 loss)
I1129 13:18:48.236960 30336 sgd_solver.cpp:105] Iteration 86200, lr = 0.0001
I1129 13:20:12.127104 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:21:03.071993 30336 solver.cpp:218] Iteration 86400 (1.48334 iter/s, 134.831s/200 iters), loss = 0.000667623
I1129 13:21:03.072042 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00066769 (* 1 = 0.00066769 loss)
I1129 13:21:03.072063 30336 sgd_solver.cpp:105] Iteration 86400, lr = 0.0001
I1129 13:22:09.569260 30336 solver.cpp:330] Iteration 86500, Testing net (#0)
I1129 13:22:27.615250 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:22:45.610599 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:22:45.756732 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.922103
I1129 13:22:45.756754 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.35524 (* 1 = 0.35524 loss)
I1129 13:23:53.757778 30336 solver.cpp:218] Iteration 86600 (1.17178 iter/s, 170.68s/200 iters), loss = 0.00211584
I1129 13:23:53.757848 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00211591 (* 1 = 0.00211591 loss)
I1129 13:23:53.757855 30336 sgd_solver.cpp:105] Iteration 86600, lr = 0.0001
I1129 13:25:10.556517 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:26:08.337735 30336 solver.cpp:218] Iteration 86800 (1.48616 iter/s, 134.575s/200 iters), loss = 0.000400787
I1129 13:26:08.337803 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000400855 (* 1 = 0.000400855 loss)
I1129 13:26:08.337813 30336 sgd_solver.cpp:105] Iteration 86800, lr = 0.0001
I1129 13:28:24.516347 30336 solver.cpp:330] Iteration 87000, Testing net (#0)
I1129 13:28:43.365000 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:29:02.312829 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:29:02.467295 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.922003
I1129 13:29:02.467321 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355222 (* 1 = 0.355222 loss)
I1129 13:29:03.181218 30336 solver.cpp:218] Iteration 87000 (1.14402 iter/s, 174.823s/200 iters), loss = 0.000968936
I1129 13:29:03.181268 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000969004 (* 1 = 0.000969004 loss)
I1129 13:29:03.181274 30336 sgd_solver.cpp:105] Iteration 87000, lr = 0.0001
I1129 13:30:14.296792 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:31:18.243600 30336 solver.cpp:218] Iteration 87200 (1.48095 iter/s, 135.049s/200 iters), loss = 0.00105614
I1129 13:31:18.243654 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00105621 (* 1 = 0.00105621 loss)
I1129 13:31:18.243661 30336 sgd_solver.cpp:105] Iteration 87200, lr = 0.0001
I1129 13:33:34.067514 30336 solver.cpp:218] Iteration 87400 (1.47262 iter/s, 135.812s/200 iters), loss = 0.000378593
I1129 13:33:34.067572 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000378662 (* 1 = 0.000378662 loss)
I1129 13:33:34.067581 30336 sgd_solver.cpp:105] Iteration 87400, lr = 0.0001
I1129 13:34:39.252596 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:34:41.932498 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_87500.caffemodel
I1129 13:34:42.080978 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_87500.solverstate
I1129 13:34:42.151314 30336 solver.cpp:330] Iteration 87500, Testing net (#0)
I1129 13:34:59.972918 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:35:17.931258 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:35:18.076195 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921903
I1129 13:35:18.076215 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355135 (* 1 = 0.355135 loss)
I1129 13:36:26.879674 30336 solver.cpp:218] Iteration 87600 (1.15741 iter/s, 172.799s/200 iters), loss = 0.000461434
I1129 13:36:26.879741 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000461504 (* 1 = 0.000461504 loss)
I1129 13:36:26.879750 30336 sgd_solver.cpp:105] Iteration 87600, lr = 0.0001
I1129 13:38:41.454386 30336 solver.cpp:218] Iteration 87800 (1.48627 iter/s, 134.565s/200 iters), loss = 0.000757784
I1129 13:38:41.454453 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000757854 (* 1 = 0.000757854 loss)
I1129 13:38:41.454461 30336 sgd_solver.cpp:105] Iteration 87800, lr = 0.0001
I1129 13:39:39.219396 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:40:55.028028 30336 solver.cpp:330] Iteration 88000, Testing net (#0)
I1129 13:41:12.900425 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:41:30.895789 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:41:31.041532 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921903
I1129 13:41:31.041553 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355127 (* 1 = 0.355127 loss)
I1129 13:41:31.708695 30336 solver.cpp:218] Iteration 88000 (1.17479 iter/s, 170.244s/200 iters), loss = 0.0017408
I1129 13:41:31.708730 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00174087 (* 1 = 0.00174087 loss)
I1129 13:41:31.708739 30336 sgd_solver.cpp:105] Iteration 88000, lr = 0.0001
I1129 13:43:45.898414 30336 solver.cpp:218] Iteration 88200 (1.49051 iter/s, 134.182s/200 iters), loss = 0.000922436
I1129 13:43:45.898475 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000922505 (* 1 = 0.000922505 loss)
I1129 13:43:45.898483 30336 sgd_solver.cpp:105] Iteration 88200, lr = 0.0001
I1129 13:44:37.545156 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:46:00.124205 30336 solver.cpp:218] Iteration 88400 (1.49011 iter/s, 134.218s/200 iters), loss = 0.00171876
I1129 13:46:00.124276 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00171882 (* 1 = 0.00171882 loss)
I1129 13:46:00.124287 30336 sgd_solver.cpp:105] Iteration 88400, lr = 0.0001
I1129 13:47:06.635807 30336 solver.cpp:330] Iteration 88500, Testing net (#0)
I1129 13:47:24.547068 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:47:42.573890 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:47:42.721027 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.922003
I1129 13:47:42.721051 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355098 (* 1 = 0.355098 loss)
I1129 13:48:51.158746 30336 solver.cpp:218] Iteration 88600 (1.16942 iter/s, 171.025s/200 iters), loss = 0.000832731
I1129 13:48:51.158797 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0008328 (* 1 = 0.0008328 loss)
I1129 13:48:51.158804 30336 sgd_solver.cpp:105] Iteration 88600, lr = 0.0001
I1129 13:49:36.418393 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:51:06.310016 30336 solver.cpp:218] Iteration 88800 (1.4799 iter/s, 135.144s/200 iters), loss = 0.000805613
I1129 13:51:06.310066 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000805682 (* 1 = 0.000805682 loss)
I1129 13:51:06.310075 30336 sgd_solver.cpp:105] Iteration 88800, lr = 0.0001
I1129 13:53:20.356685 30336 solver.cpp:330] Iteration 89000, Testing net (#0)
I1129 13:53:38.181378 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:53:56.138598 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:53:56.283181 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.921803
I1129 13:53:56.283203 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355136 (* 1 = 0.355136 loss)
I1129 13:53:56.950251 30336 solver.cpp:218] Iteration 89000 (1.17211 iter/s, 170.632s/200 iters), loss = 0.000849608
I1129 13:53:56.950296 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000849678 (* 1 = 0.000849678 loss)
I1129 13:53:56.950304 30336 sgd_solver.cpp:105] Iteration 89000, lr = 0.0001
I1129 13:54:35.978060 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:56:11.356472 30336 solver.cpp:218] Iteration 89200 (1.4881 iter/s, 134.4s/200 iters), loss = 0.00057107
I1129 13:56:11.356523 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000571139 (* 1 = 0.000571139 loss)
I1129 13:56:11.356530 30336 sgd_solver.cpp:105] Iteration 89200, lr = 0.0001
I1129 13:58:25.424219 30336 solver.cpp:218] Iteration 89400 (1.49185 iter/s, 134.061s/200 iters), loss = 0.000407364
I1129 13:58:25.424279 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000407433 (* 1 = 0.000407433 loss)
I1129 13:58:25.424286 30336 sgd_solver.cpp:105] Iteration 89400, lr = 0.0001
I1129 13:58:58.385308 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 13:59:31.992041 30336 solver.cpp:330] Iteration 89500, Testing net (#0)
I1129 13:59:49.922222 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 14:00:07.918328 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 14:00:08.063581 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.922003
I1129 14:00:08.063603 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355132 (* 1 = 0.355132 loss)
I1129 14:01:16.412910 30336 solver.cpp:218] Iteration 89600 (1.16971 iter/s, 170.982s/200 iters), loss = 0.000704437
I1129 14:01:16.412966 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000704506 (* 1 = 0.000704506 loss)
I1129 14:01:16.412973 30336 sgd_solver.cpp:105] Iteration 89600, lr = 0.0001
I1129 14:03:31.870684 30336 solver.cpp:218] Iteration 89800 (1.47652 iter/s, 135.454s/200 iters), loss = 0.000410784
I1129 14:03:31.870738 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000410852 (* 1 = 0.000410852 loss)
I1129 14:03:31.870746 30336 sgd_solver.cpp:105] Iteration 89800, lr = 0.0001
I1129 14:03:58.008299 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 14:05:45.186883 30336 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_90000.caffemodel
I1129 14:05:45.922348 30336 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_WRN/model_save/caffe_ljftest_train_iter_90000.solverstate
I1129 14:05:45.996902 30336 solver.cpp:330] Iteration 90000, Testing net (#0)
I1129 14:06:03.815692 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 14:06:21.768434 30343 data_layer.cpp:73] Restarting data prefetching from start.
I1129 14:06:21.913488 30336 solver.cpp:397]     Test net output #0: Accuracy1 = 0.922003
I1129 14:06:21.913509 30336 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.355119 (* 1 = 0.355119 loss)
I1129 14:06:22.583571 30336 solver.cpp:218] Iteration 90000 (1.1716 iter/s, 170.707s/200 iters), loss = 0.000529256
I1129 14:06:22.583603 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000529325 (* 1 = 0.000529325 loss)
I1129 14:06:22.583611 30336 sgd_solver.cpp:105] Iteration 90000, lr = 0.0001
I1129 14:08:36.559284 30336 solver.cpp:218] Iteration 90200 (1.49286 iter/s, 133.971s/200 iters), loss = 0.00167886
I1129 14:08:36.559361 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00167893 (* 1 = 0.00167893 loss)
I1129 14:08:36.559370 30336 sgd_solver.cpp:105] Iteration 90200, lr = 0.0001
I1129 14:08:56.664572 30342 data_layer.cpp:73] Restarting data prefetching from start.
I1129 14:10:50.533592 30336 solver.cpp:218] Iteration 90400 (1.49288 iter/s, 133.969s/200 iters), loss = 0.00469423
I1129 14:10:50.533653 30336 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0046943 (* 1 = 0.0046943 loss)
I1129 14:10:50.533661 30336 sgd_solver.cpp:105] Iteration 90400, lr = 0.0001
I1129 14:11:56.883170 30336 solver.cpp:330] Iteration 90500, Testing net (#0)
I1129 14:12:14.705181 30343 data_layer.cpp:73] Restarting data prefetching from start.
