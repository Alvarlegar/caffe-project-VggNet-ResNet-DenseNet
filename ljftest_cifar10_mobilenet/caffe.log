I1128 15:19:18.682121 26047 caffe.cpp:218] Using GPUs 0
I1128 15:19:18.685207 26047 caffe.cpp:223] GPU 0: GeForce GTX 1060 6GB
I1128 15:19:18.883486 26047 solver.cpp:44] Initializing solver from parameters: 
train_net: "/home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/train.prototxt"
test_net: "/home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/test.prototxt"
test_iter: 100
test_interval: 1000
base_lr: 0.1
display: 200
max_iter: 100000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 200000
snapshot: 2500
snapshot_prefix: "/home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train"
solver_mode: GPU
device_id: 0
random_seed: 831486
train_state {
  level: 0
  stage: ""
}
stepvalue: 36000
stepvalue: 50000
stepvalue: 72000
stepvalue: 90000
iter_size: 1
type: "Nesterov"
I1128 15:19:18.883656 26047 solver.cpp:77] Creating training net from train_net file: /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/train.prototxt
I1128 15:19:18.884021 26047 net.cpp:51] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  transform_param {
    mirror: true
    crop_size: 28
  }
  data_param {
    source: "/home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 32
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Convolution3"
  top: "Convolution3"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Convolution3"
  top: "Convolution4"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 64
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Convolution5"
  top: "Convolution5"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Convolution5"
  top: "Convolution6"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 128
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Convolution7"
  top: "Convolution7"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Convolution7"
  top: "Convolution8"
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 256
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Convolution9"
  top: "Convolution9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 512
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution10"
  top: "Convolution10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  convolution_param {
    num_output: 1024
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution11"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
I1128 15:19:18.884145 26047 layer_factory.hpp:77] Creating layer Data1
I1128 15:19:18.884224 26047 db_lmdb.cpp:35] Opened lmdb /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/train_lmdb
I1128 15:19:18.884271 26047 net.cpp:84] Creating Layer Data1
I1128 15:19:18.884276 26047 net.cpp:380] Data1 -> Data1
I1128 15:19:18.884294 26047 net.cpp:380] Data1 -> Data2
I1128 15:19:18.885011 26047 data_layer.cpp:45] output data size: 128,3,28,28
I1128 15:19:18.888813 26047 net.cpp:122] Setting up Data1
I1128 15:19:18.888833 26047 net.cpp:129] Top shape: 128 3 28 28 (301056)
I1128 15:19:18.888836 26047 net.cpp:129] Top shape: 128 (128)
I1128 15:19:18.888839 26047 net.cpp:137] Memory required for data: 1204736
I1128 15:19:18.888849 26047 layer_factory.hpp:77] Creating layer Convolution1
I1128 15:19:18.888867 26047 net.cpp:84] Creating Layer Convolution1
I1128 15:19:18.888873 26047 net.cpp:406] Convolution1 <- Data1
I1128 15:19:18.888885 26047 net.cpp:380] Convolution1 -> Convolution1
I1128 15:19:19.092967 26047 net.cpp:122] Setting up Convolution1
I1128 15:19:19.093003 26047 net.cpp:129] Top shape: 128 32 28 28 (3211264)
I1128 15:19:19.093008 26047 net.cpp:137] Memory required for data: 14049792
I1128 15:19:19.093031 26047 layer_factory.hpp:77] Creating layer BatchNorm1
I1128 15:19:19.093040 26047 net.cpp:84] Creating Layer BatchNorm1
I1128 15:19:19.093044 26047 net.cpp:406] BatchNorm1 <- Convolution1
I1128 15:19:19.093047 26047 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1128 15:19:19.093183 26047 net.cpp:122] Setting up BatchNorm1
I1128 15:19:19.093186 26047 net.cpp:129] Top shape: 128 32 28 28 (3211264)
I1128 15:19:19.093202 26047 net.cpp:137] Memory required for data: 26894848
I1128 15:19:19.093209 26047 layer_factory.hpp:77] Creating layer Scale1
I1128 15:19:19.093214 26047 net.cpp:84] Creating Layer Scale1
I1128 15:19:19.093215 26047 net.cpp:406] Scale1 <- Convolution1
I1128 15:19:19.093219 26047 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1128 15:19:19.093243 26047 layer_factory.hpp:77] Creating layer Scale1
I1128 15:19:19.093314 26047 net.cpp:122] Setting up Scale1
I1128 15:19:19.093318 26047 net.cpp:129] Top shape: 128 32 28 28 (3211264)
I1128 15:19:19.093320 26047 net.cpp:137] Memory required for data: 39739904
I1128 15:19:19.093323 26047 layer_factory.hpp:77] Creating layer ReLU1
I1128 15:19:19.093328 26047 net.cpp:84] Creating Layer ReLU1
I1128 15:19:19.093330 26047 net.cpp:406] ReLU1 <- Convolution1
I1128 15:19:19.093333 26047 net.cpp:367] ReLU1 -> Convolution1 (in-place)
I1128 15:19:19.093469 26047 net.cpp:122] Setting up ReLU1
I1128 15:19:19.093475 26047 net.cpp:129] Top shape: 128 32 28 28 (3211264)
I1128 15:19:19.093477 26047 net.cpp:137] Memory required for data: 52584960
I1128 15:19:19.093478 26047 layer_factory.hpp:77] Creating layer Convolution2
I1128 15:19:19.093484 26047 net.cpp:84] Creating Layer Convolution2
I1128 15:19:19.093499 26047 net.cpp:406] Convolution2 <- Convolution1
I1128 15:19:19.093502 26047 net.cpp:380] Convolution2 -> Convolution2
I1128 15:19:19.109099 26047 net.cpp:122] Setting up Convolution2
I1128 15:19:19.109117 26047 net.cpp:129] Top shape: 128 32 28 28 (3211264)
I1128 15:19:19.109119 26047 net.cpp:137] Memory required for data: 65430016
I1128 15:19:19.109128 26047 layer_factory.hpp:77] Creating layer BatchNorm2
I1128 15:19:19.109134 26047 net.cpp:84] Creating Layer BatchNorm2
I1128 15:19:19.109138 26047 net.cpp:406] BatchNorm2 <- Convolution2
I1128 15:19:19.109143 26047 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1128 15:19:19.109308 26047 net.cpp:122] Setting up BatchNorm2
I1128 15:19:19.109313 26047 net.cpp:129] Top shape: 128 32 28 28 (3211264)
I1128 15:19:19.109313 26047 net.cpp:137] Memory required for data: 78275072
I1128 15:19:19.109318 26047 layer_factory.hpp:77] Creating layer Scale2
I1128 15:19:19.109321 26047 net.cpp:84] Creating Layer Scale2
I1128 15:19:19.109323 26047 net.cpp:406] Scale2 <- Convolution2
I1128 15:19:19.109325 26047 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1128 15:19:19.109349 26047 layer_factory.hpp:77] Creating layer Scale2
I1128 15:19:19.109457 26047 net.cpp:122] Setting up Scale2
I1128 15:19:19.109460 26047 net.cpp:129] Top shape: 128 32 28 28 (3211264)
I1128 15:19:19.109462 26047 net.cpp:137] Memory required for data: 91120128
I1128 15:19:19.109465 26047 layer_factory.hpp:77] Creating layer ReLU2
I1128 15:19:19.109470 26047 net.cpp:84] Creating Layer ReLU2
I1128 15:19:19.109472 26047 net.cpp:406] ReLU2 <- Convolution2
I1128 15:19:19.109474 26047 net.cpp:367] ReLU2 -> Convolution2 (in-place)
I1128 15:19:19.109619 26047 net.cpp:122] Setting up ReLU2
I1128 15:19:19.109624 26047 net.cpp:129] Top shape: 128 32 28 28 (3211264)
I1128 15:19:19.109625 26047 net.cpp:137] Memory required for data: 103965184
I1128 15:19:19.109627 26047 layer_factory.hpp:77] Creating layer Convolution3
I1128 15:19:19.109633 26047 net.cpp:84] Creating Layer Convolution3
I1128 15:19:19.109635 26047 net.cpp:406] Convolution3 <- Convolution2
I1128 15:19:19.109639 26047 net.cpp:380] Convolution3 -> Convolution3
I1128 15:19:19.110893 26047 net.cpp:122] Setting up Convolution3
I1128 15:19:19.110903 26047 net.cpp:129] Top shape: 128 64 28 28 (6422528)
I1128 15:19:19.110904 26047 net.cpp:137] Memory required for data: 129655296
I1128 15:19:19.110908 26047 layer_factory.hpp:77] Creating layer BatchNorm3
I1128 15:19:19.110913 26047 net.cpp:84] Creating Layer BatchNorm3
I1128 15:19:19.110914 26047 net.cpp:406] BatchNorm3 <- Convolution3
I1128 15:19:19.110918 26047 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1128 15:19:19.111074 26047 net.cpp:122] Setting up BatchNorm3
I1128 15:19:19.111079 26047 net.cpp:129] Top shape: 128 64 28 28 (6422528)
I1128 15:19:19.111080 26047 net.cpp:137] Memory required for data: 155345408
I1128 15:19:19.111088 26047 layer_factory.hpp:77] Creating layer Scale3
I1128 15:19:19.111090 26047 net.cpp:84] Creating Layer Scale3
I1128 15:19:19.111091 26047 net.cpp:406] Scale3 <- Convolution3
I1128 15:19:19.111094 26047 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1128 15:19:19.111116 26047 layer_factory.hpp:77] Creating layer Scale3
I1128 15:19:19.111222 26047 net.cpp:122] Setting up Scale3
I1128 15:19:19.111227 26047 net.cpp:129] Top shape: 128 64 28 28 (6422528)
I1128 15:19:19.111227 26047 net.cpp:137] Memory required for data: 181035520
I1128 15:19:19.111232 26047 layer_factory.hpp:77] Creating layer ReLU3
I1128 15:19:19.111235 26047 net.cpp:84] Creating Layer ReLU3
I1128 15:19:19.111238 26047 net.cpp:406] ReLU3 <- Convolution3
I1128 15:19:19.111239 26047 net.cpp:367] ReLU3 -> Convolution3 (in-place)
I1128 15:19:19.111379 26047 net.cpp:122] Setting up ReLU3
I1128 15:19:19.111384 26047 net.cpp:129] Top shape: 128 64 28 28 (6422528)
I1128 15:19:19.111387 26047 net.cpp:137] Memory required for data: 206725632
I1128 15:19:19.111387 26047 layer_factory.hpp:77] Creating layer Convolution4
I1128 15:19:19.111393 26047 net.cpp:84] Creating Layer Convolution4
I1128 15:19:19.111395 26047 net.cpp:406] Convolution4 <- Convolution3
I1128 15:19:19.111409 26047 net.cpp:380] Convolution4 -> Convolution4
I1128 15:19:19.143265 26047 net.cpp:122] Setting up Convolution4
I1128 15:19:19.143298 26047 net.cpp:129] Top shape: 128 64 14 14 (1605632)
I1128 15:19:19.143301 26047 net.cpp:137] Memory required for data: 213148160
I1128 15:19:19.143309 26047 layer_factory.hpp:77] Creating layer BatchNorm4
I1128 15:19:19.143318 26047 net.cpp:84] Creating Layer BatchNorm4
I1128 15:19:19.143321 26047 net.cpp:406] BatchNorm4 <- Convolution4
I1128 15:19:19.143326 26047 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1128 15:19:19.143491 26047 net.cpp:122] Setting up BatchNorm4
I1128 15:19:19.143496 26047 net.cpp:129] Top shape: 128 64 14 14 (1605632)
I1128 15:19:19.143512 26047 net.cpp:137] Memory required for data: 219570688
I1128 15:19:19.143515 26047 layer_factory.hpp:77] Creating layer Scale4
I1128 15:19:19.143518 26047 net.cpp:84] Creating Layer Scale4
I1128 15:19:19.143522 26047 net.cpp:406] Scale4 <- Convolution4
I1128 15:19:19.143524 26047 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1128 15:19:19.143566 26047 layer_factory.hpp:77] Creating layer Scale4
I1128 15:19:19.143688 26047 net.cpp:122] Setting up Scale4
I1128 15:19:19.143693 26047 net.cpp:129] Top shape: 128 64 14 14 (1605632)
I1128 15:19:19.143695 26047 net.cpp:137] Memory required for data: 225993216
I1128 15:19:19.143712 26047 layer_factory.hpp:77] Creating layer ReLU4
I1128 15:19:19.143718 26047 net.cpp:84] Creating Layer ReLU4
I1128 15:19:19.143720 26047 net.cpp:406] ReLU4 <- Convolution4
I1128 15:19:19.143723 26047 net.cpp:367] ReLU4 -> Convolution4 (in-place)
I1128 15:19:19.143852 26047 net.cpp:122] Setting up ReLU4
I1128 15:19:19.143857 26047 net.cpp:129] Top shape: 128 64 14 14 (1605632)
I1128 15:19:19.143859 26047 net.cpp:137] Memory required for data: 232415744
I1128 15:19:19.143875 26047 layer_factory.hpp:77] Creating layer Convolution5
I1128 15:19:19.143882 26047 net.cpp:84] Creating Layer Convolution5
I1128 15:19:19.143883 26047 net.cpp:406] Convolution5 <- Convolution4
I1128 15:19:19.143888 26047 net.cpp:380] Convolution5 -> Convolution5
I1128 15:19:19.145403 26047 net.cpp:122] Setting up Convolution5
I1128 15:19:19.145412 26047 net.cpp:129] Top shape: 128 128 14 14 (3211264)
I1128 15:19:19.145428 26047 net.cpp:137] Memory required for data: 245260800
I1128 15:19:19.145432 26047 layer_factory.hpp:77] Creating layer BatchNorm5
I1128 15:19:19.145437 26047 net.cpp:84] Creating Layer BatchNorm5
I1128 15:19:19.145439 26047 net.cpp:406] BatchNorm5 <- Convolution5
I1128 15:19:19.145442 26047 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1128 15:19:19.145598 26047 net.cpp:122] Setting up BatchNorm5
I1128 15:19:19.145604 26047 net.cpp:129] Top shape: 128 128 14 14 (3211264)
I1128 15:19:19.145619 26047 net.cpp:137] Memory required for data: 258105856
I1128 15:19:19.145627 26047 layer_factory.hpp:77] Creating layer Scale5
I1128 15:19:19.145629 26047 net.cpp:84] Creating Layer Scale5
I1128 15:19:19.145632 26047 net.cpp:406] Scale5 <- Convolution5
I1128 15:19:19.145635 26047 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1128 15:19:19.145660 26047 layer_factory.hpp:77] Creating layer Scale5
I1128 15:19:19.145767 26047 net.cpp:122] Setting up Scale5
I1128 15:19:19.145772 26047 net.cpp:129] Top shape: 128 128 14 14 (3211264)
I1128 15:19:19.145774 26047 net.cpp:137] Memory required for data: 270950912
I1128 15:19:19.145777 26047 layer_factory.hpp:77] Creating layer ReLU5
I1128 15:19:19.145779 26047 net.cpp:84] Creating Layer ReLU5
I1128 15:19:19.145781 26047 net.cpp:406] ReLU5 <- Convolution5
I1128 15:19:19.145784 26047 net.cpp:367] ReLU5 -> Convolution5 (in-place)
I1128 15:19:19.145917 26047 net.cpp:122] Setting up ReLU5
I1128 15:19:19.145923 26047 net.cpp:129] Top shape: 128 128 14 14 (3211264)
I1128 15:19:19.145939 26047 net.cpp:137] Memory required for data: 283795968
I1128 15:19:19.145941 26047 layer_factory.hpp:77] Creating layer Convolution6
I1128 15:19:19.145947 26047 net.cpp:84] Creating Layer Convolution6
I1128 15:19:19.145949 26047 net.cpp:406] Convolution6 <- Convolution5
I1128 15:19:19.145963 26047 net.cpp:380] Convolution6 -> Convolution6
I1128 15:19:19.212424 26047 net.cpp:122] Setting up Convolution6
I1128 15:19:19.212458 26047 net.cpp:129] Top shape: 128 128 7 7 (802816)
I1128 15:19:19.212462 26047 net.cpp:137] Memory required for data: 287007232
I1128 15:19:19.212472 26047 layer_factory.hpp:77] Creating layer BatchNorm6
I1128 15:19:19.212482 26047 net.cpp:84] Creating Layer BatchNorm6
I1128 15:19:19.212486 26047 net.cpp:406] BatchNorm6 <- Convolution6
I1128 15:19:19.212489 26047 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1128 15:19:19.212694 26047 net.cpp:122] Setting up BatchNorm6
I1128 15:19:19.212698 26047 net.cpp:129] Top shape: 128 128 7 7 (802816)
I1128 15:19:19.212714 26047 net.cpp:137] Memory required for data: 290218496
I1128 15:19:19.212719 26047 layer_factory.hpp:77] Creating layer Scale6
I1128 15:19:19.212723 26047 net.cpp:84] Creating Layer Scale6
I1128 15:19:19.212725 26047 net.cpp:406] Scale6 <- Convolution6
I1128 15:19:19.212728 26047 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1128 15:19:19.212760 26047 layer_factory.hpp:77] Creating layer Scale6
I1128 15:19:19.212901 26047 net.cpp:122] Setting up Scale6
I1128 15:19:19.212905 26047 net.cpp:129] Top shape: 128 128 7 7 (802816)
I1128 15:19:19.212908 26047 net.cpp:137] Memory required for data: 293429760
I1128 15:19:19.212924 26047 layer_factory.hpp:77] Creating layer ReLU6
I1128 15:19:19.212929 26047 net.cpp:84] Creating Layer ReLU6
I1128 15:19:19.212930 26047 net.cpp:406] ReLU6 <- Convolution6
I1128 15:19:19.212932 26047 net.cpp:367] ReLU6 -> Convolution6 (in-place)
I1128 15:19:19.213261 26047 net.cpp:122] Setting up ReLU6
I1128 15:19:19.213284 26047 net.cpp:129] Top shape: 128 128 7 7 (802816)
I1128 15:19:19.213286 26047 net.cpp:137] Memory required for data: 296641024
I1128 15:19:19.213289 26047 layer_factory.hpp:77] Creating layer Convolution7
I1128 15:19:19.213294 26047 net.cpp:84] Creating Layer Convolution7
I1128 15:19:19.213296 26047 net.cpp:406] Convolution7 <- Convolution6
I1128 15:19:19.213300 26047 net.cpp:380] Convolution7 -> Convolution7
I1128 15:19:19.214876 26047 net.cpp:122] Setting up Convolution7
I1128 15:19:19.214900 26047 net.cpp:129] Top shape: 128 256 7 7 (1605632)
I1128 15:19:19.214902 26047 net.cpp:137] Memory required for data: 303063552
I1128 15:19:19.214906 26047 layer_factory.hpp:77] Creating layer BatchNorm7
I1128 15:19:19.214910 26047 net.cpp:84] Creating Layer BatchNorm7
I1128 15:19:19.214911 26047 net.cpp:406] BatchNorm7 <- Convolution7
I1128 15:19:19.214915 26047 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1128 15:19:19.215112 26047 net.cpp:122] Setting up BatchNorm7
I1128 15:19:19.215116 26047 net.cpp:129] Top shape: 128 256 7 7 (1605632)
I1128 15:19:19.215132 26047 net.cpp:137] Memory required for data: 309486080
I1128 15:19:19.215135 26047 layer_factory.hpp:77] Creating layer Scale7
I1128 15:19:19.215139 26047 net.cpp:84] Creating Layer Scale7
I1128 15:19:19.215140 26047 net.cpp:406] Scale7 <- Convolution7
I1128 15:19:19.215143 26047 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1128 15:19:19.215174 26047 layer_factory.hpp:77] Creating layer Scale7
I1128 15:19:19.215304 26047 net.cpp:122] Setting up Scale7
I1128 15:19:19.215308 26047 net.cpp:129] Top shape: 128 256 7 7 (1605632)
I1128 15:19:19.215311 26047 net.cpp:137] Memory required for data: 315908608
I1128 15:19:19.215313 26047 layer_factory.hpp:77] Creating layer ReLU7
I1128 15:19:19.215328 26047 net.cpp:84] Creating Layer ReLU7
I1128 15:19:19.215330 26047 net.cpp:406] ReLU7 <- Convolution7
I1128 15:19:19.215333 26047 net.cpp:367] ReLU7 -> Convolution7 (in-place)
I1128 15:19:19.215487 26047 net.cpp:122] Setting up ReLU7
I1128 15:19:19.215493 26047 net.cpp:129] Top shape: 128 256 7 7 (1605632)
I1128 15:19:19.215509 26047 net.cpp:137] Memory required for data: 322331136
I1128 15:19:19.215510 26047 layer_factory.hpp:77] Creating layer Convolution8
I1128 15:19:19.215515 26047 net.cpp:84] Creating Layer Convolution8
I1128 15:19:19.215518 26047 net.cpp:406] Convolution8 <- Convolution7
I1128 15:19:19.215533 26047 net.cpp:380] Convolution8 -> Convolution8
I1128 15:19:19.360308 26047 net.cpp:122] Setting up Convolution8
I1128 15:19:19.360342 26047 net.cpp:129] Top shape: 128 256 4 4 (524288)
I1128 15:19:19.360347 26047 net.cpp:137] Memory required for data: 324428288
I1128 15:19:19.360357 26047 layer_factory.hpp:77] Creating layer BatchNorm8
I1128 15:19:19.360366 26047 net.cpp:84] Creating Layer BatchNorm8
I1128 15:19:19.360368 26047 net.cpp:406] BatchNorm8 <- Convolution8
I1128 15:19:19.360374 26047 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1128 15:19:19.360687 26047 net.cpp:122] Setting up BatchNorm8
I1128 15:19:19.360692 26047 net.cpp:129] Top shape: 128 256 4 4 (524288)
I1128 15:19:19.360707 26047 net.cpp:137] Memory required for data: 326525440
I1128 15:19:19.360711 26047 layer_factory.hpp:77] Creating layer Scale8
I1128 15:19:19.360715 26047 net.cpp:84] Creating Layer Scale8
I1128 15:19:19.360716 26047 net.cpp:406] Scale8 <- Convolution8
I1128 15:19:19.360720 26047 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1128 15:19:19.360769 26047 layer_factory.hpp:77] Creating layer Scale8
I1128 15:19:19.360941 26047 net.cpp:122] Setting up Scale8
I1128 15:19:19.360945 26047 net.cpp:129] Top shape: 128 256 4 4 (524288)
I1128 15:19:19.360961 26047 net.cpp:137] Memory required for data: 328622592
I1128 15:19:19.360965 26047 layer_factory.hpp:77] Creating layer ReLU8
I1128 15:19:19.360971 26047 net.cpp:84] Creating Layer ReLU8
I1128 15:19:19.360973 26047 net.cpp:406] ReLU8 <- Convolution8
I1128 15:19:19.360975 26047 net.cpp:367] ReLU8 -> Convolution8 (in-place)
I1128 15:19:19.361112 26047 net.cpp:122] Setting up ReLU8
I1128 15:19:19.361117 26047 net.cpp:129] Top shape: 128 256 4 4 (524288)
I1128 15:19:19.361133 26047 net.cpp:137] Memory required for data: 330719744
I1128 15:19:19.361135 26047 layer_factory.hpp:77] Creating layer Convolution9
I1128 15:19:19.361141 26047 net.cpp:84] Creating Layer Convolution9
I1128 15:19:19.361143 26047 net.cpp:406] Convolution9 <- Convolution8
I1128 15:19:19.361147 26047 net.cpp:380] Convolution9 -> Convolution9
I1128 15:19:19.362934 26047 net.cpp:122] Setting up Convolution9
I1128 15:19:19.362958 26047 net.cpp:129] Top shape: 128 512 4 4 (1048576)
I1128 15:19:19.362960 26047 net.cpp:137] Memory required for data: 334914048
I1128 15:19:19.362963 26047 layer_factory.hpp:77] Creating layer BatchNorm9
I1128 15:19:19.362967 26047 net.cpp:84] Creating Layer BatchNorm9
I1128 15:19:19.362969 26047 net.cpp:406] BatchNorm9 <- Convolution9
I1128 15:19:19.362972 26047 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1128 15:19:19.363277 26047 net.cpp:122] Setting up BatchNorm9
I1128 15:19:19.363281 26047 net.cpp:129] Top shape: 128 512 4 4 (1048576)
I1128 15:19:19.363298 26047 net.cpp:137] Memory required for data: 339108352
I1128 15:19:19.363301 26047 layer_factory.hpp:77] Creating layer Scale9
I1128 15:19:19.363304 26047 net.cpp:84] Creating Layer Scale9
I1128 15:19:19.363306 26047 net.cpp:406] Scale9 <- Convolution9
I1128 15:19:19.363309 26047 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1128 15:19:19.363353 26047 layer_factory.hpp:77] Creating layer Scale9
I1128 15:19:19.363559 26047 net.cpp:122] Setting up Scale9
I1128 15:19:19.363562 26047 net.cpp:129] Top shape: 128 512 4 4 (1048576)
I1128 15:19:19.363579 26047 net.cpp:137] Memory required for data: 343302656
I1128 15:19:19.363581 26047 layer_factory.hpp:77] Creating layer ReLU9
I1128 15:19:19.363584 26047 net.cpp:84] Creating Layer ReLU9
I1128 15:19:19.363586 26047 net.cpp:406] ReLU9 <- Convolution9
I1128 15:19:19.363590 26047 net.cpp:367] ReLU9 -> Convolution9 (in-place)
I1128 15:19:19.363931 26047 net.cpp:122] Setting up ReLU9
I1128 15:19:19.363955 26047 net.cpp:129] Top shape: 128 512 4 4 (1048576)
I1128 15:19:19.363956 26047 net.cpp:137] Memory required for data: 347496960
I1128 15:19:19.363958 26047 layer_factory.hpp:77] Creating layer Convolution10
I1128 15:19:19.363963 26047 net.cpp:84] Creating Layer Convolution10
I1128 15:19:19.363965 26047 net.cpp:406] Convolution10 <- Convolution9
I1128 15:19:19.363981 26047 net.cpp:380] Convolution10 -> Convolution10
I1128 15:19:19.660491 26047 net.cpp:122] Setting up Convolution10
I1128 15:19:19.660526 26047 net.cpp:129] Top shape: 128 512 2 2 (262144)
I1128 15:19:19.660529 26047 net.cpp:137] Memory required for data: 348545536
I1128 15:19:19.660550 26047 layer_factory.hpp:77] Creating layer BatchNorm10
I1128 15:19:19.660560 26047 net.cpp:84] Creating Layer BatchNorm10
I1128 15:19:19.660565 26047 net.cpp:406] BatchNorm10 <- Convolution10
I1128 15:19:19.660568 26047 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1128 15:19:19.661201 26047 net.cpp:122] Setting up BatchNorm10
I1128 15:19:19.661206 26047 net.cpp:129] Top shape: 128 512 2 2 (262144)
I1128 15:19:19.661221 26047 net.cpp:137] Memory required for data: 349594112
I1128 15:19:19.661226 26047 layer_factory.hpp:77] Creating layer Scale10
I1128 15:19:19.661231 26047 net.cpp:84] Creating Layer Scale10
I1128 15:19:19.661233 26047 net.cpp:406] Scale10 <- Convolution10
I1128 15:19:19.661237 26047 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1128 15:19:19.661331 26047 layer_factory.hpp:77] Creating layer Scale10
I1128 15:19:19.661676 26047 net.cpp:122] Setting up Scale10
I1128 15:19:19.661681 26047 net.cpp:129] Top shape: 128 512 2 2 (262144)
I1128 15:19:19.661697 26047 net.cpp:137] Memory required for data: 350642688
I1128 15:19:19.661700 26047 layer_factory.hpp:77] Creating layer ReLU10
I1128 15:19:19.661703 26047 net.cpp:84] Creating Layer ReLU10
I1128 15:19:19.661706 26047 net.cpp:406] ReLU10 <- Convolution10
I1128 15:19:19.661708 26047 net.cpp:367] ReLU10 -> Convolution10 (in-place)
I1128 15:19:19.662083 26047 net.cpp:122] Setting up ReLU10
I1128 15:19:19.662106 26047 net.cpp:129] Top shape: 128 512 2 2 (262144)
I1128 15:19:19.662108 26047 net.cpp:137] Memory required for data: 351691264
I1128 15:19:19.662111 26047 layer_factory.hpp:77] Creating layer Convolution11
I1128 15:19:19.662117 26047 net.cpp:84] Creating Layer Convolution11
I1128 15:19:19.662119 26047 net.cpp:406] Convolution11 <- Convolution10
I1128 15:19:19.662123 26047 net.cpp:380] Convolution11 -> Convolution11
I1128 15:19:19.667332 26047 net.cpp:122] Setting up Convolution11
I1128 15:19:19.667356 26047 net.cpp:129] Top shape: 128 1024 2 2 (524288)
I1128 15:19:19.667359 26047 net.cpp:137] Memory required for data: 353788416
I1128 15:19:19.667363 26047 layer_factory.hpp:77] Creating layer BatchNorm11
I1128 15:19:19.667369 26047 net.cpp:84] Creating Layer BatchNorm11
I1128 15:19:19.667371 26047 net.cpp:406] BatchNorm11 <- Convolution11
I1128 15:19:19.667374 26047 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1128 15:19:19.668042 26047 net.cpp:122] Setting up BatchNorm11
I1128 15:19:19.668048 26047 net.cpp:129] Top shape: 128 1024 2 2 (524288)
I1128 15:19:19.668051 26047 net.cpp:137] Memory required for data: 355885568
I1128 15:19:19.668066 26047 layer_factory.hpp:77] Creating layer Scale11
I1128 15:19:19.668071 26047 net.cpp:84] Creating Layer Scale11
I1128 15:19:19.668071 26047 net.cpp:406] Scale11 <- Convolution11
I1128 15:19:19.668074 26047 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1128 15:19:19.668218 26047 layer_factory.hpp:77] Creating layer Scale11
I1128 15:19:19.668565 26047 net.cpp:122] Setting up Scale11
I1128 15:19:19.668568 26047 net.cpp:129] Top shape: 128 1024 2 2 (524288)
I1128 15:19:19.668570 26047 net.cpp:137] Memory required for data: 357982720
I1128 15:19:19.668587 26047 layer_factory.hpp:77] Creating layer ReLU11
I1128 15:19:19.668592 26047 net.cpp:84] Creating Layer ReLU11
I1128 15:19:19.668594 26047 net.cpp:406] ReLU11 <- Convolution11
I1128 15:19:19.668596 26047 net.cpp:367] ReLU11 -> Convolution11 (in-place)
I1128 15:19:19.668733 26047 net.cpp:122] Setting up ReLU11
I1128 15:19:19.668738 26047 net.cpp:129] Top shape: 128 1024 2 2 (524288)
I1128 15:19:19.668754 26047 net.cpp:137] Memory required for data: 360079872
I1128 15:19:19.668756 26047 layer_factory.hpp:77] Creating layer Pooling1
I1128 15:19:19.668759 26047 net.cpp:84] Creating Layer Pooling1
I1128 15:19:19.668761 26047 net.cpp:406] Pooling1 <- Convolution11
I1128 15:19:19.668777 26047 net.cpp:380] Pooling1 -> Pooling1
I1128 15:19:19.669230 26047 net.cpp:122] Setting up Pooling1
I1128 15:19:19.669239 26047 net.cpp:129] Top shape: 128 1024 1 1 (131072)
I1128 15:19:19.669255 26047 net.cpp:137] Memory required for data: 360604160
I1128 15:19:19.669256 26047 layer_factory.hpp:77] Creating layer InnerProduct1
I1128 15:19:19.669260 26047 net.cpp:84] Creating Layer InnerProduct1
I1128 15:19:19.669262 26047 net.cpp:406] InnerProduct1 <- Pooling1
I1128 15:19:19.669266 26047 net.cpp:380] InnerProduct1 -> InnerProduct1
I1128 15:19:19.669626 26047 net.cpp:122] Setting up InnerProduct1
I1128 15:19:19.669631 26047 net.cpp:129] Top shape: 128 10 (1280)
I1128 15:19:19.669632 26047 net.cpp:137] Memory required for data: 360609280
I1128 15:19:19.669651 26047 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1128 15:19:19.669654 26047 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1128 15:19:19.669656 26047 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1
I1128 15:19:19.669658 26047 net.cpp:406] SoftmaxWithLoss1 <- Data2
I1128 15:19:19.669663 26047 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1128 15:19:19.669669 26047 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1128 15:19:19.670065 26047 net.cpp:122] Setting up SoftmaxWithLoss1
I1128 15:19:19.670071 26047 net.cpp:129] Top shape: (1)
I1128 15:19:19.670073 26047 net.cpp:132]     with loss weight 1
I1128 15:19:19.670090 26047 net.cpp:137] Memory required for data: 360609284
I1128 15:19:19.670092 26047 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1128 15:19:19.670097 26047 net.cpp:198] InnerProduct1 needs backward computation.
I1128 15:19:19.670099 26047 net.cpp:198] Pooling1 needs backward computation.
I1128 15:19:19.670102 26047 net.cpp:198] ReLU11 needs backward computation.
I1128 15:19:19.670104 26047 net.cpp:198] Scale11 needs backward computation.
I1128 15:19:19.670105 26047 net.cpp:198] BatchNorm11 needs backward computation.
I1128 15:19:19.670107 26047 net.cpp:198] Convolution11 needs backward computation.
I1128 15:19:19.670125 26047 net.cpp:198] ReLU10 needs backward computation.
I1128 15:19:19.670126 26047 net.cpp:198] Scale10 needs backward computation.
I1128 15:19:19.670128 26047 net.cpp:198] BatchNorm10 needs backward computation.
I1128 15:19:19.670130 26047 net.cpp:198] Convolution10 needs backward computation.
I1128 15:19:19.670132 26047 net.cpp:198] ReLU9 needs backward computation.
I1128 15:19:19.670133 26047 net.cpp:198] Scale9 needs backward computation.
I1128 15:19:19.670135 26047 net.cpp:198] BatchNorm9 needs backward computation.
I1128 15:19:19.670136 26047 net.cpp:198] Convolution9 needs backward computation.
I1128 15:19:19.670140 26047 net.cpp:198] ReLU8 needs backward computation.
I1128 15:19:19.670141 26047 net.cpp:198] Scale8 needs backward computation.
I1128 15:19:19.670142 26047 net.cpp:198] BatchNorm8 needs backward computation.
I1128 15:19:19.670145 26047 net.cpp:198] Convolution8 needs backward computation.
I1128 15:19:19.670146 26047 net.cpp:198] ReLU7 needs backward computation.
I1128 15:19:19.670148 26047 net.cpp:198] Scale7 needs backward computation.
I1128 15:19:19.670150 26047 net.cpp:198] BatchNorm7 needs backward computation.
I1128 15:19:19.670151 26047 net.cpp:198] Convolution7 needs backward computation.
I1128 15:19:19.670153 26047 net.cpp:198] ReLU6 needs backward computation.
I1128 15:19:19.670156 26047 net.cpp:198] Scale6 needs backward computation.
I1128 15:19:19.670157 26047 net.cpp:198] BatchNorm6 needs backward computation.
I1128 15:19:19.670158 26047 net.cpp:198] Convolution6 needs backward computation.
I1128 15:19:19.670161 26047 net.cpp:198] ReLU5 needs backward computation.
I1128 15:19:19.670162 26047 net.cpp:198] Scale5 needs backward computation.
I1128 15:19:19.670163 26047 net.cpp:198] BatchNorm5 needs backward computation.
I1128 15:19:19.670166 26047 net.cpp:198] Convolution5 needs backward computation.
I1128 15:19:19.670168 26047 net.cpp:198] ReLU4 needs backward computation.
I1128 15:19:19.670171 26047 net.cpp:198] Scale4 needs backward computation.
I1128 15:19:19.670179 26047 net.cpp:198] BatchNorm4 needs backward computation.
I1128 15:19:19.670181 26047 net.cpp:198] Convolution4 needs backward computation.
I1128 15:19:19.670183 26047 net.cpp:198] ReLU3 needs backward computation.
I1128 15:19:19.670186 26047 net.cpp:198] Scale3 needs backward computation.
I1128 15:19:19.670186 26047 net.cpp:198] BatchNorm3 needs backward computation.
I1128 15:19:19.670188 26047 net.cpp:198] Convolution3 needs backward computation.
I1128 15:19:19.670191 26047 net.cpp:198] ReLU2 needs backward computation.
I1128 15:19:19.670192 26047 net.cpp:198] Scale2 needs backward computation.
I1128 15:19:19.670194 26047 net.cpp:198] BatchNorm2 needs backward computation.
I1128 15:19:19.670195 26047 net.cpp:198] Convolution2 needs backward computation.
I1128 15:19:19.670210 26047 net.cpp:198] ReLU1 needs backward computation.
I1128 15:19:19.670212 26047 net.cpp:198] Scale1 needs backward computation.
I1128 15:19:19.670213 26047 net.cpp:198] BatchNorm1 needs backward computation.
I1128 15:19:19.670217 26047 net.cpp:198] Convolution1 needs backward computation.
I1128 15:19:19.670218 26047 net.cpp:200] Data1 does not need backward computation.
I1128 15:19:19.670220 26047 net.cpp:242] This network produces output SoftmaxWithLoss1
I1128 15:19:19.670250 26047 net.cpp:255] Network initialization done.
I1128 15:19:19.670503 26047 solver.cpp:172] Creating test net (#0) specified by test_net file: /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/test.prototxt
I1128 15:19:19.670670 26047 net.cpp:51] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  transform_param {
    crop_size: 28
  }
  data_param {
    source: "/home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/test_lmdb"
    batch_size: 100
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 32
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Convolution3"
  top: "Convolution3"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Convolution3"
  top: "Convolution4"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 64
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Convolution5"
  top: "Convolution5"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Convolution5"
  top: "Convolution6"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 128
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Convolution7"
  top: "Convolution7"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Convolution7"
  top: "Convolution8"
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 256
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Convolution9"
  top: "Convolution9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 512
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution10"
  top: "Convolution10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  convolution_param {
    num_output: 1024
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution11"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Accuracy1"
  type: "Accuracy"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "Accuracy1"
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
I1128 15:19:19.670775 26047 layer_factory.hpp:77] Creating layer Data1
I1128 15:19:19.670814 26047 db_lmdb.cpp:35] Opened lmdb /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/test_lmdb
I1128 15:19:19.670822 26047 net.cpp:84] Creating Layer Data1
I1128 15:19:19.670825 26047 net.cpp:380] Data1 -> Data1
I1128 15:19:19.670831 26047 net.cpp:380] Data1 -> Data2
I1128 15:19:19.671104 26047 data_layer.cpp:45] output data size: 100,3,28,28
I1128 15:19:19.674434 26047 net.cpp:122] Setting up Data1
I1128 15:19:19.674466 26047 net.cpp:129] Top shape: 100 3 28 28 (235200)
I1128 15:19:19.674470 26047 net.cpp:129] Top shape: 100 (100)
I1128 15:19:19.674473 26047 net.cpp:137] Memory required for data: 941200
I1128 15:19:19.674477 26047 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I1128 15:19:19.674487 26047 net.cpp:84] Creating Layer Data2_Data1_1_split
I1128 15:19:19.674490 26047 net.cpp:406] Data2_Data1_1_split <- Data2
I1128 15:19:19.674495 26047 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_0
I1128 15:19:19.674504 26047 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_1
I1128 15:19:19.674641 26047 net.cpp:122] Setting up Data2_Data1_1_split
I1128 15:19:19.674646 26047 net.cpp:129] Top shape: 100 (100)
I1128 15:19:19.674649 26047 net.cpp:129] Top shape: 100 (100)
I1128 15:19:19.674650 26047 net.cpp:137] Memory required for data: 942000
I1128 15:19:19.674651 26047 layer_factory.hpp:77] Creating layer Convolution1
I1128 15:19:19.674661 26047 net.cpp:84] Creating Layer Convolution1
I1128 15:19:19.674664 26047 net.cpp:406] Convolution1 <- Data1
I1128 15:19:19.674667 26047 net.cpp:380] Convolution1 -> Convolution1
I1128 15:19:19.676676 26047 net.cpp:122] Setting up Convolution1
I1128 15:19:19.676687 26047 net.cpp:129] Top shape: 100 32 28 28 (2508800)
I1128 15:19:19.676689 26047 net.cpp:137] Memory required for data: 10977200
I1128 15:19:19.676697 26047 layer_factory.hpp:77] Creating layer BatchNorm1
I1128 15:19:19.676703 26047 net.cpp:84] Creating Layer BatchNorm1
I1128 15:19:19.676707 26047 net.cpp:406] BatchNorm1 <- Convolution1
I1128 15:19:19.676710 26047 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1128 15:19:19.677372 26047 net.cpp:122] Setting up BatchNorm1
I1128 15:19:19.677378 26047 net.cpp:129] Top shape: 100 32 28 28 (2508800)
I1128 15:19:19.677392 26047 net.cpp:137] Memory required for data: 21012400
I1128 15:19:19.677398 26047 layer_factory.hpp:77] Creating layer Scale1
I1128 15:19:19.677403 26047 net.cpp:84] Creating Layer Scale1
I1128 15:19:19.677405 26047 net.cpp:406] Scale1 <- Convolution1
I1128 15:19:19.677409 26047 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1128 15:19:19.677582 26047 layer_factory.hpp:77] Creating layer Scale1
I1128 15:19:19.677948 26047 net.cpp:122] Setting up Scale1
I1128 15:19:19.677953 26047 net.cpp:129] Top shape: 100 32 28 28 (2508800)
I1128 15:19:19.677955 26047 net.cpp:137] Memory required for data: 31047600
I1128 15:19:19.677959 26047 layer_factory.hpp:77] Creating layer ReLU1
I1128 15:19:19.677963 26047 net.cpp:84] Creating Layer ReLU1
I1128 15:19:19.677964 26047 net.cpp:406] ReLU1 <- Convolution1
I1128 15:19:19.677968 26047 net.cpp:367] ReLU1 -> Convolution1 (in-place)
I1128 15:19:19.678493 26047 net.cpp:122] Setting up ReLU1
I1128 15:19:19.678499 26047 net.cpp:129] Top shape: 100 32 28 28 (2508800)
I1128 15:19:19.678501 26047 net.cpp:137] Memory required for data: 41082800
I1128 15:19:19.678504 26047 layer_factory.hpp:77] Creating layer Convolution2
I1128 15:19:19.678510 26047 net.cpp:84] Creating Layer Convolution2
I1128 15:19:19.678513 26047 net.cpp:406] Convolution2 <- Convolution1
I1128 15:19:19.678515 26047 net.cpp:380] Convolution2 -> Convolution2
I1128 15:19:19.698778 26047 net.cpp:122] Setting up Convolution2
I1128 15:19:19.698797 26047 net.cpp:129] Top shape: 100 32 28 28 (2508800)
I1128 15:19:19.698799 26047 net.cpp:137] Memory required for data: 51118000
I1128 15:19:19.698808 26047 layer_factory.hpp:77] Creating layer BatchNorm2
I1128 15:19:19.698815 26047 net.cpp:84] Creating Layer BatchNorm2
I1128 15:19:19.698817 26047 net.cpp:406] BatchNorm2 <- Convolution2
I1128 15:19:19.698822 26047 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1128 15:19:19.699460 26047 net.cpp:122] Setting up BatchNorm2
I1128 15:19:19.699465 26047 net.cpp:129] Top shape: 100 32 28 28 (2508800)
I1128 15:19:19.699466 26047 net.cpp:137] Memory required for data: 61153200
I1128 15:19:19.699470 26047 layer_factory.hpp:77] Creating layer Scale2
I1128 15:19:19.699476 26047 net.cpp:84] Creating Layer Scale2
I1128 15:19:19.699478 26047 net.cpp:406] Scale2 <- Convolution2
I1128 15:19:19.699481 26047 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1128 15:19:19.699576 26047 layer_factory.hpp:77] Creating layer Scale2
I1128 15:19:19.699954 26047 net.cpp:122] Setting up Scale2
I1128 15:19:19.699960 26047 net.cpp:129] Top shape: 100 32 28 28 (2508800)
I1128 15:19:19.699961 26047 net.cpp:137] Memory required for data: 71188400
I1128 15:19:19.699965 26047 layer_factory.hpp:77] Creating layer ReLU2
I1128 15:19:19.699968 26047 net.cpp:84] Creating Layer ReLU2
I1128 15:19:19.699970 26047 net.cpp:406] ReLU2 <- Convolution2
I1128 15:19:19.699973 26047 net.cpp:367] ReLU2 -> Convolution2 (in-place)
I1128 15:19:19.700366 26047 net.cpp:122] Setting up ReLU2
I1128 15:19:19.700374 26047 net.cpp:129] Top shape: 100 32 28 28 (2508800)
I1128 15:19:19.700376 26047 net.cpp:137] Memory required for data: 81223600
I1128 15:19:19.700378 26047 layer_factory.hpp:77] Creating layer Convolution3
I1128 15:19:19.700384 26047 net.cpp:84] Creating Layer Convolution3
I1128 15:19:19.700387 26047 net.cpp:406] Convolution3 <- Convolution2
I1128 15:19:19.700390 26047 net.cpp:380] Convolution3 -> Convolution3
I1128 15:19:19.701812 26047 net.cpp:122] Setting up Convolution3
I1128 15:19:19.701818 26047 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1128 15:19:19.701820 26047 net.cpp:137] Memory required for data: 101294000
I1128 15:19:19.701824 26047 layer_factory.hpp:77] Creating layer BatchNorm3
I1128 15:19:19.701828 26047 net.cpp:84] Creating Layer BatchNorm3
I1128 15:19:19.701830 26047 net.cpp:406] BatchNorm3 <- Convolution3
I1128 15:19:19.701833 26047 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1128 15:19:19.702452 26047 net.cpp:122] Setting up BatchNorm3
I1128 15:19:19.702457 26047 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1128 15:19:19.702469 26047 net.cpp:137] Memory required for data: 121364400
I1128 15:19:19.702476 26047 layer_factory.hpp:77] Creating layer Scale3
I1128 15:19:19.702481 26047 net.cpp:84] Creating Layer Scale3
I1128 15:19:19.702481 26047 net.cpp:406] Scale3 <- Convolution3
I1128 15:19:19.702484 26047 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1128 15:19:19.702579 26047 layer_factory.hpp:77] Creating layer Scale3
I1128 15:19:19.702922 26047 net.cpp:122] Setting up Scale3
I1128 15:19:19.702927 26047 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1128 15:19:19.702929 26047 net.cpp:137] Memory required for data: 141434800
I1128 15:19:19.702932 26047 layer_factory.hpp:77] Creating layer ReLU3
I1128 15:19:19.702936 26047 net.cpp:84] Creating Layer ReLU3
I1128 15:19:19.702937 26047 net.cpp:406] ReLU3 <- Convolution3
I1128 15:19:19.702940 26047 net.cpp:367] ReLU3 -> Convolution3 (in-place)
I1128 15:19:19.703073 26047 net.cpp:122] Setting up ReLU3
I1128 15:19:19.703078 26047 net.cpp:129] Top shape: 100 64 28 28 (5017600)
I1128 15:19:19.703080 26047 net.cpp:137] Memory required for data: 161505200
I1128 15:19:19.703081 26047 layer_factory.hpp:77] Creating layer Convolution4
I1128 15:19:19.703088 26047 net.cpp:84] Creating Layer Convolution4
I1128 15:19:19.703089 26047 net.cpp:406] Convolution4 <- Convolution3
I1128 15:19:19.703092 26047 net.cpp:380] Convolution4 -> Convolution4
I1128 15:19:19.742725 26047 net.cpp:122] Setting up Convolution4
I1128 15:19:19.742759 26047 net.cpp:129] Top shape: 100 64 14 14 (1254400)
I1128 15:19:19.742763 26047 net.cpp:137] Memory required for data: 166522800
I1128 15:19:19.742771 26047 layer_factory.hpp:77] Creating layer BatchNorm4
I1128 15:19:19.742779 26047 net.cpp:84] Creating Layer BatchNorm4
I1128 15:19:19.742782 26047 net.cpp:406] BatchNorm4 <- Convolution4
I1128 15:19:19.742789 26047 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1128 15:19:19.743472 26047 net.cpp:122] Setting up BatchNorm4
I1128 15:19:19.743477 26047 net.cpp:129] Top shape: 100 64 14 14 (1254400)
I1128 15:19:19.743494 26047 net.cpp:137] Memory required for data: 171540400
I1128 15:19:19.743499 26047 layer_factory.hpp:77] Creating layer Scale4
I1128 15:19:19.743505 26047 net.cpp:84] Creating Layer Scale4
I1128 15:19:19.743506 26047 net.cpp:406] Scale4 <- Convolution4
I1128 15:19:19.743510 26047 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1128 15:19:19.743643 26047 layer_factory.hpp:77] Creating layer Scale4
I1128 15:19:19.744066 26047 net.cpp:122] Setting up Scale4
I1128 15:19:19.744071 26047 net.cpp:129] Top shape: 100 64 14 14 (1254400)
I1128 15:19:19.744072 26047 net.cpp:137] Memory required for data: 176558000
I1128 15:19:19.744091 26047 layer_factory.hpp:77] Creating layer ReLU4
I1128 15:19:19.744094 26047 net.cpp:84] Creating Layer ReLU4
I1128 15:19:19.744096 26047 net.cpp:406] ReLU4 <- Convolution4
I1128 15:19:19.744099 26047 net.cpp:367] ReLU4 -> Convolution4 (in-place)
I1128 15:19:19.744504 26047 net.cpp:122] Setting up ReLU4
I1128 15:19:19.744513 26047 net.cpp:129] Top shape: 100 64 14 14 (1254400)
I1128 15:19:19.744514 26047 net.cpp:137] Memory required for data: 181575600
I1128 15:19:19.744530 26047 layer_factory.hpp:77] Creating layer Convolution5
I1128 15:19:19.744537 26047 net.cpp:84] Creating Layer Convolution5
I1128 15:19:19.744539 26047 net.cpp:406] Convolution5 <- Convolution4
I1128 15:19:19.744542 26047 net.cpp:380] Convolution5 -> Convolution5
I1128 15:19:19.746119 26047 net.cpp:122] Setting up Convolution5
I1128 15:19:19.746141 26047 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1128 15:19:19.746145 26047 net.cpp:137] Memory required for data: 191610800
I1128 15:19:19.746147 26047 layer_factory.hpp:77] Creating layer BatchNorm5
I1128 15:19:19.746151 26047 net.cpp:84] Creating Layer BatchNorm5
I1128 15:19:19.746155 26047 net.cpp:406] BatchNorm5 <- Convolution5
I1128 15:19:19.746157 26047 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1128 15:19:19.746822 26047 net.cpp:122] Setting up BatchNorm5
I1128 15:19:19.746826 26047 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1128 15:19:19.746853 26047 net.cpp:137] Memory required for data: 201646000
I1128 15:19:19.746861 26047 layer_factory.hpp:77] Creating layer Scale5
I1128 15:19:19.746865 26047 net.cpp:84] Creating Layer Scale5
I1128 15:19:19.746867 26047 net.cpp:406] Scale5 <- Convolution5
I1128 15:19:19.746871 26047 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1128 15:19:19.747011 26047 layer_factory.hpp:77] Creating layer Scale5
I1128 15:19:19.747392 26047 net.cpp:122] Setting up Scale5
I1128 15:19:19.747397 26047 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1128 15:19:19.747413 26047 net.cpp:137] Memory required for data: 211681200
I1128 15:19:19.747416 26047 layer_factory.hpp:77] Creating layer ReLU5
I1128 15:19:19.747419 26047 net.cpp:84] Creating Layer ReLU5
I1128 15:19:19.747421 26047 net.cpp:406] ReLU5 <- Convolution5
I1128 15:19:19.747424 26047 net.cpp:367] ReLU5 -> Convolution5 (in-place)
I1128 15:19:19.747560 26047 net.cpp:122] Setting up ReLU5
I1128 15:19:19.747565 26047 net.cpp:129] Top shape: 100 128 14 14 (2508800)
I1128 15:19:19.747582 26047 net.cpp:137] Memory required for data: 221716400
I1128 15:19:19.747584 26047 layer_factory.hpp:77] Creating layer Convolution6
I1128 15:19:19.747589 26047 net.cpp:84] Creating Layer Convolution6
I1128 15:19:19.747592 26047 net.cpp:406] Convolution6 <- Convolution5
I1128 15:19:19.747596 26047 net.cpp:380] Convolution6 -> Convolution6
I1128 15:19:19.833863 26047 net.cpp:122] Setting up Convolution6
I1128 15:19:19.833883 26047 net.cpp:129] Top shape: 100 128 7 7 (627200)
I1128 15:19:19.833899 26047 net.cpp:137] Memory required for data: 224225200
I1128 15:19:19.833907 26047 layer_factory.hpp:77] Creating layer BatchNorm6
I1128 15:19:19.833916 26047 net.cpp:84] Creating Layer BatchNorm6
I1128 15:19:19.833920 26047 net.cpp:406] BatchNorm6 <- Convolution6
I1128 15:19:19.833925 26047 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1128 15:19:19.834703 26047 net.cpp:122] Setting up BatchNorm6
I1128 15:19:19.834708 26047 net.cpp:129] Top shape: 100 128 7 7 (627200)
I1128 15:19:19.834725 26047 net.cpp:137] Memory required for data: 226734000
I1128 15:19:19.834729 26047 layer_factory.hpp:77] Creating layer Scale6
I1128 15:19:19.834733 26047 net.cpp:84] Creating Layer Scale6
I1128 15:19:19.834735 26047 net.cpp:406] Scale6 <- Convolution6
I1128 15:19:19.834738 26047 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1128 15:19:19.834867 26047 layer_factory.hpp:77] Creating layer Scale6
I1128 15:19:19.835301 26047 net.cpp:122] Setting up Scale6
I1128 15:19:19.835306 26047 net.cpp:129] Top shape: 100 128 7 7 (627200)
I1128 15:19:19.835322 26047 net.cpp:137] Memory required for data: 229242800
I1128 15:19:19.835325 26047 layer_factory.hpp:77] Creating layer ReLU6
I1128 15:19:19.835328 26047 net.cpp:84] Creating Layer ReLU6
I1128 15:19:19.835330 26047 net.cpp:406] ReLU6 <- Convolution6
I1128 15:19:19.835335 26047 net.cpp:367] ReLU6 -> Convolution6 (in-place)
I1128 15:19:19.835481 26047 net.cpp:122] Setting up ReLU6
I1128 15:19:19.835486 26047 net.cpp:129] Top shape: 100 128 7 7 (627200)
I1128 15:19:19.835489 26047 net.cpp:137] Memory required for data: 231751600
I1128 15:19:19.835490 26047 layer_factory.hpp:77] Creating layer Convolution7
I1128 15:19:19.835497 26047 net.cpp:84] Creating Layer Convolution7
I1128 15:19:19.835500 26047 net.cpp:406] Convolution7 <- Convolution6
I1128 15:19:19.835502 26047 net.cpp:380] Convolution7 -> Convolution7
I1128 15:19:19.837456 26047 net.cpp:122] Setting up Convolution7
I1128 15:19:19.837465 26047 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1128 15:19:19.837467 26047 net.cpp:137] Memory required for data: 236769200
I1128 15:19:19.837471 26047 layer_factory.hpp:77] Creating layer BatchNorm7
I1128 15:19:19.837476 26047 net.cpp:84] Creating Layer BatchNorm7
I1128 15:19:19.837478 26047 net.cpp:406] BatchNorm7 <- Convolution7
I1128 15:19:19.837481 26047 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1128 15:19:19.838243 26047 net.cpp:122] Setting up BatchNorm7
I1128 15:19:19.838248 26047 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1128 15:19:19.838261 26047 net.cpp:137] Memory required for data: 241786800
I1128 15:19:19.838265 26047 layer_factory.hpp:77] Creating layer Scale7
I1128 15:19:19.838268 26047 net.cpp:84] Creating Layer Scale7
I1128 15:19:19.838270 26047 net.cpp:406] Scale7 <- Convolution7
I1128 15:19:19.838274 26047 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1128 15:19:19.838428 26047 layer_factory.hpp:77] Creating layer Scale7
I1128 15:19:19.838865 26047 net.cpp:122] Setting up Scale7
I1128 15:19:19.838870 26047 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1128 15:19:19.838886 26047 net.cpp:137] Memory required for data: 246804400
I1128 15:19:19.838889 26047 layer_factory.hpp:77] Creating layer ReLU7
I1128 15:19:19.838892 26047 net.cpp:84] Creating Layer ReLU7
I1128 15:19:19.838893 26047 net.cpp:406] ReLU7 <- Convolution7
I1128 15:19:19.838897 26047 net.cpp:367] ReLU7 -> Convolution7 (in-place)
I1128 15:19:19.839326 26047 net.cpp:122] Setting up ReLU7
I1128 15:19:19.839334 26047 net.cpp:129] Top shape: 100 256 7 7 (1254400)
I1128 15:19:19.839349 26047 net.cpp:137] Memory required for data: 251822000
I1128 15:19:19.839352 26047 layer_factory.hpp:77] Creating layer Convolution8
I1128 15:19:19.839357 26047 net.cpp:84] Creating Layer Convolution8
I1128 15:19:19.839359 26047 net.cpp:406] Convolution8 <- Convolution7
I1128 15:19:19.839363 26047 net.cpp:380] Convolution8 -> Convolution8
I1128 15:19:20.004468 26047 net.cpp:122] Setting up Convolution8
I1128 15:19:20.004501 26047 net.cpp:129] Top shape: 100 256 4 4 (409600)
I1128 15:19:20.004503 26047 net.cpp:137] Memory required for data: 253460400
I1128 15:19:20.004513 26047 layer_factory.hpp:77] Creating layer BatchNorm8
I1128 15:19:20.004523 26047 net.cpp:84] Creating Layer BatchNorm8
I1128 15:19:20.004525 26047 net.cpp:406] BatchNorm8 <- Convolution8
I1128 15:19:20.004530 26047 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1128 15:19:20.005569 26047 net.cpp:122] Setting up BatchNorm8
I1128 15:19:20.005575 26047 net.cpp:129] Top shape: 100 256 4 4 (409600)
I1128 15:19:20.005591 26047 net.cpp:137] Memory required for data: 255098800
I1128 15:19:20.005595 26047 layer_factory.hpp:77] Creating layer Scale8
I1128 15:19:20.005604 26047 net.cpp:84] Creating Layer Scale8
I1128 15:19:20.005607 26047 net.cpp:406] Scale8 <- Convolution8
I1128 15:19:20.005610 26047 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1128 15:19:20.005760 26047 layer_factory.hpp:77] Creating layer Scale8
I1128 15:19:20.006289 26047 net.cpp:122] Setting up Scale8
I1128 15:19:20.006294 26047 net.cpp:129] Top shape: 100 256 4 4 (409600)
I1128 15:19:20.006310 26047 net.cpp:137] Memory required for data: 256737200
I1128 15:19:20.006314 26047 layer_factory.hpp:77] Creating layer ReLU8
I1128 15:19:20.006316 26047 net.cpp:84] Creating Layer ReLU8
I1128 15:19:20.006319 26047 net.cpp:406] ReLU8 <- Convolution8
I1128 15:19:20.006321 26047 net.cpp:367] ReLU8 -> Convolution8 (in-place)
I1128 15:19:20.006476 26047 net.cpp:122] Setting up ReLU8
I1128 15:19:20.006481 26047 net.cpp:129] Top shape: 100 256 4 4 (409600)
I1128 15:19:20.006497 26047 net.cpp:137] Memory required for data: 258375600
I1128 15:19:20.006500 26047 layer_factory.hpp:77] Creating layer Convolution9
I1128 15:19:20.006506 26047 net.cpp:84] Creating Layer Convolution9
I1128 15:19:20.006508 26047 net.cpp:406] Convolution9 <- Convolution8
I1128 15:19:20.006512 26047 net.cpp:380] Convolution9 -> Convolution9
I1128 15:19:20.009290 26047 net.cpp:122] Setting up Convolution9
I1128 15:19:20.009313 26047 net.cpp:129] Top shape: 100 512 4 4 (819200)
I1128 15:19:20.009315 26047 net.cpp:137] Memory required for data: 261652400
I1128 15:19:20.009320 26047 layer_factory.hpp:77] Creating layer BatchNorm9
I1128 15:19:20.009325 26047 net.cpp:84] Creating Layer BatchNorm9
I1128 15:19:20.009325 26047 net.cpp:406] BatchNorm9 <- Convolution9
I1128 15:19:20.009330 26047 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1128 15:19:20.010275 26047 net.cpp:122] Setting up BatchNorm9
I1128 15:19:20.010280 26047 net.cpp:129] Top shape: 100 512 4 4 (819200)
I1128 15:19:20.010296 26047 net.cpp:137] Memory required for data: 264929200
I1128 15:19:20.010310 26047 layer_factory.hpp:77] Creating layer Scale9
I1128 15:19:20.010314 26047 net.cpp:84] Creating Layer Scale9
I1128 15:19:20.010316 26047 net.cpp:406] Scale9 <- Convolution9
I1128 15:19:20.010319 26047 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1128 15:19:20.010465 26047 layer_factory.hpp:77] Creating layer Scale9
I1128 15:19:20.010999 26047 net.cpp:122] Setting up Scale9
I1128 15:19:20.011004 26047 net.cpp:129] Top shape: 100 512 4 4 (819200)
I1128 15:19:20.011020 26047 net.cpp:137] Memory required for data: 268206000
I1128 15:19:20.011023 26047 layer_factory.hpp:77] Creating layer ReLU9
I1128 15:19:20.011027 26047 net.cpp:84] Creating Layer ReLU9
I1128 15:19:20.011029 26047 net.cpp:406] ReLU9 <- Convolution9
I1128 15:19:20.011031 26047 net.cpp:367] ReLU9 -> Convolution9 (in-place)
I1128 15:19:20.011180 26047 net.cpp:122] Setting up ReLU9
I1128 15:19:20.011186 26047 net.cpp:129] Top shape: 100 512 4 4 (819200)
I1128 15:19:20.011203 26047 net.cpp:137] Memory required for data: 271482800
I1128 15:19:20.011204 26047 layer_factory.hpp:77] Creating layer Convolution10
I1128 15:19:20.011209 26047 net.cpp:84] Creating Layer Convolution10
I1128 15:19:20.011211 26047 net.cpp:406] Convolution10 <- Convolution9
I1128 15:19:20.011216 26047 net.cpp:380] Convolution10 -> Convolution10
I1128 15:19:20.359854 26047 net.cpp:122] Setting up Convolution10
I1128 15:19:20.359887 26047 net.cpp:129] Top shape: 100 512 2 2 (204800)
I1128 15:19:20.359889 26047 net.cpp:137] Memory required for data: 272302000
I1128 15:19:20.359907 26047 layer_factory.hpp:77] Creating layer BatchNorm10
I1128 15:19:20.359915 26047 net.cpp:84] Creating Layer BatchNorm10
I1128 15:19:20.359918 26047 net.cpp:406] BatchNorm10 <- Convolution10
I1128 15:19:20.359922 26047 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1128 15:19:20.361189 26047 net.cpp:122] Setting up BatchNorm10
I1128 15:19:20.361196 26047 net.cpp:129] Top shape: 100 512 2 2 (204800)
I1128 15:19:20.361212 26047 net.cpp:137] Memory required for data: 273121200
I1128 15:19:20.361217 26047 layer_factory.hpp:77] Creating layer Scale10
I1128 15:19:20.361220 26047 net.cpp:84] Creating Layer Scale10
I1128 15:19:20.361222 26047 net.cpp:406] Scale10 <- Convolution10
I1128 15:19:20.361225 26047 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1128 15:19:20.361416 26047 layer_factory.hpp:77] Creating layer Scale10
I1128 15:19:20.362116 26047 net.cpp:122] Setting up Scale10
I1128 15:19:20.362123 26047 net.cpp:129] Top shape: 100 512 2 2 (204800)
I1128 15:19:20.362138 26047 net.cpp:137] Memory required for data: 273940400
I1128 15:19:20.362141 26047 layer_factory.hpp:77] Creating layer ReLU10
I1128 15:19:20.362146 26047 net.cpp:84] Creating Layer ReLU10
I1128 15:19:20.362149 26047 net.cpp:406] ReLU10 <- Convolution10
I1128 15:19:20.362150 26047 net.cpp:367] ReLU10 -> Convolution10 (in-place)
I1128 15:19:20.362313 26047 net.cpp:122] Setting up ReLU10
I1128 15:19:20.362320 26047 net.cpp:129] Top shape: 100 512 2 2 (204800)
I1128 15:19:20.362336 26047 net.cpp:137] Memory required for data: 274759600
I1128 15:19:20.362339 26047 layer_factory.hpp:77] Creating layer Convolution11
I1128 15:19:20.362344 26047 net.cpp:84] Creating Layer Convolution11
I1128 15:19:20.362346 26047 net.cpp:406] Convolution11 <- Convolution10
I1128 15:19:20.362350 26047 net.cpp:380] Convolution11 -> Convolution11
I1128 15:19:20.368713 26047 net.cpp:122] Setting up Convolution11
I1128 15:19:20.368741 26047 net.cpp:129] Top shape: 100 1024 2 2 (409600)
I1128 15:19:20.368743 26047 net.cpp:137] Memory required for data: 276398000
I1128 15:19:20.368749 26047 layer_factory.hpp:77] Creating layer BatchNorm11
I1128 15:19:20.368754 26047 net.cpp:84] Creating Layer BatchNorm11
I1128 15:19:20.368757 26047 net.cpp:406] BatchNorm11 <- Convolution11
I1128 15:19:20.368760 26047 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1128 15:19:20.370092 26047 net.cpp:122] Setting up BatchNorm11
I1128 15:19:20.370100 26047 net.cpp:129] Top shape: 100 1024 2 2 (409600)
I1128 15:19:20.370129 26047 net.cpp:137] Memory required for data: 278036400
I1128 15:19:20.370136 26047 layer_factory.hpp:77] Creating layer Scale11
I1128 15:19:20.370138 26047 net.cpp:84] Creating Layer Scale11
I1128 15:19:20.370141 26047 net.cpp:406] Scale11 <- Convolution11
I1128 15:19:20.370143 26047 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1128 15:19:20.370365 26047 layer_factory.hpp:77] Creating layer Scale11
I1128 15:19:20.371292 26047 net.cpp:122] Setting up Scale11
I1128 15:19:20.371305 26047 net.cpp:129] Top shape: 100 1024 2 2 (409600)
I1128 15:19:20.371309 26047 net.cpp:137] Memory required for data: 279674800
I1128 15:19:20.371315 26047 layer_factory.hpp:77] Creating layer ReLU11
I1128 15:19:20.371325 26047 net.cpp:84] Creating Layer ReLU11
I1128 15:19:20.371328 26047 net.cpp:406] ReLU11 <- Convolution11
I1128 15:19:20.371348 26047 net.cpp:367] ReLU11 -> Convolution11 (in-place)
I1128 15:19:20.372375 26047 net.cpp:122] Setting up ReLU11
I1128 15:19:20.372398 26047 net.cpp:129] Top shape: 100 1024 2 2 (409600)
I1128 15:19:20.372400 26047 net.cpp:137] Memory required for data: 281313200
I1128 15:19:20.372403 26047 layer_factory.hpp:77] Creating layer Pooling1
I1128 15:19:20.372407 26047 net.cpp:84] Creating Layer Pooling1
I1128 15:19:20.372409 26047 net.cpp:406] Pooling1 <- Convolution11
I1128 15:19:20.372413 26047 net.cpp:380] Pooling1 -> Pooling1
I1128 15:19:20.372663 26047 net.cpp:122] Setting up Pooling1
I1128 15:19:20.372671 26047 net.cpp:129] Top shape: 100 1024 1 1 (102400)
I1128 15:19:20.372674 26047 net.cpp:137] Memory required for data: 281722800
I1128 15:19:20.372690 26047 layer_factory.hpp:77] Creating layer InnerProduct1
I1128 15:19:20.372696 26047 net.cpp:84] Creating Layer InnerProduct1
I1128 15:19:20.372700 26047 net.cpp:406] InnerProduct1 <- Pooling1
I1128 15:19:20.372707 26047 net.cpp:380] InnerProduct1 -> InnerProduct1
I1128 15:19:20.373389 26047 net.cpp:122] Setting up InnerProduct1
I1128 15:19:20.373394 26047 net.cpp:129] Top shape: 100 10 (1000)
I1128 15:19:20.373396 26047 net.cpp:137] Memory required for data: 281726800
I1128 15:19:20.373400 26047 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I1128 15:19:20.373406 26047 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
I1128 15:19:20.373410 26047 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I1128 15:19:20.373416 26047 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I1128 15:19:20.373423 26047 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I1128 15:19:20.373605 26047 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
I1128 15:19:20.373611 26047 net.cpp:129] Top shape: 100 10 (1000)
I1128 15:19:20.373613 26047 net.cpp:129] Top shape: 100 10 (1000)
I1128 15:19:20.373615 26047 net.cpp:137] Memory required for data: 281734800
I1128 15:19:20.373616 26047 layer_factory.hpp:77] Creating layer Accuracy1
I1128 15:19:20.373620 26047 net.cpp:84] Creating Layer Accuracy1
I1128 15:19:20.373625 26047 net.cpp:406] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_0
I1128 15:19:20.373626 26047 net.cpp:406] Accuracy1 <- Data2_Data1_1_split_0
I1128 15:19:20.373631 26047 net.cpp:380] Accuracy1 -> Accuracy1
I1128 15:19:20.373636 26047 net.cpp:122] Setting up Accuracy1
I1128 15:19:20.373639 26047 net.cpp:129] Top shape: (1)
I1128 15:19:20.373641 26047 net.cpp:137] Memory required for data: 281734804
I1128 15:19:20.373642 26047 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1128 15:19:20.373646 26047 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1128 15:19:20.373663 26047 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_1
I1128 15:19:20.373668 26047 net.cpp:406] SoftmaxWithLoss1 <- Data2_Data1_1_split_1
I1128 15:19:20.373677 26047 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1128 15:19:20.373685 26047 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1128 15:19:20.374408 26047 net.cpp:122] Setting up SoftmaxWithLoss1
I1128 15:19:20.374416 26047 net.cpp:129] Top shape: (1)
I1128 15:19:20.374428 26047 net.cpp:132]     with loss weight 1
I1128 15:19:20.374435 26047 net.cpp:137] Memory required for data: 281734808
I1128 15:19:20.374438 26047 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1128 15:19:20.374440 26047 net.cpp:200] Accuracy1 does not need backward computation.
I1128 15:19:20.374444 26047 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
I1128 15:19:20.374446 26047 net.cpp:198] InnerProduct1 needs backward computation.
I1128 15:19:20.374449 26047 net.cpp:198] Pooling1 needs backward computation.
I1128 15:19:20.374451 26047 net.cpp:198] ReLU11 needs backward computation.
I1128 15:19:20.374469 26047 net.cpp:198] Scale11 needs backward computation.
I1128 15:19:20.374474 26047 net.cpp:198] BatchNorm11 needs backward computation.
I1128 15:19:20.374476 26047 net.cpp:198] Convolution11 needs backward computation.
I1128 15:19:20.374480 26047 net.cpp:198] ReLU10 needs backward computation.
I1128 15:19:20.374482 26047 net.cpp:198] Scale10 needs backward computation.
I1128 15:19:20.374486 26047 net.cpp:198] BatchNorm10 needs backward computation.
I1128 15:19:20.374503 26047 net.cpp:198] Convolution10 needs backward computation.
I1128 15:19:20.374507 26047 net.cpp:198] ReLU9 needs backward computation.
I1128 15:19:20.374511 26047 net.cpp:198] Scale9 needs backward computation.
I1128 15:19:20.374528 26047 net.cpp:198] BatchNorm9 needs backward computation.
I1128 15:19:20.374531 26047 net.cpp:198] Convolution9 needs backward computation.
I1128 15:19:20.374536 26047 net.cpp:198] ReLU8 needs backward computation.
I1128 15:19:20.374539 26047 net.cpp:198] Scale8 needs backward computation.
I1128 15:19:20.374557 26047 net.cpp:198] BatchNorm8 needs backward computation.
I1128 15:19:20.374559 26047 net.cpp:198] Convolution8 needs backward computation.
I1128 15:19:20.374563 26047 net.cpp:198] ReLU7 needs backward computation.
I1128 15:19:20.374578 26047 net.cpp:198] Scale7 needs backward computation.
I1128 15:19:20.374583 26047 net.cpp:198] BatchNorm7 needs backward computation.
I1128 15:19:20.374588 26047 net.cpp:198] Convolution7 needs backward computation.
I1128 15:19:20.374590 26047 net.cpp:198] ReLU6 needs backward computation.
I1128 15:19:20.374593 26047 net.cpp:198] Scale6 needs backward computation.
I1128 15:19:20.374610 26047 net.cpp:198] BatchNorm6 needs backward computation.
I1128 15:19:20.374614 26047 net.cpp:198] Convolution6 needs backward computation.
I1128 15:19:20.374617 26047 net.cpp:198] ReLU5 needs backward computation.
I1128 15:19:20.374634 26047 net.cpp:198] Scale5 needs backward computation.
I1128 15:19:20.374639 26047 net.cpp:198] BatchNorm5 needs backward computation.
I1128 15:19:20.374640 26047 net.cpp:198] Convolution5 needs backward computation.
I1128 15:19:20.374644 26047 net.cpp:198] ReLU4 needs backward computation.
I1128 15:19:20.374646 26047 net.cpp:198] Scale4 needs backward computation.
I1128 15:19:20.374650 26047 net.cpp:198] BatchNorm4 needs backward computation.
I1128 15:19:20.374653 26047 net.cpp:198] Convolution4 needs backward computation.
I1128 15:19:20.374656 26047 net.cpp:198] ReLU3 needs backward computation.
I1128 15:19:20.374660 26047 net.cpp:198] Scale3 needs backward computation.
I1128 15:19:20.374665 26047 net.cpp:198] BatchNorm3 needs backward computation.
I1128 15:19:20.374668 26047 net.cpp:198] Convolution3 needs backward computation.
I1128 15:19:20.374671 26047 net.cpp:198] ReLU2 needs backward computation.
I1128 15:19:20.374675 26047 net.cpp:198] Scale2 needs backward computation.
I1128 15:19:20.374680 26047 net.cpp:198] BatchNorm2 needs backward computation.
I1128 15:19:20.374682 26047 net.cpp:198] Convolution2 needs backward computation.
I1128 15:19:20.374686 26047 net.cpp:198] ReLU1 needs backward computation.
I1128 15:19:20.374689 26047 net.cpp:198] Scale1 needs backward computation.
I1128 15:19:20.374693 26047 net.cpp:198] BatchNorm1 needs backward computation.
I1128 15:19:20.374696 26047 net.cpp:198] Convolution1 needs backward computation.
I1128 15:19:20.374701 26047 net.cpp:200] Data2_Data1_1_split does not need backward computation.
I1128 15:19:20.374711 26047 net.cpp:200] Data1 does not need backward computation.
I1128 15:19:20.374716 26047 net.cpp:242] This network produces output Accuracy1
I1128 15:19:20.374719 26047 net.cpp:242] This network produces output SoftmaxWithLoss1
I1128 15:19:20.374742 26047 net.cpp:255] Network initialization done.
I1128 15:19:20.374810 26047 solver.cpp:56] Solver scaffolding done.
I1128 15:19:20.395262 26047 caffe.cpp:248] Starting Optimization
I1128 15:19:20.395275 26047 solver.cpp:272] Solving 
I1128 15:19:20.395292 26047 solver.cpp:273] Learning Rate Policy: multistep
I1128 15:19:20.397137 26047 solver.cpp:330] Iteration 0, Testing net (#0)
I1128 15:19:22.577433 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:19:22.668388 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.0929
I1128 15:19:22.668411 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 79.223 (* 1 = 79.223 loss)
I1128 15:19:22.790478 26047 solver.cpp:218] Iteration 0 (-5.6696e+22 iter/s, 2.39505s/200 iters), loss = 2.35655
I1128 15:19:22.790520 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.35655 (* 1 = 2.35655 loss)
I1128 15:19:22.790532 26047 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1128 15:19:46.126284 26047 solver.cpp:218] Iteration 200 (8.57074 iter/s, 23.3352s/200 iters), loss = 1.46625
I1128 15:19:46.126310 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.46625 (* 1 = 1.46625 loss)
I1128 15:19:46.126317 26047 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1128 15:20:07.876147 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:20:09.517005 26047 solver.cpp:218] Iteration 400 (8.55062 iter/s, 23.3901s/200 iters), loss = 1.19891
I1128 15:20:09.517047 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.19891 (* 1 = 1.19891 loss)
I1128 15:20:09.517052 26047 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1128 15:20:32.952793 26047 solver.cpp:218] Iteration 600 (8.53419 iter/s, 23.4351s/200 iters), loss = 1.23816
I1128 15:20:32.952837 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.23816 (* 1 = 1.23816 loss)
I1128 15:20:32.952842 26047 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1128 15:20:53.677449 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:20:56.373065 26047 solver.cpp:218] Iteration 800 (8.53984 iter/s, 23.4196s/200 iters), loss = 0.953206
I1128 15:20:56.373107 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.953206 (* 1 = 0.953206 loss)
I1128 15:20:56.373113 26047 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1128 15:21:19.707918 26047 solver.cpp:330] Iteration 1000, Testing net (#0)
I1128 15:21:21.859483 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:21:21.949766 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5769
I1128 15:21:21.949790 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.33561 (* 1 = 1.33561 loss)
I1128 15:21:22.069553 26047 solver.cpp:218] Iteration 1000 (7.78338 iter/s, 25.6958s/200 iters), loss = 0.983042
I1128 15:21:22.069586 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.983042 (* 1 = 0.983042 loss)
I1128 15:21:22.069608 26047 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1128 15:21:41.702023 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:21:45.581151 26047 solver.cpp:218] Iteration 1200 (8.50667 iter/s, 23.511s/200 iters), loss = 0.628958
I1128 15:21:45.581183 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.628958 (* 1 = 0.628958 loss)
I1128 15:21:45.581204 26047 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1128 15:22:09.089067 26047 solver.cpp:218] Iteration 1400 (8.508 iter/s, 23.5073s/200 iters), loss = 0.82138
I1128 15:22:09.089098 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.82138 (* 1 = 0.82138 loss)
I1128 15:22:09.089105 26047 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1128 15:22:27.670904 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:22:32.611783 26047 solver.cpp:218] Iteration 1600 (8.50265 iter/s, 23.5221s/200 iters), loss = 0.938814
I1128 15:22:32.611814 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.938814 (* 1 = 0.938814 loss)
I1128 15:22:32.611835 26047 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1128 15:22:56.121714 26047 solver.cpp:218] Iteration 1800 (8.50728 iter/s, 23.5093s/200 iters), loss = 0.747975
I1128 15:22:56.121744 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.747975 (* 1 = 0.747975 loss)
I1128 15:22:56.121765 26047 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1128 15:23:13.648627 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:23:19.532315 26047 solver.cpp:330] Iteration 2000, Testing net (#0)
I1128 15:23:21.678203 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:23:21.769037 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.609
I1128 15:23:21.769059 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.08534 (* 1 = 1.08534 loss)
I1128 15:23:21.889508 26047 solver.cpp:218] Iteration 2000 (7.76184 iter/s, 25.7671s/200 iters), loss = 0.797004
I1128 15:23:21.889544 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.797004 (* 1 = 0.797004 loss)
I1128 15:23:21.889550 26047 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1128 15:23:45.395225 26047 solver.cpp:218] Iteration 2200 (8.50881 iter/s, 23.5051s/200 iters), loss = 0.619546
I1128 15:23:45.395284 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.619546 (* 1 = 0.619546 loss)
I1128 15:23:45.395292 26047 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1128 15:24:01.756947 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:24:08.932909 26047 solver.cpp:218] Iteration 2400 (8.49726 iter/s, 23.537s/200 iters), loss = 0.57716
I1128 15:24:08.932942 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.57716 (* 1 = 0.57716 loss)
I1128 15:24:08.932948 26047 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1128 15:24:20.572536 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_2500.caffemodel
I1128 15:24:20.598878 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_2500.solverstate
I1128 15:24:32.492892 26047 solver.cpp:218] Iteration 2600 (8.48921 iter/s, 23.5593s/200 iters), loss = 0.734613
I1128 15:24:32.492923 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.734613 (* 1 = 0.734613 loss)
I1128 15:24:32.492929 26047 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1128 15:24:47.792024 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:24:56.028527 26047 solver.cpp:218] Iteration 2800 (8.49799 iter/s, 23.535s/200 iters), loss = 0.650784
I1128 15:24:56.028622 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.650784 (* 1 = 0.650784 loss)
I1128 15:24:56.028645 26047 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1128 15:25:19.424167 26047 solver.cpp:330] Iteration 3000, Testing net (#0)
I1128 15:25:21.570292 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:25:21.661284 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4187
I1128 15:25:21.661306 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.73971 (* 1 = 1.73971 loss)
I1128 15:25:21.780958 26047 solver.cpp:218] Iteration 3000 (7.76649 iter/s, 25.7517s/200 iters), loss = 0.596888
I1128 15:25:21.780988 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.596888 (* 1 = 0.596888 loss)
I1128 15:25:21.781008 26047 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1128 15:25:35.883815 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:25:45.291821 26047 solver.cpp:218] Iteration 3200 (8.50695 iter/s, 23.5102s/200 iters), loss = 0.531598
I1128 15:25:45.291849 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.531598 (* 1 = 0.531598 loss)
I1128 15:25:45.291856 26047 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1128 15:26:08.786309 26047 solver.cpp:218] Iteration 3400 (8.51288 iter/s, 23.4938s/200 iters), loss = 0.606445
I1128 15:26:08.786422 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.606445 (* 1 = 0.606445 loss)
I1128 15:26:08.786443 26047 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1128 15:26:21.832592 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:26:32.297160 26047 solver.cpp:218] Iteration 3600 (8.50698 iter/s, 23.5101s/200 iters), loss = 0.54107
I1128 15:26:32.297190 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.54107 (* 1 = 0.54107 loss)
I1128 15:26:32.297195 26047 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1128 15:26:55.778218 26047 solver.cpp:218] Iteration 3800 (8.51775 iter/s, 23.4804s/200 iters), loss = 0.550816
I1128 15:26:55.778337 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.550816 (* 1 = 0.550816 loss)
I1128 15:26:55.778342 26047 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1128 15:27:07.778033 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:27:19.188933 26047 solver.cpp:330] Iteration 4000, Testing net (#0)
I1128 15:27:21.336225 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:27:21.428400 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.3364
I1128 15:27:21.428428 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.23225 (* 1 = 2.23225 loss)
I1128 15:27:21.548508 26047 solver.cpp:218] Iteration 4000 (7.76112 iter/s, 25.7695s/200 iters), loss = 0.541135
I1128 15:27:21.548540 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.541135 (* 1 = 0.541135 loss)
I1128 15:27:21.548563 26047 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1128 15:27:45.061151 26047 solver.cpp:218] Iteration 4200 (8.50631 iter/s, 23.512s/200 iters), loss = 0.475222
I1128 15:27:45.061275 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.475222 (* 1 = 0.475222 loss)
I1128 15:27:45.061286 26047 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1128 15:27:55.892776 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:28:08.601692 26047 solver.cpp:218] Iteration 4400 (8.49626 iter/s, 23.5398s/200 iters), loss = 0.469271
I1128 15:28:08.601723 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.469271 (* 1 = 0.469271 loss)
I1128 15:28:08.601744 26047 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1128 15:28:32.128552 26047 solver.cpp:218] Iteration 4600 (8.50117 iter/s, 23.5262s/200 iters), loss = 0.37477
I1128 15:28:32.128689 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.37477 (* 1 = 0.37477 loss)
I1128 15:28:32.128697 26047 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1128 15:28:41.903513 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:28:55.662502 26047 solver.cpp:218] Iteration 4800 (8.49864 iter/s, 23.5332s/200 iters), loss = 0.513194
I1128 15:28:55.662533 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.513194 (* 1 = 0.513194 loss)
I1128 15:28:55.662539 26047 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1128 15:29:19.067806 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_5000.caffemodel
I1128 15:29:19.074472 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_5000.solverstate
I1128 15:29:19.077693 26047 solver.cpp:330] Iteration 5000, Testing net (#0)
I1128 15:29:21.224437 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:29:21.315388 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.32
I1128 15:29:21.315410 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.07564 (* 1 = 2.07564 loss)
I1128 15:29:21.435504 26047 solver.cpp:218] Iteration 5000 (7.76029 iter/s, 25.7722s/200 iters), loss = 0.47614
I1128 15:29:21.435547 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.47614 (* 1 = 0.47614 loss)
I1128 15:29:21.435552 26047 sgd_solver.cpp:105] Iteration 5000, lr = 0.1
I1128 15:29:30.151573 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:29:44.969403 26047 solver.cpp:218] Iteration 5200 (8.49863 iter/s, 23.5332s/200 iters), loss = 0.516367
I1128 15:29:44.969444 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.516367 (* 1 = 0.516367 loss)
I1128 15:29:44.969449 26047 sgd_solver.cpp:105] Iteration 5200, lr = 0.1
I1128 15:30:08.473577 26047 solver.cpp:218] Iteration 5400 (8.50938 iter/s, 23.5035s/200 iters), loss = 0.389463
I1128 15:30:08.473755 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.389463 (* 1 = 0.389463 loss)
I1128 15:30:08.473762 26047 sgd_solver.cpp:105] Iteration 5400, lr = 0.1
I1128 15:30:16.003517 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:30:31.982686 26047 solver.cpp:218] Iteration 5600 (8.50764 iter/s, 23.5083s/200 iters), loss = 0.431497
I1128 15:30:31.982727 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.431497 (* 1 = 0.431497 loss)
I1128 15:30:31.982733 26047 sgd_solver.cpp:105] Iteration 5600, lr = 0.1
I1128 15:30:55.485782 26047 solver.cpp:218] Iteration 5800 (8.50977 iter/s, 23.5024s/200 iters), loss = 0.412118
I1128 15:30:55.485905 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.412118 (* 1 = 0.412118 loss)
I1128 15:30:55.485913 26047 sgd_solver.cpp:105] Iteration 5800, lr = 0.1
I1128 15:31:01.961366 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:31:18.880378 26047 solver.cpp:330] Iteration 6000, Testing net (#0)
I1128 15:31:21.025094 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:31:21.114648 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.4447
I1128 15:31:21.114670 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.73896 (* 1 = 1.73896 loss)
I1128 15:31:21.234992 26047 solver.cpp:218] Iteration 6000 (7.76749 iter/s, 25.7484s/200 iters), loss = 0.461701
I1128 15:31:21.235024 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.461701 (* 1 = 0.461701 loss)
I1128 15:31:21.235030 26047 sgd_solver.cpp:105] Iteration 6000, lr = 0.1
I1128 15:31:44.740216 26047 solver.cpp:218] Iteration 6200 (8.509 iter/s, 23.5045s/200 iters), loss = 0.432402
I1128 15:31:44.740352 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.432402 (* 1 = 0.432402 loss)
I1128 15:31:44.740373 26047 sgd_solver.cpp:105] Iteration 6200, lr = 0.1
I1128 15:31:50.035506 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:32:08.243201 26047 solver.cpp:218] Iteration 6400 (8.50985 iter/s, 23.5022s/200 iters), loss = 0.434197
I1128 15:32:08.243244 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.434197 (* 1 = 0.434197 loss)
I1128 15:32:08.243249 26047 sgd_solver.cpp:105] Iteration 6400, lr = 0.1
I1128 15:32:31.743353 26047 solver.cpp:218] Iteration 6600 (8.51084 iter/s, 23.4994s/200 iters), loss = 0.482127
I1128 15:32:31.743441 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.482127 (* 1 = 0.482127 loss)
I1128 15:32:31.743448 26047 sgd_solver.cpp:105] Iteration 6600, lr = 0.1
I1128 15:32:35.984131 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:32:55.256428 26047 solver.cpp:218] Iteration 6800 (8.50618 iter/s, 23.5123s/200 iters), loss = 0.487531
I1128 15:32:55.256458 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.487531 (* 1 = 0.487531 loss)
I1128 15:32:55.256464 26047 sgd_solver.cpp:105] Iteration 6800, lr = 0.1
I1128 15:33:18.646548 26047 solver.cpp:330] Iteration 7000, Testing net (#0)
I1128 15:33:20.793012 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:33:20.883316 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.3961
I1128 15:33:20.883339 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.28833 (* 1 = 2.28833 loss)
I1128 15:33:21.003589 26047 solver.cpp:218] Iteration 7000 (7.76808 iter/s, 25.7464s/200 iters), loss = 0.419355
I1128 15:33:21.003644 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.419355 (* 1 = 0.419355 loss)
I1128 15:33:21.003651 26047 sgd_solver.cpp:105] Iteration 7000, lr = 0.1
I1128 15:33:24.186173 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:33:44.504220 26047 solver.cpp:218] Iteration 7200 (8.51067 iter/s, 23.4999s/200 iters), loss = 0.450479
I1128 15:33:44.504250 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.450479 (* 1 = 0.450479 loss)
I1128 15:33:44.504254 26047 sgd_solver.cpp:105] Iteration 7200, lr = 0.1
I1128 15:34:07.999966 26047 solver.cpp:218] Iteration 7400 (8.51244 iter/s, 23.495s/200 iters), loss = 0.340055
I1128 15:34:08.000133 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.340055 (* 1 = 0.340055 loss)
I1128 15:34:08.000140 26047 sgd_solver.cpp:105] Iteration 7400, lr = 0.1
I1128 15:34:10.002358 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:34:19.638674 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_7500.caffemodel
I1128 15:34:19.645145 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_7500.solverstate
I1128 15:34:31.507786 26047 solver.cpp:218] Iteration 7600 (8.50811 iter/s, 23.507s/200 iters), loss = 0.524397
I1128 15:34:31.507815 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.524397 (* 1 = 0.524397 loss)
I1128 15:34:31.507820 26047 sgd_solver.cpp:105] Iteration 7600, lr = 0.1
I1128 15:34:54.952215 26047 solver.cpp:218] Iteration 7800 (8.53107 iter/s, 23.4437s/200 iters), loss = 0.376337
I1128 15:34:54.952368 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.376337 (* 1 = 0.376337 loss)
I1128 15:34:54.952374 26047 sgd_solver.cpp:105] Iteration 7800, lr = 0.1
I1128 15:34:55.895238 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:35:18.306824 26047 solver.cpp:330] Iteration 8000, Testing net (#0)
I1128 15:35:20.464746 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:35:20.555981 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6777
I1128 15:35:20.556005 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.895343 (* 1 = 0.895343 loss)
I1128 15:35:20.675977 26047 solver.cpp:218] Iteration 8000 (7.77518 iter/s, 25.7229s/200 iters), loss = 0.344637
I1128 15:35:20.676009 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.344637 (* 1 = 0.344637 loss)
I1128 15:35:20.676017 26047 sgd_solver.cpp:105] Iteration 8000, lr = 0.1
I1128 15:35:44.028044 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:35:44.145539 26047 solver.cpp:218] Iteration 8200 (8.52194 iter/s, 23.4688s/200 iters), loss = 0.348363
I1128 15:35:44.145568 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.348363 (* 1 = 0.348363 loss)
I1128 15:35:44.145576 26047 sgd_solver.cpp:105] Iteration 8200, lr = 0.1
I1128 15:36:07.611162 26047 solver.cpp:218] Iteration 8400 (8.52337 iter/s, 23.4649s/200 iters), loss = 0.457766
I1128 15:36:07.611193 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.457766 (* 1 = 0.457766 loss)
I1128 15:36:07.611215 26047 sgd_solver.cpp:105] Iteration 8400, lr = 0.1
I1128 15:36:29.801180 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:36:31.092648 26047 solver.cpp:218] Iteration 8600 (8.51761 iter/s, 23.4808s/200 iters), loss = 0.443938
I1128 15:36:31.092679 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.443938 (* 1 = 0.443938 loss)
I1128 15:36:31.092700 26047 sgd_solver.cpp:105] Iteration 8600, lr = 0.1
I1128 15:36:54.560950 26047 solver.cpp:218] Iteration 8800 (8.52239 iter/s, 23.4676s/200 iters), loss = 0.306602
I1128 15:36:54.560981 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.306602 (* 1 = 0.306602 loss)
I1128 15:36:54.560988 26047 sgd_solver.cpp:105] Iteration 8800, lr = 0.1
I1128 15:37:15.688910 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:37:17.922782 26047 solver.cpp:330] Iteration 9000, Testing net (#0)
I1128 15:37:20.101884 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:37:20.194814 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6492
I1128 15:37:20.194836 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.04455 (* 1 = 1.04455 loss)
I1128 15:37:20.314653 26047 solver.cpp:218] Iteration 9000 (7.76611 iter/s, 25.7529s/200 iters), loss = 0.44404
I1128 15:37:20.314685 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.44404 (* 1 = 0.44404 loss)
I1128 15:37:20.314708 26047 sgd_solver.cpp:105] Iteration 9000, lr = 0.1
I1128 15:37:43.786435 26047 solver.cpp:218] Iteration 9200 (8.52113 iter/s, 23.4711s/200 iters), loss = 0.320915
I1128 15:37:43.786468 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.320915 (* 1 = 0.320915 loss)
I1128 15:37:43.786475 26047 sgd_solver.cpp:105] Iteration 9200, lr = 0.1
I1128 15:38:03.756634 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:38:07.276901 26047 solver.cpp:218] Iteration 9400 (8.51435 iter/s, 23.4897s/200 iters), loss = 0.309868
I1128 15:38:07.276931 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.309868 (* 1 = 0.309868 loss)
I1128 15:38:07.276935 26047 sgd_solver.cpp:105] Iteration 9400, lr = 0.1
I1128 15:38:30.752849 26047 solver.cpp:218] Iteration 9600 (8.51962 iter/s, 23.4752s/200 iters), loss = 0.397226
I1128 15:38:30.752878 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.397226 (* 1 = 0.397226 loss)
I1128 15:38:30.752883 26047 sgd_solver.cpp:105] Iteration 9600, lr = 0.1
I1128 15:38:49.652942 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:38:54.236444 26047 solver.cpp:218] Iteration 9800 (8.51684 iter/s, 23.4829s/200 iters), loss = 0.353904
I1128 15:38:54.236474 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.353904 (* 1 = 0.353904 loss)
I1128 15:38:54.236479 26047 sgd_solver.cpp:105] Iteration 9800, lr = 0.1
I1128 15:39:17.582216 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_10000.caffemodel
I1128 15:39:17.589085 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_10000.solverstate
I1128 15:39:17.592273 26047 solver.cpp:330] Iteration 10000, Testing net (#0)
I1128 15:39:19.764263 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:39:19.856225 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6879
I1128 15:39:19.856248 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.927357 (* 1 = 0.927357 loss)
I1128 15:39:19.975708 26047 solver.cpp:218] Iteration 10000 (7.77047 iter/s, 25.7385s/200 iters), loss = 0.351551
I1128 15:39:19.975752 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.351551 (* 1 = 0.351551 loss)
I1128 15:39:19.975759 26047 sgd_solver.cpp:105] Iteration 10000, lr = 0.1
I1128 15:39:37.814213 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:39:43.451768 26047 solver.cpp:218] Iteration 10200 (8.51958 iter/s, 23.4753s/200 iters), loss = 0.387876
I1128 15:39:43.451797 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.387876 (* 1 = 0.387876 loss)
I1128 15:39:43.451802 26047 sgd_solver.cpp:105] Iteration 10200, lr = 0.1
I1128 15:40:06.918470 26047 solver.cpp:218] Iteration 10400 (8.52298 iter/s, 23.466s/200 iters), loss = 0.295192
I1128 15:40:06.918658 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.295192 (* 1 = 0.295192 loss)
I1128 15:40:06.918665 26047 sgd_solver.cpp:105] Iteration 10400, lr = 0.1
I1128 15:40:23.593461 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:40:30.402791 26047 solver.cpp:218] Iteration 10600 (8.51664 iter/s, 23.4834s/200 iters), loss = 0.308959
I1128 15:40:30.402817 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.308959 (* 1 = 0.308959 loss)
I1128 15:40:30.402823 26047 sgd_solver.cpp:105] Iteration 10600, lr = 0.1
I1128 15:40:53.870673 26047 solver.cpp:218] Iteration 10800 (8.52255 iter/s, 23.4672s/200 iters), loss = 0.305271
I1128 15:40:53.870795 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.305271 (* 1 = 0.305271 loss)
I1128 15:40:53.870801 26047 sgd_solver.cpp:105] Iteration 10800, lr = 0.1
I1128 15:41:09.489923 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:41:17.240109 26047 solver.cpp:330] Iteration 11000, Testing net (#0)
I1128 15:41:19.411092 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:41:19.501402 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7584
I1128 15:41:19.501423 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.682776 (* 1 = 0.682776 loss)
I1128 15:41:19.621330 26047 solver.cpp:218] Iteration 11000 (7.76705 iter/s, 25.7498s/200 iters), loss = 0.394898
I1128 15:41:19.621359 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.394898 (* 1 = 0.394898 loss)
I1128 15:41:19.621379 26047 sgd_solver.cpp:105] Iteration 11000, lr = 0.1
I1128 15:41:43.088994 26047 solver.cpp:218] Iteration 11200 (8.52263 iter/s, 23.4669s/200 iters), loss = 0.381598
I1128 15:41:43.089176 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.381598 (* 1 = 0.381598 loss)
I1128 15:41:43.089184 26047 sgd_solver.cpp:105] Iteration 11200, lr = 0.1
I1128 15:41:57.653519 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:42:06.578724 26047 solver.cpp:218] Iteration 11400 (8.51467 iter/s, 23.4889s/200 iters), loss = 0.401052
I1128 15:42:06.578768 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.401052 (* 1 = 0.401052 loss)
I1128 15:42:06.578773 26047 sgd_solver.cpp:105] Iteration 11400, lr = 0.1
I1128 15:42:30.044493 26047 solver.cpp:218] Iteration 11600 (8.52332 iter/s, 23.465s/200 iters), loss = 0.330105
I1128 15:42:30.044637 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.330105 (* 1 = 0.330105 loss)
I1128 15:42:30.044646 26047 sgd_solver.cpp:105] Iteration 11600, lr = 0.1
I1128 15:42:43.437001 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:42:53.536056 26047 solver.cpp:218] Iteration 11800 (8.514 iter/s, 23.4907s/200 iters), loss = 0.391756
I1128 15:42:53.536099 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.391756 (* 1 = 0.391756 loss)
I1128 15:42:53.536104 26047 sgd_solver.cpp:105] Iteration 11800, lr = 0.1
I1128 15:43:16.879986 26047 solver.cpp:330] Iteration 12000, Testing net (#0)
I1128 15:43:19.043287 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:43:19.137073 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6375
I1128 15:43:19.137095 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.24324 (* 1 = 1.24324 loss)
I1128 15:43:19.257414 26047 solver.cpp:218] Iteration 12000 (7.77599 iter/s, 25.7202s/200 iters), loss = 0.378612
I1128 15:43:19.257443 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.378612 (* 1 = 0.378612 loss)
I1128 15:43:19.257448 26047 sgd_solver.cpp:105] Iteration 12000, lr = 0.1
I1128 15:43:31.574826 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:43:42.717629 26047 solver.cpp:218] Iteration 12200 (8.52622 iter/s, 23.4571s/200 iters), loss = 0.406035
I1128 15:43:42.717672 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.406035 (* 1 = 0.406035 loss)
I1128 15:43:42.717677 26047 sgd_solver.cpp:105] Iteration 12200, lr = 0.1
I1128 15:44:06.175226 26047 solver.cpp:218] Iteration 12400 (8.52713 iter/s, 23.4545s/200 iters), loss = 0.386089
I1128 15:44:06.175369 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.386089 (* 1 = 0.386089 loss)
I1128 15:44:06.175387 26047 sgd_solver.cpp:105] Iteration 12400, lr = 0.1
I1128 15:44:17.327035 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:44:17.797040 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_12500.caffemodel
I1128 15:44:17.803493 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_12500.solverstate
I1128 15:44:29.659122 26047 solver.cpp:218] Iteration 12600 (8.51759 iter/s, 23.4808s/200 iters), loss = 0.224839
I1128 15:44:29.659165 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.224839 (* 1 = 0.224839 loss)
I1128 15:44:29.659170 26047 sgd_solver.cpp:105] Iteration 12600, lr = 0.1
I1128 15:44:53.119369 26047 solver.cpp:218] Iteration 12800 (8.52611 iter/s, 23.4573s/200 iters), loss = 0.290052
I1128 15:44:53.119501 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.290052 (* 1 = 0.290052 loss)
I1128 15:44:53.119509 26047 sgd_solver.cpp:105] Iteration 12800, lr = 0.1
I1128 15:45:03.217937 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:45:16.457278 26047 solver.cpp:330] Iteration 13000, Testing net (#0)
I1128 15:45:18.623514 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:45:18.713572 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.73
I1128 15:45:18.713593 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.8343 (* 1 = 0.8343 loss)
I1128 15:45:18.834040 26047 solver.cpp:218] Iteration 13000 (7.77861 iter/s, 25.7115s/200 iters), loss = 0.334865
I1128 15:45:18.834072 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.334865 (* 1 = 0.334865 loss)
I1128 15:45:18.834077 26047 sgd_solver.cpp:105] Iteration 13000, lr = 0.1
I1128 15:45:42.274019 26047 solver.cpp:218] Iteration 13200 (8.53342 iter/s, 23.4373s/200 iters), loss = 0.346608
I1128 15:45:42.274075 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.346608 (* 1 = 0.346608 loss)
I1128 15:45:42.274081 26047 sgd_solver.cpp:105] Iteration 13200, lr = 0.1
I1128 15:45:51.309038 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:46:05.728744 26047 solver.cpp:218] Iteration 13400 (8.52804 iter/s, 23.4521s/200 iters), loss = 0.394667
I1128 15:46:05.728811 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.394667 (* 1 = 0.394667 loss)
I1128 15:46:05.728818 26047 sgd_solver.cpp:105] Iteration 13400, lr = 0.1
I1128 15:46:29.178894 26047 solver.cpp:218] Iteration 13600 (8.52967 iter/s, 23.4476s/200 iters), loss = 0.256808
I1128 15:46:29.179014 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.256808 (* 1 = 0.256808 loss)
I1128 15:46:29.179021 26047 sgd_solver.cpp:105] Iteration 13600, lr = 0.1
I1128 15:46:37.041502 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:46:52.629372 26047 solver.cpp:218] Iteration 13800 (8.52955 iter/s, 23.4479s/200 iters), loss = 0.275499
I1128 15:46:52.629402 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.275499 (* 1 = 0.275499 loss)
I1128 15:46:52.629408 26047 sgd_solver.cpp:105] Iteration 13800, lr = 0.1
I1128 15:47:15.967433 26047 solver.cpp:330] Iteration 14000, Testing net (#0)
I1128 15:47:18.119061 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:47:18.211839 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6323
I1128 15:47:18.211860 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.28585 (* 1 = 1.28585 loss)
I1128 15:47:18.331784 26047 solver.cpp:218] Iteration 14000 (7.78217 iter/s, 25.6998s/200 iters), loss = 0.278017
I1128 15:47:18.331814 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.278017 (* 1 = 0.278017 loss)
I1128 15:47:18.331835 26047 sgd_solver.cpp:105] Iteration 14000, lr = 0.1
I1128 15:47:25.138824 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:47:41.785085 26047 solver.cpp:218] Iteration 14200 (8.52844 iter/s, 23.4509s/200 iters), loss = 0.397271
I1128 15:47:41.785128 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.397271 (* 1 = 0.397271 loss)
I1128 15:47:41.785135 26047 sgd_solver.cpp:105] Iteration 14200, lr = 0.1
I1128 15:48:05.238267 26047 solver.cpp:218] Iteration 14400 (8.52847 iter/s, 23.4509s/200 iters), loss = 0.400287
I1128 15:48:05.238379 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.400287 (* 1 = 0.400287 loss)
I1128 15:48:05.238384 26047 sgd_solver.cpp:105] Iteration 14400, lr = 0.1
I1128 15:48:10.991544 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:48:28.697371 26047 solver.cpp:218] Iteration 14600 (8.52632 iter/s, 23.4568s/200 iters), loss = 0.303552
I1128 15:48:28.697414 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.303552 (* 1 = 0.303552 loss)
I1128 15:48:28.697420 26047 sgd_solver.cpp:105] Iteration 14600, lr = 0.1
I1128 15:48:52.151860 26047 solver.cpp:218] Iteration 14800 (8.52795 iter/s, 23.4523s/200 iters), loss = 0.265537
I1128 15:48:52.151962 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265537 (* 1 = 0.265537 loss)
I1128 15:48:52.151983 26047 sgd_solver.cpp:105] Iteration 14800, lr = 0.1
I1128 15:48:56.731588 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:49:15.495609 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_15000.caffemodel
I1128 15:49:15.502076 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_15000.solverstate
I1128 15:49:15.505219 26047 solver.cpp:330] Iteration 15000, Testing net (#0)
I1128 15:49:17.657127 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:49:17.748759 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6887
I1128 15:49:17.748792 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.984496 (* 1 = 0.984496 loss)
I1128 15:49:17.868558 26047 solver.cpp:218] Iteration 15000 (7.77777 iter/s, 25.7143s/200 iters), loss = 0.309577
I1128 15:49:17.868589 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.309577 (* 1 = 0.309577 loss)
I1128 15:49:17.868595 26047 sgd_solver.cpp:105] Iteration 15000, lr = 0.1
I1128 15:49:41.318708 26047 solver.cpp:218] Iteration 15200 (8.52948 iter/s, 23.4481s/200 iters), loss = 0.294643
I1128 15:49:41.318852 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.294643 (* 1 = 0.294643 loss)
I1128 15:49:41.318874 26047 sgd_solver.cpp:105] Iteration 15200, lr = 0.1
I1128 15:49:44.844068 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:50:04.780988 26047 solver.cpp:218] Iteration 15400 (8.5251 iter/s, 23.4601s/200 iters), loss = 0.372416
I1128 15:50:04.781018 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.372416 (* 1 = 0.372416 loss)
I1128 15:50:04.781023 26047 sgd_solver.cpp:105] Iteration 15400, lr = 0.1
I1128 15:50:28.227766 26047 solver.cpp:218] Iteration 15600 (8.53068 iter/s, 23.4448s/200 iters), loss = 0.253356
I1128 15:50:28.227890 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.253356 (* 1 = 0.253356 loss)
I1128 15:50:28.227896 26047 sgd_solver.cpp:105] Iteration 15600, lr = 0.1
I1128 15:50:30.577049 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:50:51.681495 26047 solver.cpp:218] Iteration 15800 (8.52816 iter/s, 23.4517s/200 iters), loss = 0.434142
I1128 15:50:51.681525 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.434142 (* 1 = 0.434142 loss)
I1128 15:50:51.681531 26047 sgd_solver.cpp:105] Iteration 15800, lr = 0.1
I1128 15:51:15.022255 26047 solver.cpp:330] Iteration 16000, Testing net (#0)
I1128 15:51:17.188302 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:51:17.280833 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7257
I1128 15:51:17.280855 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.895293 (* 1 = 0.895293 loss)
I1128 15:51:17.400640 26047 solver.cpp:218] Iteration 16000 (7.77694 iter/s, 25.7171s/200 iters), loss = 0.32046
I1128 15:51:17.400684 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.32046 (* 1 = 0.32046 loss)
I1128 15:51:17.400691 26047 sgd_solver.cpp:105] Iteration 16000, lr = 0.1
I1128 15:51:18.695374 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:51:40.853260 26047 solver.cpp:218] Iteration 16200 (8.52851 iter/s, 23.4508s/200 iters), loss = 0.305346
I1128 15:51:40.853288 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.305346 (* 1 = 0.305346 loss)
I1128 15:51:40.853294 26047 sgd_solver.cpp:105] Iteration 16200, lr = 0.1
I1128 15:52:04.305107 26047 solver.cpp:218] Iteration 16400 (8.52877 iter/s, 23.45s/200 iters), loss = 0.331588
I1128 15:52:04.305205 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.331588 (* 1 = 0.331588 loss)
I1128 15:52:04.305212 26047 sgd_solver.cpp:105] Iteration 16400, lr = 0.1
I1128 15:52:04.543170 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:52:27.755226 26047 solver.cpp:218] Iteration 16600 (8.52941 iter/s, 23.4483s/200 iters), loss = 0.317845
I1128 15:52:27.755269 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.317845 (* 1 = 0.317845 loss)
I1128 15:52:27.755275 26047 sgd_solver.cpp:105] Iteration 16600, lr = 0.1
I1128 15:52:50.269044 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:52:51.206653 26047 solver.cpp:218] Iteration 16800 (8.5289 iter/s, 23.4497s/200 iters), loss = 0.256214
I1128 15:52:51.206696 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.256214 (* 1 = 0.256214 loss)
I1128 15:52:51.206702 26047 sgd_solver.cpp:105] Iteration 16800, lr = 0.1
I1128 15:53:14.541831 26047 solver.cpp:330] Iteration 17000, Testing net (#0)
I1128 15:53:16.704481 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:53:16.796145 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7577
I1128 15:53:16.796169 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.796512 (* 1 = 0.796512 loss)
I1128 15:53:16.916046 26047 solver.cpp:218] Iteration 17000 (7.77983 iter/s, 25.7075s/200 iters), loss = 0.188598
I1128 15:53:16.916091 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.188598 (* 1 = 0.188598 loss)
I1128 15:53:16.916097 26047 sgd_solver.cpp:105] Iteration 17000, lr = 0.1
I1128 15:53:38.376679 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:53:40.372360 26047 solver.cpp:218] Iteration 17200 (8.5271 iter/s, 23.4546s/200 iters), loss = 0.299861
I1128 15:53:40.372390 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.299861 (* 1 = 0.299861 loss)
I1128 15:53:40.372395 26047 sgd_solver.cpp:105] Iteration 17200, lr = 0.1
I1128 15:54:03.823117 26047 solver.cpp:218] Iteration 17400 (8.52911 iter/s, 23.4491s/200 iters), loss = 0.31359
I1128 15:54:03.823148 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.31359 (* 1 = 0.31359 loss)
I1128 15:54:03.823153 26047 sgd_solver.cpp:105] Iteration 17400, lr = 0.1
I1128 15:54:15.443702 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_17500.caffemodel
I1128 15:54:15.450414 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_17500.solverstate
I1128 15:54:24.258098 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:54:27.307170 26047 solver.cpp:218] Iteration 17600 (8.51701 iter/s, 23.4824s/200 iters), loss = 0.252645
I1128 15:54:27.307199 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.252645 (* 1 = 0.252645 loss)
I1128 15:54:27.307219 26047 sgd_solver.cpp:105] Iteration 17600, lr = 0.1
I1128 15:54:50.755370 26047 solver.cpp:218] Iteration 17800 (8.53002 iter/s, 23.4466s/200 iters), loss = 0.330518
I1128 15:54:50.755478 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.330518 (* 1 = 0.330518 loss)
I1128 15:54:50.755484 26047 sgd_solver.cpp:105] Iteration 17800, lr = 0.1
I1128 15:55:09.991899 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:55:14.097149 26047 solver.cpp:330] Iteration 18000, Testing net (#0)
I1128 15:55:16.251571 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:55:16.342761 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.5347
I1128 15:55:16.342782 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.09678 (* 1 = 2.09678 loss)
I1128 15:55:16.461901 26047 solver.cpp:218] Iteration 18000 (7.78067 iter/s, 25.7047s/200 iters), loss = 0.257759
I1128 15:55:16.461931 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.257759 (* 1 = 0.257759 loss)
I1128 15:55:16.461951 26047 sgd_solver.cpp:105] Iteration 18000, lr = 0.1
I1128 15:55:39.904495 26047 solver.cpp:218] Iteration 18200 (8.53204 iter/s, 23.4411s/200 iters), loss = 0.174013
I1128 15:55:39.904589 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.174013 (* 1 = 0.174013 loss)
I1128 15:55:39.904595 26047 sgd_solver.cpp:105] Iteration 18200, lr = 0.1
I1128 15:55:58.081830 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:56:03.363215 26047 solver.cpp:218] Iteration 18400 (8.52619 iter/s, 23.4571s/200 iters), loss = 0.327143
I1128 15:56:03.363260 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.327143 (* 1 = 0.327143 loss)
I1128 15:56:03.363265 26047 sgd_solver.cpp:105] Iteration 18400, lr = 0.1
I1128 15:56:26.807538 26047 solver.cpp:218] Iteration 18600 (8.5314 iter/s, 23.4428s/200 iters), loss = 0.38472
I1128 15:56:26.807678 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.384719 (* 1 = 0.384719 loss)
I1128 15:56:26.807687 26047 sgd_solver.cpp:105] Iteration 18600, lr = 0.1
I1128 15:56:43.812146 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:56:50.266363 26047 solver.cpp:218] Iteration 18800 (8.52615 iter/s, 23.4572s/200 iters), loss = 0.267046
I1128 15:56:50.266391 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.267046 (* 1 = 0.267046 loss)
I1128 15:56:50.266396 26047 sgd_solver.cpp:105] Iteration 18800, lr = 0.1
I1128 15:57:13.592077 26047 solver.cpp:330] Iteration 19000, Testing net (#0)
I1128 15:57:15.745854 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:57:15.835860 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7824
I1128 15:57:15.835881 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.667718 (* 1 = 0.667718 loss)
I1128 15:57:15.956106 26047 solver.cpp:218] Iteration 19000 (7.78569 iter/s, 25.6882s/200 iters), loss = 0.26875
I1128 15:57:15.956135 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.26875 (* 1 = 0.26875 loss)
I1128 15:57:15.956141 26047 sgd_solver.cpp:105] Iteration 19000, lr = 0.1
I1128 15:57:31.906970 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:57:39.426723 26047 solver.cpp:218] Iteration 19200 (8.52181 iter/s, 23.4692s/200 iters), loss = 0.276523
I1128 15:57:39.426757 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.276523 (* 1 = 0.276523 loss)
I1128 15:57:39.426779 26047 sgd_solver.cpp:105] Iteration 19200, lr = 0.1
I1128 15:58:02.903786 26047 solver.cpp:218] Iteration 19400 (8.51947 iter/s, 23.4757s/200 iters), loss = 0.374204
I1128 15:58:02.903853 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.374204 (* 1 = 0.374204 loss)
I1128 15:58:02.903862 26047 sgd_solver.cpp:105] Iteration 19400, lr = 0.1
I1128 15:58:17.825481 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:58:26.402324 26047 solver.cpp:218] Iteration 19600 (8.51169 iter/s, 23.4971s/200 iters), loss = 0.222781
I1128 15:58:26.402355 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.222781 (* 1 = 0.222781 loss)
I1128 15:58:26.402362 26047 sgd_solver.cpp:105] Iteration 19600, lr = 0.1
I1128 15:58:49.873890 26047 solver.cpp:218] Iteration 19800 (8.52145 iter/s, 23.4702s/200 iters), loss = 0.272576
I1128 15:58:49.874048 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.272576 (* 1 = 0.272576 loss)
I1128 15:58:49.874058 26047 sgd_solver.cpp:105] Iteration 19800, lr = 0.1
I1128 15:59:03.615876 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:59:13.251878 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_20000.caffemodel
I1128 15:59:13.258364 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_20000.solverstate
I1128 15:59:13.261544 26047 solver.cpp:330] Iteration 20000, Testing net (#0)
I1128 15:59:15.425179 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:59:15.516865 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6454
I1128 15:59:15.516887 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.26779 (* 1 = 1.26779 loss)
I1128 15:59:15.636919 26047 solver.cpp:218] Iteration 20000 (7.76355 iter/s, 25.7614s/200 iters), loss = 0.209071
I1128 15:59:15.636953 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.209071 (* 1 = 0.209071 loss)
I1128 15:59:15.636961 26047 sgd_solver.cpp:105] Iteration 20000, lr = 0.1
I1128 15:59:39.115988 26047 solver.cpp:218] Iteration 20200 (8.51871 iter/s, 23.4777s/200 iters), loss = 0.290739
I1128 15:59:39.116075 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.290739 (* 1 = 0.290739 loss)
I1128 15:59:39.116083 26047 sgd_solver.cpp:105] Iteration 20200, lr = 0.1
I1128 15:59:51.808377 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:00:02.620813 26047 solver.cpp:218] Iteration 20400 (8.50939 iter/s, 23.5034s/200 iters), loss = 0.219521
I1128 16:00:02.620844 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.219521 (* 1 = 0.219521 loss)
I1128 16:00:02.620851 26047 sgd_solver.cpp:105] Iteration 20400, lr = 0.1
I1128 16:00:26.091094 26047 solver.cpp:218] Iteration 20600 (8.52189 iter/s, 23.469s/200 iters), loss = 0.357679
I1128 16:00:26.091176 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.357679 (* 1 = 0.357679 loss)
I1128 16:00:26.091197 26047 sgd_solver.cpp:105] Iteration 20600, lr = 0.1
I1128 16:00:37.724687 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:00:49.590368 26047 solver.cpp:218] Iteration 20800 (8.51139 iter/s, 23.4979s/200 iters), loss = 0.252462
I1128 16:00:49.590399 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.252462 (* 1 = 0.252462 loss)
I1128 16:00:49.590405 26047 sgd_solver.cpp:105] Iteration 20800, lr = 0.1
I1128 16:01:12.964865 26047 solver.cpp:330] Iteration 21000, Testing net (#0)
I1128 16:01:15.160506 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:01:15.250300 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7356
I1128 16:01:15.250321 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.874381 (* 1 = 0.874381 loss)
I1128 16:01:15.370313 26047 solver.cpp:218] Iteration 21000 (7.75839 iter/s, 25.7785s/200 iters), loss = 0.356574
I1128 16:01:15.370348 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.356574 (* 1 = 0.356574 loss)
I1128 16:01:15.370355 26047 sgd_solver.cpp:105] Iteration 21000, lr = 0.1
I1128 16:01:25.831130 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:01:38.870893 26047 solver.cpp:218] Iteration 21200 (8.51089 iter/s, 23.4993s/200 iters), loss = 0.376425
I1128 16:01:38.870924 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.376425 (* 1 = 0.376425 loss)
I1128 16:01:38.870931 26047 sgd_solver.cpp:105] Iteration 21200, lr = 0.1
I1128 16:02:02.349675 26047 solver.cpp:218] Iteration 21400 (8.51879 iter/s, 23.4775s/200 iters), loss = 0.226562
I1128 16:02:02.349778 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.226562 (* 1 = 0.226562 loss)
I1128 16:02:02.349802 26047 sgd_solver.cpp:105] Iteration 21400, lr = 0.1
I1128 16:02:11.755386 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:02:25.837185 26047 solver.cpp:218] Iteration 21600 (8.51564 iter/s, 23.4862s/200 iters), loss = 0.233478
I1128 16:02:25.837218 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.233478 (* 1 = 0.233478 loss)
I1128 16:02:25.837239 26047 sgd_solver.cpp:105] Iteration 21600, lr = 0.1
I1128 16:02:49.313720 26047 solver.cpp:218] Iteration 21800 (8.5196 iter/s, 23.4753s/200 iters), loss = 0.19588
I1128 16:02:49.313787 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.195879 (* 1 = 0.195879 loss)
I1128 16:02:49.313794 26047 sgd_solver.cpp:105] Iteration 21800, lr = 0.1
I1128 16:02:57.544816 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:03:12.689862 26047 solver.cpp:330] Iteration 22000, Testing net (#0)
I1128 16:03:14.855808 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:03:14.948500 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6528
I1128 16:03:14.948529 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.3771 (* 1 = 1.3771 loss)
I1128 16:03:15.068313 26047 solver.cpp:218] Iteration 22000 (7.76602 iter/s, 25.7532s/200 iters), loss = 0.282387
I1128 16:03:15.068348 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.282387 (* 1 = 0.282387 loss)
I1128 16:03:15.068370 26047 sgd_solver.cpp:105] Iteration 22000, lr = 0.1
I1128 16:03:38.544795 26047 solver.cpp:218] Iteration 22200 (8.51961 iter/s, 23.4753s/200 iters), loss = 0.250453
I1128 16:03:38.544981 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.250453 (* 1 = 0.250453 loss)
I1128 16:03:38.544991 26047 sgd_solver.cpp:105] Iteration 22200, lr = 0.1
I1128 16:03:45.715582 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:04:02.034858 26047 solver.cpp:218] Iteration 22400 (8.51473 iter/s, 23.4887s/200 iters), loss = 0.333662
I1128 16:04:02.034888 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.333662 (* 1 = 0.333662 loss)
I1128 16:04:02.034894 26047 sgd_solver.cpp:105] Iteration 22400, lr = 0.1
I1128 16:04:13.664060 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_22500.caffemodel
I1128 16:04:13.670737 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_22500.solverstate
I1128 16:04:25.540259 26047 solver.cpp:218] Iteration 22600 (8.50912 iter/s, 23.5042s/200 iters), loss = 0.17777
I1128 16:04:25.540289 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.17777 (* 1 = 0.17777 loss)
I1128 16:04:25.540294 26047 sgd_solver.cpp:105] Iteration 22600, lr = 0.1
I1128 16:04:31.652148 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:04:49.030309 26047 solver.cpp:218] Iteration 22800 (8.51468 iter/s, 23.4889s/200 iters), loss = 0.270354
I1128 16:04:49.030431 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270354 (* 1 = 0.270354 loss)
I1128 16:04:49.030437 26047 sgd_solver.cpp:105] Iteration 22800, lr = 0.1
I1128 16:05:12.400975 26047 solver.cpp:330] Iteration 23000, Testing net (#0)
I1128 16:05:14.549198 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:05:14.638777 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.741
I1128 16:05:14.638801 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.811628 (* 1 = 0.811628 loss)
I1128 16:05:14.758726 26047 solver.cpp:218] Iteration 23000 (7.77392 iter/s, 25.727s/200 iters), loss = 0.200743
I1128 16:05:14.758760 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.200743 (* 1 = 0.200743 loss)
I1128 16:05:14.758769 26047 sgd_solver.cpp:105] Iteration 23000, lr = 0.1
I1128 16:05:19.698897 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:05:38.243948 26047 solver.cpp:218] Iteration 23200 (8.51642 iter/s, 23.484s/200 iters), loss = 0.241173
I1128 16:05:38.243979 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.241173 (* 1 = 0.241173 loss)
I1128 16:05:38.243986 26047 sgd_solver.cpp:105] Iteration 23200, lr = 0.1
I1128 16:06:01.733943 26047 solver.cpp:218] Iteration 23400 (8.51469 iter/s, 23.4888s/200 iters), loss = 0.250637
I1128 16:06:01.734068 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.250637 (* 1 = 0.250637 loss)
I1128 16:06:01.734093 26047 sgd_solver.cpp:105] Iteration 23400, lr = 0.1
I1128 16:06:05.615181 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:06:25.214885 26047 solver.cpp:218] Iteration 23600 (8.518 iter/s, 23.4797s/200 iters), loss = 0.261847
I1128 16:06:25.214915 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.261847 (* 1 = 0.261847 loss)
I1128 16:06:25.214936 26047 sgd_solver.cpp:105] Iteration 23600, lr = 0.1
I1128 16:06:48.689432 26047 solver.cpp:218] Iteration 23800 (8.52028 iter/s, 23.4734s/200 iters), loss = 0.275026
I1128 16:06:48.689571 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.275026 (* 1 = 0.275026 loss)
I1128 16:06:48.689581 26047 sgd_solver.cpp:105] Iteration 23800, lr = 0.1
I1128 16:06:51.512847 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:07:12.054877 26047 solver.cpp:330] Iteration 24000, Testing net (#0)
I1128 16:07:14.212819 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:07:14.302494 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7183
I1128 16:07:14.302516 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.965279 (* 1 = 0.965279 loss)
I1128 16:07:14.422613 26047 solver.cpp:218] Iteration 24000 (7.77248 iter/s, 25.7318s/200 iters), loss = 0.20224
I1128 16:07:14.422646 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.20224 (* 1 = 0.20224 loss)
I1128 16:07:14.422652 26047 sgd_solver.cpp:105] Iteration 24000, lr = 0.1
I1128 16:07:37.901783 26047 solver.cpp:218] Iteration 24200 (8.5186 iter/s, 23.478s/200 iters), loss = 0.270401
I1128 16:07:37.901882 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270401 (* 1 = 0.270401 loss)
I1128 16:07:37.901906 26047 sgd_solver.cpp:105] Iteration 24200, lr = 0.1
I1128 16:07:39.550431 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:08:01.380993 26047 solver.cpp:218] Iteration 24400 (8.51861 iter/s, 23.478s/200 iters), loss = 0.235858
I1128 16:08:01.381041 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.235858 (* 1 = 0.235858 loss)
I1128 16:08:01.381072 26047 sgd_solver.cpp:105] Iteration 24400, lr = 0.1
I1128 16:08:24.846225 26047 solver.cpp:218] Iteration 24600 (8.52366 iter/s, 23.4641s/200 iters), loss = 0.144416
I1128 16:08:24.846374 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.144416 (* 1 = 0.144416 loss)
I1128 16:08:24.846382 26047 sgd_solver.cpp:105] Iteration 24600, lr = 0.1
I1128 16:08:25.438019 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:08:48.311520 26047 solver.cpp:218] Iteration 24800 (8.52367 iter/s, 23.4641s/200 iters), loss = 0.194823
I1128 16:08:48.311549 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.194823 (* 1 = 0.194823 loss)
I1128 16:08:48.311555 26047 sgd_solver.cpp:105] Iteration 24800, lr = 0.1
I1128 16:09:11.192044 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:09:11.661784 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_25000.caffemodel
I1128 16:09:11.668313 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_25000.solverstate
I1128 16:09:11.671474 26047 solver.cpp:330] Iteration 25000, Testing net (#0)
I1128 16:09:13.821825 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:09:13.913363 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.746
I1128 16:09:13.913386 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.83221 (* 1 = 0.83221 loss)
I1128 16:09:14.033731 26047 solver.cpp:218] Iteration 25000 (7.77575 iter/s, 25.721s/200 iters), loss = 0.154248
I1128 16:09:14.033777 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.154248 (* 1 = 0.154248 loss)
I1128 16:09:14.033784 26047 sgd_solver.cpp:105] Iteration 25000, lr = 0.1
I1128 16:09:37.495272 26047 solver.cpp:218] Iteration 25200 (8.525 iter/s, 23.4604s/200 iters), loss = 0.292733
I1128 16:09:37.495316 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.292733 (* 1 = 0.292733 loss)
I1128 16:09:37.495321 26047 sgd_solver.cpp:105] Iteration 25200, lr = 0.1
I1128 16:09:59.319320 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:10:00.965028 26047 solver.cpp:218] Iteration 25400 (8.52201 iter/s, 23.4686s/200 iters), loss = 0.203653
I1128 16:10:00.965070 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.203652 (* 1 = 0.203652 loss)
I1128 16:10:00.965075 26047 sgd_solver.cpp:105] Iteration 25400, lr = 0.1
I1128 16:10:24.434228 26047 solver.cpp:218] Iteration 25600 (8.52221 iter/s, 23.4681s/200 iters), loss = 0.3232
I1128 16:10:24.434273 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.3232 (* 1 = 0.3232 loss)
I1128 16:10:24.434278 26047 sgd_solver.cpp:105] Iteration 25600, lr = 0.1
I1128 16:10:45.212754 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:10:47.912125 26047 solver.cpp:218] Iteration 25800 (8.51905 iter/s, 23.4768s/200 iters), loss = 0.215582
I1128 16:10:47.912168 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.215582 (* 1 = 0.215582 loss)
I1128 16:10:47.912173 26047 sgd_solver.cpp:105] Iteration 25800, lr = 0.1
I1128 16:11:11.260460 26047 solver.cpp:330] Iteration 26000, Testing net (#0)
I1128 16:11:13.416939 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:11:13.508636 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6888
I1128 16:11:13.508657 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.13084 (* 1 = 1.13084 loss)
I1128 16:11:13.628450 26047 solver.cpp:218] Iteration 26000 (7.77753 iter/s, 25.7151s/200 iters), loss = 0.20205
I1128 16:11:13.628481 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.20205 (* 1 = 0.20205 loss)
I1128 16:11:13.628500 26047 sgd_solver.cpp:105] Iteration 26000, lr = 0.1
I1128 16:11:33.227371 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:11:37.101037 26047 solver.cpp:218] Iteration 26200 (8.52098 iter/s, 23.4715s/200 iters), loss = 0.106478
I1128 16:11:37.101065 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106478 (* 1 = 0.106478 loss)
I1128 16:11:37.101070 26047 sgd_solver.cpp:105] Iteration 26200, lr = 0.1
I1128 16:12:00.563781 26047 solver.cpp:218] Iteration 26400 (8.52455 iter/s, 23.4617s/200 iters), loss = 0.251723
I1128 16:12:00.563824 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.251723 (* 1 = 0.251723 loss)
I1128 16:12:00.563830 26047 sgd_solver.cpp:105] Iteration 26400, lr = 0.1
I1128 16:12:19.114969 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:12:24.046564 26047 solver.cpp:218] Iteration 26600 (8.51728 iter/s, 23.4817s/200 iters), loss = 0.265051
I1128 16:12:24.046609 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.26505 (* 1 = 0.26505 loss)
I1128 16:12:24.046615 26047 sgd_solver.cpp:105] Iteration 26600, lr = 0.1
I1128 16:12:47.508257 26047 solver.cpp:218] Iteration 26800 (8.52493 iter/s, 23.4606s/200 iters), loss = 0.141215
I1128 16:12:47.508301 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.141215 (* 1 = 0.141215 loss)
I1128 16:12:47.508306 26047 sgd_solver.cpp:105] Iteration 26800, lr = 0.1
I1128 16:13:04.997318 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:13:10.867377 26047 solver.cpp:330] Iteration 27000, Testing net (#0)
I1128 16:13:13.020949 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:13:13.111294 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7483
I1128 16:13:13.111315 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.822992 (* 1 = 0.822992 loss)
I1128 16:13:13.230998 26047 solver.cpp:218] Iteration 27000 (7.77558 iter/s, 25.7215s/200 iters), loss = 0.239197
I1128 16:13:13.231029 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.239197 (* 1 = 0.239197 loss)
I1128 16:13:13.231050 26047 sgd_solver.cpp:105] Iteration 27000, lr = 0.1
I1128 16:13:36.685750 26047 solver.cpp:218] Iteration 27200 (8.52745 iter/s, 23.4537s/200 iters), loss = 0.263045
I1128 16:13:36.685878 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.263045 (* 1 = 0.263045 loss)
I1128 16:13:36.685884 26047 sgd_solver.cpp:105] Iteration 27200, lr = 0.1
I1128 16:13:52.992774 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:14:00.154909 26047 solver.cpp:218] Iteration 27400 (8.52225 iter/s, 23.468s/200 iters), loss = 0.262853
I1128 16:14:00.154953 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.262853 (* 1 = 0.262853 loss)
I1128 16:14:00.154959 26047 sgd_solver.cpp:105] Iteration 27400, lr = 0.1
I1128 16:14:11.779141 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_27500.caffemodel
I1128 16:14:11.785964 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_27500.solverstate
I1128 16:14:23.657822 26047 solver.cpp:218] Iteration 27600 (8.50998 iter/s, 23.5018s/200 iters), loss = 0.323838
I1128 16:14:23.657851 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.323838 (* 1 = 0.323838 loss)
I1128 16:14:23.657856 26047 sgd_solver.cpp:105] Iteration 27600, lr = 0.1
I1128 16:14:38.923178 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:14:47.146842 26047 solver.cpp:218] Iteration 27800 (8.515 iter/s, 23.488s/200 iters), loss = 0.253014
I1128 16:14:47.146966 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.253014 (* 1 = 0.253014 loss)
I1128 16:14:47.146987 26047 sgd_solver.cpp:105] Iteration 27800, lr = 0.1
I1128 16:15:10.494649 26047 solver.cpp:330] Iteration 28000, Testing net (#0)
I1128 16:15:12.662775 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:15:12.754524 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7503
I1128 16:15:12.754559 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.820564 (* 1 = 0.820564 loss)
I1128 16:15:12.874518 26047 solver.cpp:218] Iteration 28000 (7.77411 iter/s, 25.7264s/200 iters), loss = 0.235524
I1128 16:15:12.874549 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.235524 (* 1 = 0.235524 loss)
I1128 16:15:12.874555 26047 sgd_solver.cpp:105] Iteration 28000, lr = 0.1
I1128 16:15:26.967435 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:15:36.363273 26047 solver.cpp:218] Iteration 28200 (8.5151 iter/s, 23.4877s/200 iters), loss = 0.152539
I1128 16:15:36.363302 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.152539 (* 1 = 0.152539 loss)
I1128 16:15:36.363307 26047 sgd_solver.cpp:105] Iteration 28200, lr = 0.1
I1128 16:15:59.831440 26047 solver.cpp:218] Iteration 28400 (8.52257 iter/s, 23.4671s/200 iters), loss = 0.159759
I1128 16:15:59.831521 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.159758 (* 1 = 0.159758 loss)
I1128 16:15:59.831527 26047 sgd_solver.cpp:105] Iteration 28400, lr = 0.1
I1128 16:16:12.873536 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:16:23.326597 26047 solver.cpp:218] Iteration 28600 (8.51279 iter/s, 23.494s/200 iters), loss = 0.263103
I1128 16:16:23.326640 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.263103 (* 1 = 0.263103 loss)
I1128 16:16:23.326645 26047 sgd_solver.cpp:105] Iteration 28600, lr = 0.1
I1128 16:16:46.787092 26047 solver.cpp:218] Iteration 28800 (8.52536 iter/s, 23.4594s/200 iters), loss = 0.256852
I1128 16:16:46.787214 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.256852 (* 1 = 0.256852 loss)
I1128 16:16:46.787221 26047 sgd_solver.cpp:105] Iteration 28800, lr = 0.1
I1128 16:16:58.773738 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:17:10.170959 26047 solver.cpp:330] Iteration 29000, Testing net (#0)
I1128 16:17:12.331094 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:17:12.423610 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7234
I1128 16:17:12.423648 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.909294 (* 1 = 0.909294 loss)
I1128 16:17:12.543612 26047 solver.cpp:218] Iteration 29000 (7.7654 iter/s, 25.7553s/200 iters), loss = 0.235638
I1128 16:17:12.543663 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.235638 (* 1 = 0.235638 loss)
I1128 16:17:12.543668 26047 sgd_solver.cpp:105] Iteration 29000, lr = 0.1
I1128 16:17:36.021661 26047 solver.cpp:218] Iteration 29200 (8.51899 iter/s, 23.477s/200 iters), loss = 0.162328
I1128 16:17:36.021777 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.162328 (* 1 = 0.162328 loss)
I1128 16:17:36.021785 26047 sgd_solver.cpp:105] Iteration 29200, lr = 0.1
I1128 16:17:46.836232 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:17:59.519907 26047 solver.cpp:218] Iteration 29400 (8.51158 iter/s, 23.4974s/200 iters), loss = 0.213577
I1128 16:17:59.519937 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.213577 (* 1 = 0.213577 loss)
I1128 16:17:59.519942 26047 sgd_solver.cpp:105] Iteration 29400, lr = 0.1
I1128 16:18:22.999523 26047 solver.cpp:218] Iteration 29600 (8.51749 iter/s, 23.4811s/200 iters), loss = 0.220232
I1128 16:18:22.999611 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.220232 (* 1 = 0.220232 loss)
I1128 16:18:22.999629 26047 sgd_solver.cpp:105] Iteration 29600, lr = 0.1
I1128 16:18:32.750308 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:18:46.475352 26047 solver.cpp:218] Iteration 29800 (8.51892 iter/s, 23.4772s/200 iters), loss = 0.255922
I1128 16:18:46.475391 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.255922 (* 1 = 0.255922 loss)
I1128 16:18:46.475396 26047 sgd_solver.cpp:105] Iteration 29800, lr = 0.1
I1128 16:19:09.835603 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_30000.caffemodel
I1128 16:19:09.842310 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_30000.solverstate
I1128 16:19:09.845559 26047 solver.cpp:330] Iteration 30000, Testing net (#0)
I1128 16:19:12.015486 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:19:12.105912 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.786
I1128 16:19:12.105933 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.683355 (* 1 = 0.683355 loss)
I1128 16:19:12.226251 26047 solver.cpp:218] Iteration 30000 (7.76629 iter/s, 25.7523s/200 iters), loss = 0.182917
I1128 16:19:12.226295 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.182917 (* 1 = 0.182917 loss)
I1128 16:19:12.226300 26047 sgd_solver.cpp:105] Iteration 30000, lr = 0.1
I1128 16:19:20.919924 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:19:35.734771 26047 solver.cpp:218] Iteration 30200 (8.50712 iter/s, 23.5097s/200 iters), loss = 0.334253
I1128 16:19:35.734800 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.334253 (* 1 = 0.334253 loss)
I1128 16:19:35.734805 26047 sgd_solver.cpp:105] Iteration 30200, lr = 0.1
I1128 16:19:59.209602 26047 solver.cpp:218] Iteration 30400 (8.51935 iter/s, 23.476s/200 iters), loss = 0.177605
I1128 16:19:59.209748 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.177605 (* 1 = 0.177605 loss)
I1128 16:19:59.209754 26047 sgd_solver.cpp:105] Iteration 30400, lr = 0.1
I1128 16:20:06.733925 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:20:22.686143 26047 solver.cpp:218] Iteration 30600 (8.5188 iter/s, 23.4775s/200 iters), loss = 0.17425
I1128 16:20:22.686187 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.17425 (* 1 = 0.17425 loss)
I1128 16:20:22.686192 26047 sgd_solver.cpp:105] Iteration 30600, lr = 0.1
I1128 16:20:46.141036 26047 solver.cpp:218] Iteration 30800 (8.52666 iter/s, 23.4559s/200 iters), loss = 0.199604
I1128 16:20:46.141116 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.199604 (* 1 = 0.199604 loss)
I1128 16:20:46.141135 26047 sgd_solver.cpp:105] Iteration 30800, lr = 0.1
I1128 16:20:52.601241 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:21:09.485965 26047 solver.cpp:330] Iteration 31000, Testing net (#0)
I1128 16:21:11.648028 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:21:11.741267 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.6957
I1128 16:21:11.741302 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.08324 (* 1 = 1.08324 loss)
I1128 16:21:11.861199 26047 solver.cpp:218] Iteration 31000 (7.77572 iter/s, 25.7211s/200 iters), loss = 0.265568
I1128 16:21:11.861232 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.265568 (* 1 = 0.265568 loss)
I1128 16:21:11.861238 26047 sgd_solver.cpp:105] Iteration 31000, lr = 0.1
I1128 16:21:35.321734 26047 solver.cpp:218] Iteration 31200 (8.52465 iter/s, 23.4614s/200 iters), loss = 0.268828
I1128 16:21:35.321887 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.268828 (* 1 = 0.268828 loss)
I1128 16:21:35.321892 26047 sgd_solver.cpp:105] Iteration 31200, lr = 0.1
I1128 16:21:40.612155 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:21:58.807354 26047 solver.cpp:218] Iteration 31400 (8.51562 iter/s, 23.4863s/200 iters), loss = 0.187506
I1128 16:21:58.807387 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.187506 (* 1 = 0.187506 loss)
I1128 16:21:58.807392 26047 sgd_solver.cpp:105] Iteration 31400, lr = 0.1
I1128 16:22:22.276183 26047 solver.cpp:218] Iteration 31600 (8.52169 iter/s, 23.4695s/200 iters), loss = 0.293218
I1128 16:22:22.276329 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.293218 (* 1 = 0.293218 loss)
I1128 16:22:22.276336 26047 sgd_solver.cpp:105] Iteration 31600, lr = 0.1
I1128 16:22:26.505611 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:22:45.736327 26047 solver.cpp:218] Iteration 31800 (8.5249 iter/s, 23.4607s/200 iters), loss = 0.277392
I1128 16:22:45.736356 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.277392 (* 1 = 0.277392 loss)
I1128 16:22:45.736361 26047 sgd_solver.cpp:105] Iteration 31800, lr = 0.1
I1128 16:23:09.085479 26047 solver.cpp:330] Iteration 32000, Testing net (#0)
I1128 16:23:11.248677 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:23:11.338773 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7188
I1128 16:23:11.338809 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.926322 (* 1 = 0.926322 loss)
I1128 16:23:11.458405 26047 solver.cpp:218] Iteration 32000 (7.77523 iter/s, 25.7227s/200 iters), loss = 0.330213
I1128 16:23:11.458434 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.330213 (* 1 = 0.330213 loss)
I1128 16:23:11.458441 26047 sgd_solver.cpp:105] Iteration 32000, lr = 0.1
I1128 16:23:14.635767 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:23:34.915910 26047 solver.cpp:218] Iteration 32200 (8.52586 iter/s, 23.458s/200 iters), loss = 0.214082
I1128 16:23:34.915940 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.214082 (* 1 = 0.214082 loss)
I1128 16:23:34.915944 26047 sgd_solver.cpp:105] Iteration 32200, lr = 0.1
I1128 16:23:58.376194 26047 solver.cpp:218] Iteration 32400 (8.52487 iter/s, 23.4608s/200 iters), loss = 0.202548
I1128 16:23:58.376353 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.202548 (* 1 = 0.202548 loss)
I1128 16:23:58.376360 26047 sgd_solver.cpp:105] Iteration 32400, lr = 0.1
I1128 16:24:00.378103 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:24:09.997339 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_32500.caffemodel
I1128 16:24:10.004096 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_32500.solverstate
I1128 16:24:21.863323 26047 solver.cpp:218] Iteration 32600 (8.51519 iter/s, 23.4874s/200 iters), loss = 0.25308
I1128 16:24:21.863365 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.25308 (* 1 = 0.25308 loss)
I1128 16:24:21.863370 26047 sgd_solver.cpp:105] Iteration 32600, lr = 0.1
I1128 16:24:45.325904 26047 solver.cpp:218] Iteration 32800 (8.52408 iter/s, 23.463s/200 iters), loss = 0.147764
I1128 16:24:45.325994 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.147764 (* 1 = 0.147764 loss)
I1128 16:24:45.326000 26047 sgd_solver.cpp:105] Iteration 32800, lr = 0.1
I1128 16:24:46.267392 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:25:08.662330 26047 solver.cpp:330] Iteration 33000, Testing net (#0)
I1128 16:25:10.839489 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:25:10.934371 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.748
I1128 16:25:10.934393 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.813277 (* 1 = 0.813277 loss)
I1128 16:25:11.054491 26047 solver.cpp:218] Iteration 33000 (7.77336 iter/s, 25.7289s/200 iters), loss = 0.155615
I1128 16:25:11.054520 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.155615 (* 1 = 0.155615 loss)
I1128 16:25:11.054527 26047 sgd_solver.cpp:105] Iteration 33000, lr = 0.1
I1128 16:25:34.404955 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:25:34.522712 26047 solver.cpp:218] Iteration 33200 (8.52206 iter/s, 23.4685s/200 iters), loss = 0.289581
I1128 16:25:34.522758 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.28958 (* 1 = 0.28958 loss)
I1128 16:25:34.522763 26047 sgd_solver.cpp:105] Iteration 33200, lr = 0.1
I1128 16:25:57.990687 26047 solver.cpp:218] Iteration 33400 (8.52217 iter/s, 23.4682s/200 iters), loss = 0.365124
I1128 16:25:57.990720 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.365124 (* 1 = 0.365124 loss)
I1128 16:25:57.990726 26047 sgd_solver.cpp:105] Iteration 33400, lr = 0.1
I1128 16:26:20.163050 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:26:21.451755 26047 solver.cpp:218] Iteration 33600 (8.52469 iter/s, 23.4613s/200 iters), loss = 0.181571
I1128 16:26:21.451786 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.181571 (* 1 = 0.181571 loss)
I1128 16:26:21.451807 26047 sgd_solver.cpp:105] Iteration 33600, lr = 0.1
I1128 16:26:44.915896 26047 solver.cpp:218] Iteration 33800 (8.52358 iter/s, 23.4643s/200 iters), loss = 0.178011
I1128 16:26:44.915938 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.178011 (* 1 = 0.178011 loss)
I1128 16:26:44.915944 26047 sgd_solver.cpp:105] Iteration 33800, lr = 0.1
I1128 16:27:06.034389 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:27:08.267565 26047 solver.cpp:330] Iteration 34000, Testing net (#0)
I1128 16:27:10.441857 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:27:10.531961 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7578
I1128 16:27:10.531983 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.804312 (* 1 = 0.804312 loss)
I1128 16:27:10.653095 26047 solver.cpp:218] Iteration 34000 (7.77081 iter/s, 25.7373s/200 iters), loss = 0.188634
I1128 16:27:10.653123 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.188634 (* 1 = 0.188634 loss)
I1128 16:27:10.653129 26047 sgd_solver.cpp:105] Iteration 34000, lr = 0.1
I1128 16:27:34.124047 26047 solver.cpp:218] Iteration 34200 (8.52113 iter/s, 23.4711s/200 iters), loss = 0.18818
I1128 16:27:34.124089 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18818 (* 1 = 0.18818 loss)
I1128 16:27:34.124095 26047 sgd_solver.cpp:105] Iteration 34200, lr = 0.1
I1128 16:27:54.090296 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:27:57.612009 26047 solver.cpp:218] Iteration 34400 (8.51498 iter/s, 23.488s/200 iters), loss = 0.181423
I1128 16:27:57.612037 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.181423 (* 1 = 0.181423 loss)
I1128 16:27:57.612042 26047 sgd_solver.cpp:105] Iteration 34400, lr = 0.1
I1128 16:28:21.089226 26047 solver.cpp:218] Iteration 34600 (8.51888 iter/s, 23.4773s/200 iters), loss = 0.190294
I1128 16:28:21.089254 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.190294 (* 1 = 0.190294 loss)
I1128 16:28:21.089259 26047 sgd_solver.cpp:105] Iteration 34600, lr = 0.1
I1128 16:28:39.998502 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:28:44.576923 26047 solver.cpp:218] Iteration 34800 (8.51509 iter/s, 23.4877s/200 iters), loss = 0.182623
I1128 16:28:44.576967 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.182622 (* 1 = 0.182622 loss)
I1128 16:28:44.576972 26047 sgd_solver.cpp:105] Iteration 34800, lr = 0.1
I1128 16:29:07.942625 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_35000.caffemodel
I1128 16:29:07.974884 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_35000.solverstate
I1128 16:29:07.978241 26047 solver.cpp:330] Iteration 35000, Testing net (#0)
I1128 16:29:10.153189 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:29:10.244119 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7816
I1128 16:29:10.244141 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.66916 (* 1 = 0.66916 loss)
I1128 16:29:10.364804 26047 solver.cpp:218] Iteration 35000 (7.75559 iter/s, 25.7879s/200 iters), loss = 0.183392
I1128 16:29:10.364836 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.183392 (* 1 = 0.183392 loss)
I1128 16:29:10.364842 26047 sgd_solver.cpp:105] Iteration 35000, lr = 0.1
I1128 16:29:28.202762 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:29:33.831475 26047 solver.cpp:218] Iteration 35200 (8.52274 iter/s, 23.4666s/200 iters), loss = 0.226595
I1128 16:29:33.831518 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.226595 (* 1 = 0.226595 loss)
I1128 16:29:33.831524 26047 sgd_solver.cpp:105] Iteration 35200, lr = 0.1
I1128 16:29:57.281787 26047 solver.cpp:218] Iteration 35400 (8.5287 iter/s, 23.4502s/200 iters), loss = 0.158956
I1128 16:29:57.281930 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.158956 (* 1 = 0.158956 loss)
I1128 16:29:57.281937 26047 sgd_solver.cpp:105] Iteration 35400, lr = 0.1
I1128 16:30:13.946666 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:30:20.752306 26047 solver.cpp:218] Iteration 35600 (8.5214 iter/s, 23.4703s/200 iters), loss = 0.229972
I1128 16:30:20.752351 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.229972 (* 1 = 0.229972 loss)
I1128 16:30:20.752357 26047 sgd_solver.cpp:105] Iteration 35600, lr = 0.1
I1128 16:30:44.205157 26047 solver.cpp:218] Iteration 35800 (8.52779 iter/s, 23.4527s/200 iters), loss = 0.234849
I1128 16:30:44.205344 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.234849 (* 1 = 0.234849 loss)
I1128 16:30:44.205353 26047 sgd_solver.cpp:105] Iteration 35800, lr = 0.1
I1128 16:30:59.811630 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:31:07.557694 26047 solver.cpp:330] Iteration 36000, Testing net (#0)
I1128 16:31:09.722630 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:31:09.814488 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7198
I1128 16:31:09.814512 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.935139 (* 1 = 0.935139 loss)
I1128 16:31:09.934967 26047 solver.cpp:218] Iteration 36000 (7.77317 iter/s, 25.7295s/200 iters), loss = 0.182681
I1128 16:31:09.935011 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.18268 (* 1 = 0.18268 loss)
I1128 16:31:09.935016 26047 sgd_solver.cpp:46] MultiStep Status: Iteration 36000, step = 1
I1128 16:31:09.935019 26047 sgd_solver.cpp:105] Iteration 36000, lr = 0.01
I1128 16:31:33.372352 26047 solver.cpp:218] Iteration 36200 (8.53344 iter/s, 23.4372s/200 iters), loss = 0.256342
I1128 16:31:33.372489 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.256342 (* 1 = 0.256342 loss)
I1128 16:31:33.372496 26047 sgd_solver.cpp:105] Iteration 36200, lr = 0.01
I1128 16:31:47.920703 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:31:56.842761 26047 solver.cpp:218] Iteration 36400 (8.52147 iter/s, 23.4701s/200 iters), loss = 0.127302
I1128 16:31:56.842789 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.127302 (* 1 = 0.127302 loss)
I1128 16:31:56.842794 26047 sgd_solver.cpp:105] Iteration 36400, lr = 0.01
I1128 16:32:20.294584 26047 solver.cpp:218] Iteration 36600 (8.52819 iter/s, 23.4516s/200 iters), loss = 0.0940759
I1128 16:32:20.294744 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0940759 (* 1 = 0.0940759 loss)
I1128 16:32:20.294752 26047 sgd_solver.cpp:105] Iteration 36600, lr = 0.01
I1128 16:32:33.668952 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:32:43.760186 26047 solver.cpp:218] Iteration 36800 (8.52324 iter/s, 23.4653s/200 iters), loss = 0.10594
I1128 16:32:43.760219 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.10594 (* 1 = 0.10594 loss)
I1128 16:32:43.760239 26047 sgd_solver.cpp:105] Iteration 36800, lr = 0.01
I1128 16:33:07.082069 26047 solver.cpp:330] Iteration 37000, Testing net (#0)
I1128 16:33:09.246865 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:33:09.336881 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8654
I1128 16:33:09.336904 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.400563 (* 1 = 0.400563 loss)
I1128 16:33:09.456403 26047 solver.cpp:218] Iteration 37000 (7.78333 iter/s, 25.696s/200 iters), loss = 0.101027
I1128 16:33:09.456434 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.101027 (* 1 = 0.101027 loss)
I1128 16:33:09.456439 26047 sgd_solver.cpp:105] Iteration 37000, lr = 0.01
I1128 16:33:21.781971 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:33:32.935901 26047 solver.cpp:218] Iteration 37200 (8.51816 iter/s, 23.4792s/200 iters), loss = 0.0991546
I1128 16:33:32.935943 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0991545 (* 1 = 0.0991545 loss)
I1128 16:33:32.935950 26047 sgd_solver.cpp:105] Iteration 37200, lr = 0.01
I1128 16:33:56.390154 26047 solver.cpp:218] Iteration 37400 (8.52734 iter/s, 23.454s/200 iters), loss = 0.0989549
I1128 16:33:56.390281 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0989549 (* 1 = 0.0989549 loss)
I1128 16:33:56.390301 26047 sgd_solver.cpp:105] Iteration 37400, lr = 0.01
I1128 16:34:07.544670 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:34:08.013869 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_37500.caffemodel
I1128 16:34:08.022698 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_37500.solverstate
I1128 16:34:19.878700 26047 solver.cpp:218] Iteration 37600 (8.51493 iter/s, 23.4882s/200 iters), loss = 0.0819843
I1128 16:34:19.878742 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0819843 (* 1 = 0.0819843 loss)
I1128 16:34:19.878747 26047 sgd_solver.cpp:105] Iteration 37600, lr = 0.01
I1128 16:34:43.344687 26047 solver.cpp:218] Iteration 37800 (8.52308 iter/s, 23.4657s/200 iters), loss = 0.108685
I1128 16:34:43.344844 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.108685 (* 1 = 0.108685 loss)
I1128 16:34:43.344851 26047 sgd_solver.cpp:105] Iteration 37800, lr = 0.01
I1128 16:34:53.443780 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:35:06.687012 26047 solver.cpp:330] Iteration 38000, Testing net (#0)
I1128 16:35:08.865929 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:35:08.955708 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8699
I1128 16:35:08.955729 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.387872 (* 1 = 0.387872 loss)
I1128 16:35:09.075475 26047 solver.cpp:218] Iteration 38000 (7.77293 iter/s, 25.7303s/200 iters), loss = 0.0795126
I1128 16:35:09.075520 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0795126 (* 1 = 0.0795126 loss)
I1128 16:35:09.075525 26047 sgd_solver.cpp:105] Iteration 38000, lr = 0.01
I1128 16:35:32.526114 26047 solver.cpp:218] Iteration 38200 (8.52868 iter/s, 23.4503s/200 iters), loss = 0.0971283
I1128 16:35:32.526257 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0971282 (* 1 = 0.0971282 loss)
I1128 16:35:32.526263 26047 sgd_solver.cpp:105] Iteration 38200, lr = 0.01
I1128 16:35:41.567394 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:35:55.995847 26047 solver.cpp:218] Iteration 38400 (8.52178 iter/s, 23.4693s/200 iters), loss = 0.110003
I1128 16:35:55.995890 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.110003 (* 1 = 0.110003 loss)
I1128 16:35:55.995896 26047 sgd_solver.cpp:105] Iteration 38400, lr = 0.01
I1128 16:36:19.455251 26047 solver.cpp:218] Iteration 38600 (8.5255 iter/s, 23.459s/200 iters), loss = 0.0995417
I1128 16:36:19.455301 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0995416 (* 1 = 0.0995416 loss)
I1128 16:36:19.455307 26047 sgd_solver.cpp:105] Iteration 38600, lr = 0.01
I1128 16:36:27.318387 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:36:42.909799 26047 solver.cpp:218] Iteration 38800 (8.52727 iter/s, 23.4542s/200 iters), loss = 0.0759358
I1128 16:36:42.909840 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0759357 (* 1 = 0.0759357 loss)
I1128 16:36:42.909847 26047 sgd_solver.cpp:105] Iteration 38800, lr = 0.01
I1128 16:37:06.249878 26047 solver.cpp:330] Iteration 39000, Testing net (#0)
I1128 16:37:08.396713 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:37:08.487536 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8743
I1128 16:37:08.487571 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.384499 (* 1 = 0.384499 loss)
I1128 16:37:08.607719 26047 solver.cpp:218] Iteration 39000 (7.78286 iter/s, 25.6975s/200 iters), loss = 0.0792327
I1128 16:37:08.607748 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0792327 (* 1 = 0.0792327 loss)
I1128 16:37:08.607754 26047 sgd_solver.cpp:105] Iteration 39000, lr = 0.01
I1128 16:37:15.414929 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:37:32.057034 26047 solver.cpp:218] Iteration 39200 (8.52917 iter/s, 23.4489s/200 iters), loss = 0.0755205
I1128 16:37:32.057061 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0755204 (* 1 = 0.0755204 loss)
I1128 16:37:32.057066 26047 sgd_solver.cpp:105] Iteration 39200, lr = 0.01
I1128 16:37:55.506944 26047 solver.cpp:218] Iteration 39400 (8.52896 iter/s, 23.4495s/200 iters), loss = 0.102318
I1128 16:37:55.507102 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.102318 (* 1 = 0.102318 loss)
I1128 16:37:55.507108 26047 sgd_solver.cpp:105] Iteration 39400, lr = 0.01
I1128 16:38:01.260308 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:38:18.963397 26047 solver.cpp:218] Iteration 39600 (8.52663 iter/s, 23.4559s/200 iters), loss = 0.0748116
I1128 16:38:18.963426 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0748115 (* 1 = 0.0748115 loss)
I1128 16:38:18.963446 26047 sgd_solver.cpp:105] Iteration 39600, lr = 0.01
I1128 16:38:42.411978 26047 solver.cpp:218] Iteration 39800 (8.52945 iter/s, 23.4482s/200 iters), loss = 0.0365288
I1128 16:38:42.412116 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0365287 (* 1 = 0.0365287 loss)
I1128 16:38:42.412122 26047 sgd_solver.cpp:105] Iteration 39800, lr = 0.01
I1128 16:38:46.992007 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:39:05.747076 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_40000.caffemodel
I1128 16:39:05.753661 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_40000.solverstate
I1128 16:39:05.756883 26047 solver.cpp:330] Iteration 40000, Testing net (#0)
I1128 16:39:07.921264 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:39:08.014564 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8722
I1128 16:39:08.014586 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.393361 (* 1 = 0.393361 loss)
I1128 16:39:08.134526 26047 solver.cpp:218] Iteration 40000 (7.77545 iter/s, 25.722s/200 iters), loss = 0.0944593
I1128 16:39:08.134572 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0944592 (* 1 = 0.0944592 loss)
I1128 16:39:08.134577 26047 sgd_solver.cpp:105] Iteration 40000, lr = 0.01
I1128 16:39:31.590505 26047 solver.cpp:218] Iteration 40200 (8.52677 iter/s, 23.4555s/200 iters), loss = 0.0531346
I1128 16:39:31.590564 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0531345 (* 1 = 0.0531345 loss)
I1128 16:39:31.590569 26047 sgd_solver.cpp:105] Iteration 40200, lr = 0.01
I1128 16:39:35.117130 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:39:55.055699 26047 solver.cpp:218] Iteration 40400 (8.52343 iter/s, 23.4647s/200 iters), loss = 0.0841551
I1128 16:39:55.055728 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0841551 (* 1 = 0.0841551 loss)
I1128 16:39:55.055734 26047 sgd_solver.cpp:105] Iteration 40400, lr = 0.01
I1128 16:40:18.526762 26047 solver.cpp:218] Iteration 40600 (8.52129 iter/s, 23.4706s/200 iters), loss = 0.0536244
I1128 16:40:18.526906 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0536243 (* 1 = 0.0536243 loss)
I1128 16:40:18.526913 26047 sgd_solver.cpp:105] Iteration 40600, lr = 0.01
I1128 16:40:20.879300 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:40:42.118551 26047 solver.cpp:218] Iteration 40800 (8.47773 iter/s, 23.5912s/200 iters), loss = 0.127402
I1128 16:40:42.118577 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.127402 (* 1 = 0.127402 loss)
I1128 16:40:42.118582 26047 sgd_solver.cpp:105] Iteration 40800, lr = 0.01
I1128 16:41:05.479807 26047 solver.cpp:330] Iteration 41000, Testing net (#0)
I1128 16:41:07.651322 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:41:07.742799 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8734
I1128 16:41:07.742836 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.397498 (* 1 = 0.397498 loss)
I1128 16:41:07.862555 26047 solver.cpp:218] Iteration 41000 (7.76895 iter/s, 25.7435s/200 iters), loss = 0.0651566
I1128 16:41:07.862598 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0651565 (* 1 = 0.0651565 loss)
I1128 16:41:07.862604 26047 sgd_solver.cpp:105] Iteration 41000, lr = 0.01
I1128 16:41:09.163192 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:41:31.382336 26047 solver.cpp:218] Iteration 41200 (8.50366 iter/s, 23.5193s/200 iters), loss = 0.0402546
I1128 16:41:31.382366 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0402546 (* 1 = 0.0402546 loss)
I1128 16:41:31.382372 26047 sgd_solver.cpp:105] Iteration 41200, lr = 0.01
I1128 16:41:54.935113 26047 solver.cpp:218] Iteration 41400 (8.49174 iter/s, 23.5523s/200 iters), loss = 0.0425318
I1128 16:41:54.935243 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0425318 (* 1 = 0.0425318 loss)
I1128 16:41:54.935252 26047 sgd_solver.cpp:105] Iteration 41400, lr = 0.01
I1128 16:41:55.175910 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:42:18.494334 26047 solver.cpp:218] Iteration 41600 (8.48945 iter/s, 23.5586s/200 iters), loss = 0.0577619
I1128 16:42:18.494361 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0577618 (* 1 = 0.0577618 loss)
I1128 16:42:18.494366 26047 sgd_solver.cpp:105] Iteration 41600, lr = 0.01
I1128 16:42:41.002321 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:42:41.938782 26047 solver.cpp:218] Iteration 41800 (8.53098 iter/s, 23.444s/200 iters), loss = 0.0678284
I1128 16:42:41.938825 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0678283 (* 1 = 0.0678283 loss)
I1128 16:42:41.938832 26047 sgd_solver.cpp:105] Iteration 41800, lr = 0.01
I1128 16:43:05.283908 26047 solver.cpp:330] Iteration 42000, Testing net (#0)
I1128 16:43:07.436820 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:43:07.527797 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8697
I1128 16:43:07.527820 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.404061 (* 1 = 0.404061 loss)
I1128 16:43:07.647603 26047 solver.cpp:218] Iteration 42000 (7.7796 iter/s, 25.7083s/200 iters), loss = 0.0622837
I1128 16:43:07.647651 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0622836 (* 1 = 0.0622836 loss)
I1128 16:43:07.647657 26047 sgd_solver.cpp:105] Iteration 42000, lr = 0.01
I1128 16:43:29.107269 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:43:31.101347 26047 solver.cpp:218] Iteration 42200 (8.52761 iter/s, 23.4532s/200 iters), loss = 0.0383556
I1128 16:43:31.101389 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0383556 (* 1 = 0.0383556 loss)
I1128 16:43:31.101394 26047 sgd_solver.cpp:105] Iteration 42200, lr = 0.01
I1128 16:43:54.553652 26047 solver.cpp:218] Iteration 42400 (8.52813 iter/s, 23.4518s/200 iters), loss = 0.07428
I1128 16:43:54.553694 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0742799 (* 1 = 0.0742799 loss)
I1128 16:43:54.553699 26047 sgd_solver.cpp:105] Iteration 42400, lr = 0.01
I1128 16:44:06.178616 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_42500.caffemodel
I1128 16:44:06.185513 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_42500.solverstate
I1128 16:44:14.995394 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:44:18.045204 26047 solver.cpp:218] Iteration 42600 (8.51389 iter/s, 23.491s/200 iters), loss = 0.0483855
I1128 16:44:18.045233 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0483855 (* 1 = 0.0483855 loss)
I1128 16:44:18.045238 26047 sgd_solver.cpp:105] Iteration 42600, lr = 0.01
I1128 16:44:41.510781 26047 solver.cpp:218] Iteration 42800 (8.52331 iter/s, 23.4651s/200 iters), loss = 0.0620138
I1128 16:44:41.510890 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0620137 (* 1 = 0.0620137 loss)
I1128 16:44:41.510895 26047 sgd_solver.cpp:105] Iteration 42800, lr = 0.01
I1128 16:45:00.761664 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:45:04.872841 26047 solver.cpp:330] Iteration 43000, Testing net (#0)
I1128 16:45:07.030556 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:45:07.123837 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8788
I1128 16:45:07.123859 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.391632 (* 1 = 0.391632 loss)
I1128 16:45:07.243851 26047 solver.cpp:218] Iteration 43000 (7.77229 iter/s, 25.7324s/200 iters), loss = 0.0714698
I1128 16:45:07.243882 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0714698 (* 1 = 0.0714698 loss)
I1128 16:45:07.243887 26047 sgd_solver.cpp:105] Iteration 43000, lr = 0.01
I1128 16:45:30.693300 26047 solver.cpp:218] Iteration 43200 (8.52918 iter/s, 23.4489s/200 iters), loss = 0.0274595
I1128 16:45:30.693436 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0274594 (* 1 = 0.0274594 loss)
I1128 16:45:30.693444 26047 sgd_solver.cpp:105] Iteration 43200, lr = 0.01
I1128 16:45:48.869925 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:45:54.153136 26047 solver.cpp:218] Iteration 43400 (8.52543 iter/s, 23.4592s/200 iters), loss = 0.0569314
I1128 16:45:54.153163 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0569313 (* 1 = 0.0569313 loss)
I1128 16:45:54.153168 26047 sgd_solver.cpp:105] Iteration 43400, lr = 0.01
I1128 16:46:17.603148 26047 solver.cpp:218] Iteration 43600 (8.52897 iter/s, 23.4495s/200 iters), loss = 0.0991704
I1128 16:46:17.603271 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0991704 (* 1 = 0.0991704 loss)
I1128 16:46:17.603281 26047 sgd_solver.cpp:105] Iteration 43600, lr = 0.01
I1128 16:46:34.621307 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:46:41.074143 26047 solver.cpp:218] Iteration 43800 (8.52138 iter/s, 23.4704s/200 iters), loss = 0.0436746
I1128 16:46:41.074183 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0436745 (* 1 = 0.0436745 loss)
I1128 16:46:41.074189 26047 sgd_solver.cpp:105] Iteration 43800, lr = 0.01
I1128 16:47:04.407940 26047 solver.cpp:330] Iteration 44000, Testing net (#0)
I1128 16:47:06.574744 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:47:06.665966 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8793
I1128 16:47:06.666002 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.387579 (* 1 = 0.387579 loss)
I1128 16:47:06.785501 26047 solver.cpp:218] Iteration 44000 (7.77884 iter/s, 25.7108s/200 iters), loss = 0.0510285
I1128 16:47:06.785545 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0510284 (* 1 = 0.0510284 loss)
I1128 16:47:06.785552 26047 sgd_solver.cpp:105] Iteration 44000, lr = 0.01
I1128 16:47:22.742094 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:47:30.252929 26047 solver.cpp:218] Iteration 44200 (8.52265 iter/s, 23.4669s/200 iters), loss = 0.0411261
I1128 16:47:30.252971 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.041126 (* 1 = 0.041126 loss)
I1128 16:47:30.252977 26047 sgd_solver.cpp:105] Iteration 44200, lr = 0.01
I1128 16:47:53.693147 26047 solver.cpp:218] Iteration 44400 (8.53255 iter/s, 23.4397s/200 iters), loss = 0.0526809
I1128 16:47:53.693248 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0526808 (* 1 = 0.0526808 loss)
I1128 16:47:53.693266 26047 sgd_solver.cpp:105] Iteration 44400, lr = 0.01
I1128 16:48:08.598211 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:48:17.197738 26047 solver.cpp:218] Iteration 44600 (8.5092 iter/s, 23.504s/200 iters), loss = 0.0487624
I1128 16:48:17.197772 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0487623 (* 1 = 0.0487623 loss)
I1128 16:48:17.197777 26047 sgd_solver.cpp:105] Iteration 44600, lr = 0.01
I1128 16:48:40.646778 26047 solver.cpp:218] Iteration 44800 (8.52933 iter/s, 23.4485s/200 iters), loss = 0.0332902
I1128 16:48:40.646980 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0332901 (* 1 = 0.0332901 loss)
I1128 16:48:40.646986 26047 sgd_solver.cpp:105] Iteration 44800, lr = 0.01
I1128 16:48:54.365571 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:49:03.978725 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_45000.caffemodel
I1128 16:49:03.985453 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_45000.solverstate
I1128 16:49:03.988801 26047 solver.cpp:330] Iteration 45000, Testing net (#0)
I1128 16:49:06.152237 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:49:06.244524 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8754
I1128 16:49:06.244561 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.396739 (* 1 = 0.396739 loss)
I1128 16:49:06.365106 26047 solver.cpp:218] Iteration 45000 (7.77679 iter/s, 25.7176s/200 iters), loss = 0.0438422
I1128 16:49:06.365137 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0438422 (* 1 = 0.0438422 loss)
I1128 16:49:06.365159 26047 sgd_solver.cpp:105] Iteration 45000, lr = 0.01
I1128 16:49:29.802654 26047 solver.cpp:218] Iteration 45200 (8.53352 iter/s, 23.437s/200 iters), loss = 0.03453
I1128 16:49:29.802752 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0345299 (* 1 = 0.0345299 loss)
I1128 16:49:29.802772 26047 sgd_solver.cpp:105] Iteration 45200, lr = 0.01
I1128 16:49:42.465970 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:49:53.258450 26047 solver.cpp:218] Iteration 45400 (8.5269 iter/s, 23.4552s/200 iters), loss = 0.0261429
I1128 16:49:53.258493 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0261428 (* 1 = 0.0261428 loss)
I1128 16:49:53.258498 26047 sgd_solver.cpp:105] Iteration 45400, lr = 0.01
I1128 16:50:16.692965 26047 solver.cpp:218] Iteration 45600 (8.53463 iter/s, 23.4339s/200 iters), loss = 0.0441388
I1128 16:50:16.693130 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0441387 (* 1 = 0.0441387 loss)
I1128 16:50:16.693137 26047 sgd_solver.cpp:105] Iteration 45600, lr = 0.01
I1128 16:50:28.302778 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:50:40.146991 26047 solver.cpp:218] Iteration 45800 (8.52757 iter/s, 23.4533s/200 iters), loss = 0.0300098
I1128 16:50:40.147019 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0300097 (* 1 = 0.0300097 loss)
I1128 16:50:40.147024 26047 sgd_solver.cpp:105] Iteration 45800, lr = 0.01
I1128 16:51:03.481374 26047 solver.cpp:330] Iteration 46000, Testing net (#0)
I1128 16:51:05.650696 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:51:05.740486 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8747
I1128 16:51:05.740509 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.408035 (* 1 = 0.408035 loss)
I1128 16:51:05.859969 26047 solver.cpp:218] Iteration 46000 (7.77836 iter/s, 25.7124s/200 iters), loss = 0.0308343
I1128 16:51:05.859999 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0308342 (* 1 = 0.0308342 loss)
I1128 16:51:05.860005 26047 sgd_solver.cpp:105] Iteration 46000, lr = 0.01
I1128 16:51:16.305323 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:51:29.323345 26047 solver.cpp:218] Iteration 46200 (8.52413 iter/s, 23.4628s/200 iters), loss = 0.0546322
I1128 16:51:29.323388 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0546321 (* 1 = 0.0546321 loss)
I1128 16:51:29.323395 26047 sgd_solver.cpp:105] Iteration 46200, lr = 0.01
I1128 16:51:52.779940 26047 solver.cpp:218] Iteration 46400 (8.5266 iter/s, 23.456s/200 iters), loss = 0.0412819
I1128 16:51:52.780119 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0412818 (* 1 = 0.0412818 loss)
I1128 16:51:52.780128 26047 sgd_solver.cpp:105] Iteration 46400, lr = 0.01
I1128 16:52:02.182202 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:52:16.264417 26047 solver.cpp:218] Iteration 46600 (8.51674 iter/s, 23.4832s/200 iters), loss = 0.0420967
I1128 16:52:16.264459 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0420966 (* 1 = 0.0420966 loss)
I1128 16:52:16.264464 26047 sgd_solver.cpp:105] Iteration 46600, lr = 0.01
I1128 16:52:39.741372 26047 solver.cpp:218] Iteration 46800 (8.51965 iter/s, 23.4751s/200 iters), loss = 0.0550806
I1128 16:52:39.741524 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0550805 (* 1 = 0.0550805 loss)
I1128 16:52:39.741531 26047 sgd_solver.cpp:105] Iteration 46800, lr = 0.01
I1128 16:52:47.996448 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:53:03.173771 26047 solver.cpp:330] Iteration 47000, Testing net (#0)
I1128 16:53:05.342563 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:53:05.432827 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8743
I1128 16:53:05.432848 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.405923 (* 1 = 0.405923 loss)
I1128 16:53:05.552606 26047 solver.cpp:218] Iteration 47000 (7.74918 iter/s, 25.8092s/200 iters), loss = 0.0528809
I1128 16:53:05.552651 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0528809 (* 1 = 0.0528809 loss)
I1128 16:53:05.552657 26047 sgd_solver.cpp:105] Iteration 47000, lr = 0.01
I1128 16:53:28.992796 26047 solver.cpp:218] Iteration 47200 (8.53297 iter/s, 23.4385s/200 iters), loss = 0.0279567
I1128 16:53:28.992916 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0279567 (* 1 = 0.0279567 loss)
I1128 16:53:28.992923 26047 sgd_solver.cpp:105] Iteration 47200, lr = 0.01
I1128 16:53:36.154530 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:53:52.448390 26047 solver.cpp:218] Iteration 47400 (8.52739 iter/s, 23.4538s/200 iters), loss = 0.0845049
I1128 16:53:52.448434 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0845049 (* 1 = 0.0845049 loss)
I1128 16:53:52.448439 26047 sgd_solver.cpp:105] Iteration 47400, lr = 0.01
I1128 16:54:04.062686 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_47500.caffemodel
I1128 16:54:04.069334 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_47500.solverstate
I1128 16:54:15.925281 26047 solver.cpp:218] Iteration 47600 (8.51961 iter/s, 23.4753s/200 iters), loss = 0.0380439
I1128 16:54:15.925323 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0380438 (* 1 = 0.0380438 loss)
I1128 16:54:15.925328 26047 sgd_solver.cpp:105] Iteration 47600, lr = 0.01
I1128 16:54:22.031919 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:54:39.388556 26047 solver.cpp:218] Iteration 47800 (8.52454 iter/s, 23.4617s/200 iters), loss = 0.0253436
I1128 16:54:39.388710 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0253435 (* 1 = 0.0253435 loss)
I1128 16:54:39.388716 26047 sgd_solver.cpp:105] Iteration 47800, lr = 0.01
I1128 16:55:02.741488 26047 solver.cpp:330] Iteration 48000, Testing net (#0)
I1128 16:55:04.899595 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:55:04.991672 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8777
I1128 16:55:04.991704 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.403044 (* 1 = 0.403044 loss)
I1128 16:55:05.113417 26047 solver.cpp:218] Iteration 48000 (7.77513 iter/s, 25.723s/200 iters), loss = 0.0228038
I1128 16:55:05.113469 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0228038 (* 1 = 0.0228038 loss)
I1128 16:55:05.113477 26047 sgd_solver.cpp:105] Iteration 48000, lr = 0.01
I1128 16:55:10.045516 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:55:28.585000 26047 solver.cpp:218] Iteration 48200 (8.5215 iter/s, 23.4701s/200 iters), loss = 0.0309168
I1128 16:55:28.585043 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0309167 (* 1 = 0.0309167 loss)
I1128 16:55:28.585048 26047 sgd_solver.cpp:105] Iteration 48200, lr = 0.01
I1128 16:55:52.052067 26047 solver.cpp:218] Iteration 48400 (8.52312 iter/s, 23.4656s/200 iters), loss = 0.0370627
I1128 16:55:52.052219 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0370626 (* 1 = 0.0370626 loss)
I1128 16:55:52.052227 26047 sgd_solver.cpp:105] Iteration 48400, lr = 0.01
I1128 16:55:55.932277 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:56:15.524811 26047 solver.cpp:218] Iteration 48600 (8.52109 iter/s, 23.4712s/200 iters), loss = 0.0562747
I1128 16:56:15.524838 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0562746 (* 1 = 0.0562746 loss)
I1128 16:56:15.524844 26047 sgd_solver.cpp:105] Iteration 48600, lr = 0.01
I1128 16:56:38.994452 26047 solver.cpp:218] Iteration 48800 (8.52216 iter/s, 23.4682s/200 iters), loss = 0.0502333
I1128 16:56:38.994580 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0502332 (* 1 = 0.0502332 loss)
I1128 16:56:38.994586 26047 sgd_solver.cpp:105] Iteration 48800, lr = 0.01
I1128 16:56:41.817121 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:57:02.347944 26047 solver.cpp:330] Iteration 49000, Testing net (#0)
I1128 16:57:04.505676 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:57:04.597111 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8767
I1128 16:57:04.597134 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.404055 (* 1 = 0.404055 loss)
I1128 16:57:04.716804 26047 solver.cpp:218] Iteration 49000 (7.77582 iter/s, 25.7208s/200 iters), loss = 0.0230239
I1128 16:57:04.716836 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0230239 (* 1 = 0.0230239 loss)
I1128 16:57:04.716841 26047 sgd_solver.cpp:105] Iteration 49000, lr = 0.01
I1128 16:57:28.177887 26047 solver.cpp:218] Iteration 49200 (8.52525 iter/s, 23.4597s/200 iters), loss = 0.045352
I1128 16:57:28.177973 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0453519 (* 1 = 0.0453519 loss)
I1128 16:57:28.177979 26047 sgd_solver.cpp:105] Iteration 49200, lr = 0.01
I1128 16:57:29.823828 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:57:51.651932 26047 solver.cpp:218] Iteration 49400 (8.52055 iter/s, 23.4727s/200 iters), loss = 0.0389326
I1128 16:57:51.651975 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0389325 (* 1 = 0.0389325 loss)
I1128 16:57:51.651980 26047 sgd_solver.cpp:105] Iteration 49400, lr = 0.01
I1128 16:58:15.122395 26047 solver.cpp:218] Iteration 49600 (8.52182 iter/s, 23.4692s/200 iters), loss = 0.0528286
I1128 16:58:15.122512 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0528285 (* 1 = 0.0528285 loss)
I1128 16:58:15.122519 26047 sgd_solver.cpp:105] Iteration 49600, lr = 0.01
I1128 16:58:15.712674 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:58:38.591516 26047 solver.cpp:218] Iteration 49800 (8.52232 iter/s, 23.4678s/200 iters), loss = 0.0696208
I1128 16:58:38.591547 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0696207 (* 1 = 0.0696207 loss)
I1128 16:58:38.591552 26047 sgd_solver.cpp:105] Iteration 49800, lr = 0.01
I1128 16:59:01.478345 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:59:01.946877 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_50000.caffemodel
I1128 16:59:01.953524 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_50000.solverstate
I1128 16:59:01.956771 26047 solver.cpp:330] Iteration 50000, Testing net (#0)
I1128 16:59:04.121232 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:59:04.210028 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8795
I1128 16:59:04.210050 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.396647 (* 1 = 0.396647 loss)
I1128 16:59:04.330493 26047 solver.cpp:218] Iteration 50000 (7.77073 iter/s, 25.7376s/200 iters), loss = 0.0320383
I1128 16:59:04.330523 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0320382 (* 1 = 0.0320382 loss)
I1128 16:59:04.330528 26047 sgd_solver.cpp:46] MultiStep Status: Iteration 50000, step = 2
I1128 16:59:04.330531 26047 sgd_solver.cpp:105] Iteration 50000, lr = 0.001
I1128 16:59:27.792440 26047 solver.cpp:218] Iteration 50200 (8.52489 iter/s, 23.4607s/200 iters), loss = 0.0264504
I1128 16:59:27.792484 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0264504 (* 1 = 0.0264504 loss)
I1128 16:59:27.792488 26047 sgd_solver.cpp:105] Iteration 50200, lr = 0.001
I1128 16:59:49.620123 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:59:51.263988 26047 solver.cpp:218] Iteration 50400 (8.52139 iter/s, 23.4703s/200 iters), loss = 0.0313059
I1128 16:59:51.264029 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0313058 (* 1 = 0.0313058 loss)
I1128 16:59:51.264034 26047 sgd_solver.cpp:105] Iteration 50400, lr = 0.001
I1128 17:00:14.733098 26047 solver.cpp:218] Iteration 50600 (8.52227 iter/s, 23.4679s/200 iters), loss = 0.0354656
I1128 17:00:14.733127 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0354656 (* 1 = 0.0354656 loss)
I1128 17:00:14.733132 26047 sgd_solver.cpp:105] Iteration 50600, lr = 0.001
I1128 17:00:35.505079 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:00:38.205427 26047 solver.cpp:218] Iteration 50800 (8.52109 iter/s, 23.4712s/200 iters), loss = 0.018997
I1128 17:00:38.205458 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.018997 (* 1 = 0.018997 loss)
I1128 17:00:38.205463 26047 sgd_solver.cpp:105] Iteration 50800, lr = 0.001
I1128 17:01:01.547439 26047 solver.cpp:330] Iteration 51000, Testing net (#0)
I1128 17:01:03.703824 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:01:03.795567 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8843
I1128 17:01:03.795601 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.381737 (* 1 = 0.381737 loss)
I1128 17:01:03.915632 26047 solver.cpp:218] Iteration 51000 (7.77939 iter/s, 25.709s/200 iters), loss = 0.0208058
I1128 17:01:03.915663 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0208058 (* 1 = 0.0208058 loss)
I1128 17:01:03.915668 26047 sgd_solver.cpp:105] Iteration 51000, lr = 0.001
I1128 17:01:23.503854 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:01:27.375598 26047 solver.cpp:218] Iteration 51200 (8.52557 iter/s, 23.4588s/200 iters), loss = 0.059736
I1128 17:01:27.375630 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0597359 (* 1 = 0.0597359 loss)
I1128 17:01:27.375635 26047 sgd_solver.cpp:105] Iteration 51200, lr = 0.001
I1128 17:01:50.841536 26047 solver.cpp:218] Iteration 51400 (8.52339 iter/s, 23.4648s/200 iters), loss = 0.0251878
I1128 17:01:50.841568 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0251877 (* 1 = 0.0251877 loss)
I1128 17:01:50.841573 26047 sgd_solver.cpp:105] Iteration 51400, lr = 0.001
I1128 17:02:09.389320 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:02:14.320163 26047 solver.cpp:218] Iteration 51600 (8.51878 iter/s, 23.4775s/200 iters), loss = 0.0311046
I1128 17:02:14.320207 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0311046 (* 1 = 0.0311046 loss)
I1128 17:02:14.320214 26047 sgd_solver.cpp:105] Iteration 51600, lr = 0.001
I1128 17:02:37.785905 26047 solver.cpp:218] Iteration 51800 (8.52346 iter/s, 23.4647s/200 iters), loss = 0.0264021
I1128 17:02:37.785934 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.026402 (* 1 = 0.026402 loss)
I1128 17:02:37.785939 26047 sgd_solver.cpp:105] Iteration 51800, lr = 0.001
I1128 17:02:55.278097 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:03:01.149979 26047 solver.cpp:330] Iteration 52000, Testing net (#0)
I1128 17:03:03.299832 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:03:03.391211 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8849
I1128 17:03:03.391232 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.383173 (* 1 = 0.383173 loss)
I1128 17:03:03.511119 26047 solver.cpp:218] Iteration 52000 (7.77482 iter/s, 25.7241s/200 iters), loss = 0.0582661
I1128 17:03:03.511163 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.058266 (* 1 = 0.058266 loss)
I1128 17:03:03.511169 26047 sgd_solver.cpp:105] Iteration 52000, lr = 0.001
I1128 17:03:26.971068 26047 solver.cpp:218] Iteration 52200 (8.52555 iter/s, 23.4589s/200 iters), loss = 0.025152
I1128 17:03:26.971146 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0251519 (* 1 = 0.0251519 loss)
I1128 17:03:26.971153 26047 sgd_solver.cpp:105] Iteration 52200, lr = 0.001
I1128 17:03:43.292281 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:03:50.451817 26047 solver.cpp:218] Iteration 52400 (8.51801 iter/s, 23.4797s/200 iters), loss = 0.022021
I1128 17:03:50.451846 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0220209 (* 1 = 0.0220209 loss)
I1128 17:03:50.451851 26047 sgd_solver.cpp:105] Iteration 52400, lr = 0.001
I1128 17:04:02.070201 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_52500.caffemodel
I1128 17:04:02.077038 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_52500.solverstate
I1128 17:04:13.938705 26047 solver.cpp:218] Iteration 52600 (8.51576 iter/s, 23.4859s/200 iters), loss = 0.0387935
I1128 17:04:13.938736 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0387935 (* 1 = 0.0387935 loss)
I1128 17:04:13.938742 26047 sgd_solver.cpp:105] Iteration 52600, lr = 0.001
I1128 17:04:29.197783 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:04:37.411574 26047 solver.cpp:218] Iteration 52800 (8.52084 iter/s, 23.4719s/200 iters), loss = 0.040252
I1128 17:04:37.411741 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.040252 (* 1 = 0.040252 loss)
I1128 17:04:37.411747 26047 sgd_solver.cpp:105] Iteration 52800, lr = 0.001
I1128 17:05:00.745641 26047 solver.cpp:330] Iteration 53000, Testing net (#0)
I1128 17:05:02.921795 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:05:03.013455 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8855
I1128 17:05:03.013478 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.383379 (* 1 = 0.383379 loss)
I1128 17:05:03.133090 26047 solver.cpp:218] Iteration 53000 (7.77596 iter/s, 25.7203s/200 iters), loss = 0.0268254
I1128 17:05:03.133131 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0268253 (* 1 = 0.0268253 loss)
I1128 17:05:03.133137 26047 sgd_solver.cpp:105] Iteration 53000, lr = 0.001
I1128 17:05:17.213641 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:05:26.602298 26047 solver.cpp:218] Iteration 53200 (8.52216 iter/s, 23.4682s/200 iters), loss = 0.0201094
I1128 17:05:26.602341 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0201094 (* 1 = 0.0201094 loss)
I1128 17:05:26.602347 26047 sgd_solver.cpp:105] Iteration 53200, lr = 0.001
I1128 17:05:50.061807 26047 solver.cpp:218] Iteration 53400 (8.52568 iter/s, 23.4585s/200 iters), loss = 0.0256835
I1128 17:05:50.061895 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0256834 (* 1 = 0.0256834 loss)
I1128 17:05:50.061902 26047 sgd_solver.cpp:105] Iteration 53400, lr = 0.001
I1128 17:06:03.110160 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:06:13.557310 26047 solver.cpp:218] Iteration 53600 (8.51264 iter/s, 23.4945s/200 iters), loss = 0.0363509
I1128 17:06:13.557340 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0363509 (* 1 = 0.0363509 loss)
I1128 17:06:13.557345 26047 sgd_solver.cpp:105] Iteration 53600, lr = 0.001
I1128 17:06:37.029623 26047 solver.cpp:218] Iteration 53800 (8.52102 iter/s, 23.4714s/200 iters), loss = 0.0267362
I1128 17:06:37.029784 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0267361 (* 1 = 0.0267361 loss)
I1128 17:06:37.029793 26047 sgd_solver.cpp:105] Iteration 53800, lr = 0.001
I1128 17:06:49.012953 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:07:00.408566 26047 solver.cpp:330] Iteration 54000, Testing net (#0)
I1128 17:07:02.570730 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:07:02.661460 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8847
I1128 17:07:02.661484 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.382514 (* 1 = 0.382514 loss)
I1128 17:07:02.781898 26047 solver.cpp:218] Iteration 54000 (7.76665 iter/s, 25.7511s/200 iters), loss = 0.0460319
I1128 17:07:02.781941 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0460318 (* 1 = 0.0460318 loss)
I1128 17:07:02.781947 26047 sgd_solver.cpp:105] Iteration 54000, lr = 0.001
I1128 17:07:26.255060 26047 solver.cpp:218] Iteration 54200 (8.52071 iter/s, 23.4722s/200 iters), loss = 0.0160519
I1128 17:07:26.255195 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0160518 (* 1 = 0.0160518 loss)
I1128 17:07:26.255203 26047 sgd_solver.cpp:105] Iteration 54200, lr = 0.001
I1128 17:07:37.061570 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:07:49.727180 26047 solver.cpp:218] Iteration 54400 (8.52112 iter/s, 23.4711s/200 iters), loss = 0.0287165
I1128 17:07:49.727223 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0287165 (* 1 = 0.0287165 loss)
I1128 17:07:49.727228 26047 sgd_solver.cpp:105] Iteration 54400, lr = 0.001
I1128 17:08:13.201475 26047 solver.cpp:218] Iteration 54600 (8.52029 iter/s, 23.4734s/200 iters), loss = 0.0116182
I1128 17:08:13.201601 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0116182 (* 1 = 0.0116182 loss)
I1128 17:08:13.201607 26047 sgd_solver.cpp:105] Iteration 54600, lr = 0.001
I1128 17:08:22.955651 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:08:36.680284 26047 solver.cpp:218] Iteration 54800 (8.51868 iter/s, 23.4778s/200 iters), loss = 0.0205566
I1128 17:08:36.680328 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0205566 (* 1 = 0.0205566 loss)
I1128 17:08:36.680335 26047 sgd_solver.cpp:105] Iteration 54800, lr = 0.001
I1128 17:09:00.032531 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_55000.caffemodel
I1128 17:09:00.039207 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_55000.solverstate
I1128 17:09:00.056335 26047 solver.cpp:330] Iteration 55000, Testing net (#0)
I1128 17:09:02.237785 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:09:02.328100 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8874
I1128 17:09:02.328124 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.383037 (* 1 = 0.383037 loss)
I1128 17:09:02.447944 26047 solver.cpp:218] Iteration 55000 (7.76197 iter/s, 25.7667s/200 iters), loss = 0.0317384
I1128 17:09:02.447988 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0317384 (* 1 = 0.0317384 loss)
I1128 17:09:02.447993 26047 sgd_solver.cpp:105] Iteration 55000, lr = 0.001
I1128 17:09:11.145341 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:09:25.934029 26047 solver.cpp:218] Iteration 55200 (8.51601 iter/s, 23.4852s/200 iters), loss = 0.0263267
I1128 17:09:25.934072 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0263266 (* 1 = 0.0263266 loss)
I1128 17:09:25.934078 26047 sgd_solver.cpp:105] Iteration 55200, lr = 0.001
I1128 17:09:49.407532 26047 solver.cpp:218] Iteration 55400 (8.52057 iter/s, 23.4726s/200 iters), loss = 0.0246269
I1128 17:09:49.407733 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0246269 (* 1 = 0.0246269 loss)
I1128 17:09:49.407740 26047 sgd_solver.cpp:105] Iteration 55400, lr = 0.001
I1128 17:09:56.929630 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:10:12.890527 26047 solver.cpp:218] Iteration 55600 (8.51718 iter/s, 23.4819s/200 iters), loss = 0.0211972
I1128 17:10:12.890557 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0211972 (* 1 = 0.0211972 loss)
I1128 17:10:12.890563 26047 sgd_solver.cpp:105] Iteration 55600, lr = 0.001
I1128 17:10:36.364760 26047 solver.cpp:218] Iteration 55800 (8.5203 iter/s, 23.4734s/200 iters), loss = 0.0297669
I1128 17:10:36.364850 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0297668 (* 1 = 0.0297668 loss)
I1128 17:10:36.364856 26047 sgd_solver.cpp:105] Iteration 55800, lr = 0.001
I1128 17:10:42.827831 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:10:59.722556 26047 solver.cpp:330] Iteration 56000, Testing net (#0)
I1128 17:11:01.885946 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:11:01.976711 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8851
I1128 17:11:01.976747 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.381198 (* 1 = 0.381198 loss)
I1128 17:11:02.096374 26047 solver.cpp:218] Iteration 56000 (7.77285 iter/s, 25.7306s/200 iters), loss = 0.0441317
I1128 17:11:02.096415 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0441317 (* 1 = 0.0441317 loss)
I1128 17:11:02.096422 26047 sgd_solver.cpp:105] Iteration 56000, lr = 0.001
I1128 17:11:25.561712 26047 solver.cpp:218] Iteration 56200 (8.52353 iter/s, 23.4645s/200 iters), loss = 0.0271022
I1128 17:11:25.561854 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0271021 (* 1 = 0.0271021 loss)
I1128 17:11:25.561861 26047 sgd_solver.cpp:105] Iteration 56200, lr = 0.001
I1128 17:11:30.845270 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:11:49.039257 26047 solver.cpp:218] Iteration 56400 (8.51913 iter/s, 23.4766s/200 iters), loss = 0.0183633
I1128 17:11:49.039299 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0183632 (* 1 = 0.0183632 loss)
I1128 17:11:49.039305 26047 sgd_solver.cpp:105] Iteration 56400, lr = 0.001
I1128 17:12:12.508666 26047 solver.cpp:218] Iteration 56600 (8.52205 iter/s, 23.4685s/200 iters), loss = 0.024511
I1128 17:12:12.508806 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.024511 (* 1 = 0.024511 loss)
I1128 17:12:12.508813 26047 sgd_solver.cpp:105] Iteration 56600, lr = 0.001
I1128 17:12:16.739918 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:12:35.977608 26047 solver.cpp:218] Iteration 56800 (8.52224 iter/s, 23.468s/200 iters), loss = 0.0344964
I1128 17:12:35.977651 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0344963 (* 1 = 0.0344963 loss)
I1128 17:12:35.977658 26047 sgd_solver.cpp:105] Iteration 56800, lr = 0.001
I1128 17:12:59.332028 26047 solver.cpp:330] Iteration 57000, Testing net (#0)
I1128 17:13:01.499747 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:13:01.592438 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8868
I1128 17:13:01.592459 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.382222 (* 1 = 0.382222 loss)
I1128 17:13:01.712581 26047 solver.cpp:218] Iteration 57000 (7.77181 iter/s, 25.734s/200 iters), loss = 0.0321273
I1128 17:13:01.712612 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0321273 (* 1 = 0.0321273 loss)
I1128 17:13:01.712632 26047 sgd_solver.cpp:105] Iteration 57000, lr = 0.001
I1128 17:13:04.888463 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:13:25.179159 26047 solver.cpp:218] Iteration 57200 (8.52307 iter/s, 23.4657s/200 iters), loss = 0.0342631
I1128 17:13:25.179189 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0342631 (* 1 = 0.0342631 loss)
I1128 17:13:25.179194 26047 sgd_solver.cpp:105] Iteration 57200, lr = 0.001
I1128 17:13:48.655448 26047 solver.cpp:218] Iteration 57400 (8.51954 iter/s, 23.4755s/200 iters), loss = 0.0266821
I1128 17:13:48.655577 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0266821 (* 1 = 0.0266821 loss)
I1128 17:13:48.655586 26047 sgd_solver.cpp:105] Iteration 57400, lr = 0.001
I1128 17:13:50.655163 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:14:00.289346 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_57500.caffemodel
I1128 17:14:00.295981 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_57500.solverstate
I1128 17:14:12.181644 26047 solver.cpp:218] Iteration 57600 (8.5015 iter/s, 23.5253s/200 iters), loss = 0.0206407
I1128 17:14:12.181689 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0206407 (* 1 = 0.0206407 loss)
I1128 17:14:12.181694 26047 sgd_solver.cpp:105] Iteration 57600, lr = 0.001
I1128 17:14:35.652467 26047 solver.cpp:218] Iteration 57800 (8.52152 iter/s, 23.47s/200 iters), loss = 0.0353198
I1128 17:14:35.652578 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0353198 (* 1 = 0.0353198 loss)
I1128 17:14:35.652585 26047 sgd_solver.cpp:105] Iteration 57800, lr = 0.001
I1128 17:14:36.595623 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:14:59.069571 26047 solver.cpp:330] Iteration 58000, Testing net (#0)
I1128 17:15:01.232553 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:15:01.322010 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8867
I1128 17:15:01.322029 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.38102 (* 1 = 0.38102 loss)
I1128 17:15:01.444638 26047 solver.cpp:218] Iteration 58000 (7.75459 iter/s, 25.7912s/200 iters), loss = 0.0187033
I1128 17:15:01.444674 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0187033 (* 1 = 0.0187033 loss)
I1128 17:15:01.444696 26047 sgd_solver.cpp:105] Iteration 58000, lr = 0.001
I1128 17:15:24.805371 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:15:24.922818 26047 solver.cpp:218] Iteration 58200 (8.51914 iter/s, 23.4765s/200 iters), loss = 0.020502
I1128 17:15:24.922863 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.020502 (* 1 = 0.020502 loss)
I1128 17:15:24.922868 26047 sgd_solver.cpp:105] Iteration 58200, lr = 0.001
I1128 17:15:48.401681 26047 solver.cpp:218] Iteration 58400 (8.5186 iter/s, 23.478s/200 iters), loss = 0.065586
I1128 17:15:48.401723 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.065586 (* 1 = 0.065586 loss)
I1128 17:15:48.401729 26047 sgd_solver.cpp:105] Iteration 58400, lr = 0.001
I1128 17:16:10.585449 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:16:11.875507 26047 solver.cpp:218] Iteration 58600 (8.52043 iter/s, 23.473s/200 iters), loss = 0.0501805
I1128 17:16:11.875551 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0501804 (* 1 = 0.0501804 loss)
I1128 17:16:11.875558 26047 sgd_solver.cpp:105] Iteration 58600, lr = 0.001
I1128 17:16:35.345391 26047 solver.cpp:218] Iteration 58800 (8.52186 iter/s, 23.4691s/200 iters), loss = 0.0279178
I1128 17:16:35.345423 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0279178 (* 1 = 0.0279178 loss)
I1128 17:16:35.345443 26047 sgd_solver.cpp:105] Iteration 58800, lr = 0.001
I1128 17:16:56.464963 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:16:58.697378 26047 solver.cpp:330] Iteration 59000, Testing net (#0)
I1128 17:17:00.868685 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:17:00.961907 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8865
I1128 17:17:00.961943 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.384344 (* 1 = 0.384344 loss)
I1128 17:17:01.081845 26047 solver.cpp:218] Iteration 59000 (7.77134 iter/s, 25.7356s/200 iters), loss = 0.0201613
I1128 17:17:01.081872 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0201613 (* 1 = 0.0201613 loss)
I1128 17:17:01.081878 26047 sgd_solver.cpp:105] Iteration 59000, lr = 0.001
I1128 17:17:24.545370 26047 solver.cpp:218] Iteration 59200 (8.52416 iter/s, 23.4627s/200 iters), loss = 0.0314639
I1128 17:17:24.545413 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0314638 (* 1 = 0.0314638 loss)
I1128 17:17:24.545418 26047 sgd_solver.cpp:105] Iteration 59200, lr = 0.001
I1128 17:17:44.497475 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:17:48.018924 26047 solver.cpp:218] Iteration 59400 (8.52052 iter/s, 23.4727s/200 iters), loss = 0.0190844
I1128 17:17:48.018968 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0190844 (* 1 = 0.0190844 loss)
I1128 17:17:48.018973 26047 sgd_solver.cpp:105] Iteration 59400, lr = 0.001
I1128 17:18:11.474365 26047 solver.cpp:218] Iteration 59600 (8.5271 iter/s, 23.4546s/200 iters), loss = 0.015586
I1128 17:18:11.474395 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.015586 (* 1 = 0.015586 loss)
I1128 17:18:11.474416 26047 sgd_solver.cpp:105] Iteration 59600, lr = 0.001
I1128 17:18:30.362506 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:18:34.940733 26047 solver.cpp:218] Iteration 59800 (8.52312 iter/s, 23.4656s/200 iters), loss = 0.0216782
I1128 17:18:34.940767 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0216782 (* 1 = 0.0216782 loss)
I1128 17:18:34.940773 26047 sgd_solver.cpp:105] Iteration 59800, lr = 0.001
I1128 17:18:58.281716 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_60000.caffemodel
I1128 17:18:58.288373 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_60000.solverstate
I1128 17:18:58.351801 26047 solver.cpp:330] Iteration 60000, Testing net (#0)
I1128 17:19:00.529374 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:19:00.621786 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8863
I1128 17:19:00.621807 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.381358 (* 1 = 0.381358 loss)
I1128 17:19:00.743036 26047 solver.cpp:218] Iteration 60000 (7.75151 iter/s, 25.8014s/200 iters), loss = 0.0194664
I1128 17:19:00.743065 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0194664 (* 1 = 0.0194664 loss)
I1128 17:19:00.743085 26047 sgd_solver.cpp:105] Iteration 60000, lr = 0.001
I1128 17:19:18.586807 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:19:24.223879 26047 solver.cpp:218] Iteration 60200 (8.51787 iter/s, 23.4801s/200 iters), loss = 0.0522182
I1128 17:19:24.223922 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0522182 (* 1 = 0.0522182 loss)
I1128 17:19:24.223928 26047 sgd_solver.cpp:105] Iteration 60200, lr = 0.001
I1128 17:19:47.686699 26047 solver.cpp:218] Iteration 60400 (8.52442 iter/s, 23.462s/200 iters), loss = 0.0225738
I1128 17:19:47.686852 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0225738 (* 1 = 0.0225738 loss)
I1128 17:19:47.686861 26047 sgd_solver.cpp:105] Iteration 60400, lr = 0.001
I1128 17:20:04.366685 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:20:11.180496 26047 solver.cpp:218] Iteration 60600 (8.51321 iter/s, 23.4929s/200 iters), loss = 0.0318509
I1128 17:20:11.180526 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0318509 (* 1 = 0.0318509 loss)
I1128 17:20:11.180531 26047 sgd_solver.cpp:105] Iteration 60600, lr = 0.001
I1128 17:20:34.638689 26047 solver.cpp:218] Iteration 60800 (8.52609 iter/s, 23.4574s/200 iters), loss = 0.0226959
I1128 17:20:34.638782 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0226959 (* 1 = 0.0226959 loss)
I1128 17:20:34.638788 26047 sgd_solver.cpp:105] Iteration 60800, lr = 0.001
I1128 17:20:50.248554 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:20:57.992003 26047 solver.cpp:330] Iteration 61000, Testing net (#0)
I1128 17:21:00.173527 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:21:00.264051 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8871
I1128 17:21:00.264075 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.38075 (* 1 = 0.38075 loss)
I1128 17:21:00.383842 26047 solver.cpp:218] Iteration 61000 (7.76873 iter/s, 25.7442s/200 iters), loss = 0.0560416
I1128 17:21:00.383888 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0560416 (* 1 = 0.0560416 loss)
I1128 17:21:00.383895 26047 sgd_solver.cpp:105] Iteration 61000, lr = 0.001
I1128 17:21:23.835454 26047 solver.cpp:218] Iteration 61200 (8.52849 iter/s, 23.4508s/200 iters), loss = 0.046619
I1128 17:21:23.835542 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.046619 (* 1 = 0.046619 loss)
I1128 17:21:23.835548 26047 sgd_solver.cpp:105] Iteration 61200, lr = 0.001
I1128 17:21:38.395745 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:21:47.320538 26047 solver.cpp:218] Iteration 61400 (8.51635 iter/s, 23.4842s/200 iters), loss = 0.0179925
I1128 17:21:47.320581 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0179925 (* 1 = 0.0179925 loss)
I1128 17:21:47.320586 26047 sgd_solver.cpp:105] Iteration 61400, lr = 0.001
I1128 17:22:10.783221 26047 solver.cpp:218] Iteration 61600 (8.52446 iter/s, 23.4619s/200 iters), loss = 0.0269495
I1128 17:22:10.783352 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0269495 (* 1 = 0.0269495 loss)
I1128 17:22:10.783360 26047 sgd_solver.cpp:105] Iteration 61600, lr = 0.001
I1128 17:22:24.170042 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:22:34.266291 26047 solver.cpp:218] Iteration 61800 (8.51709 iter/s, 23.4822s/200 iters), loss = 0.0284755
I1128 17:22:34.266333 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0284755 (* 1 = 0.0284755 loss)
I1128 17:22:34.266338 26047 sgd_solver.cpp:105] Iteration 61800, lr = 0.001
I1128 17:22:57.609616 26047 solver.cpp:330] Iteration 62000, Testing net (#0)
I1128 17:22:59.776746 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:22:59.867008 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8882
I1128 17:22:59.867030 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.382448 (* 1 = 0.382448 loss)
I1128 17:22:59.986927 26047 solver.cpp:218] Iteration 62000 (7.77612 iter/s, 25.7198s/200 iters), loss = 0.0542323
I1128 17:22:59.986958 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0542323 (* 1 = 0.0542323 loss)
I1128 17:22:59.986963 26047 sgd_solver.cpp:105] Iteration 62000, lr = 0.001
I1128 17:23:12.310925 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:23:23.464226 26047 solver.cpp:218] Iteration 62200 (8.51915 iter/s, 23.4765s/200 iters), loss = 0.0321884
I1128 17:23:23.464270 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0321884 (* 1 = 0.0321884 loss)
I1128 17:23:23.464277 26047 sgd_solver.cpp:105] Iteration 62200, lr = 0.001
I1128 17:23:46.929054 26047 solver.cpp:218] Iteration 62400 (8.52368 iter/s, 23.464s/200 iters), loss = 0.0144991
I1128 17:23:46.929199 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0144991 (* 1 = 0.0144991 loss)
I1128 17:23:46.929221 26047 sgd_solver.cpp:105] Iteration 62400, lr = 0.001
I1128 17:23:58.084733 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:23:58.553895 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_62500.caffemodel
I1128 17:23:58.560430 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_62500.solverstate
I1128 17:24:10.424547 26047 solver.cpp:218] Iteration 62600 (8.51259 iter/s, 23.4946s/200 iters), loss = 0.0191803
I1128 17:24:10.424576 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0191803 (* 1 = 0.0191803 loss)
I1128 17:24:10.424580 26047 sgd_solver.cpp:105] Iteration 62600, lr = 0.001
I1128 17:24:33.897068 26047 solver.cpp:218] Iteration 62800 (8.52088 iter/s, 23.4718s/200 iters), loss = 0.0289644
I1128 17:24:33.897229 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0289644 (* 1 = 0.0289644 loss)
I1128 17:24:33.897238 26047 sgd_solver.cpp:105] Iteration 62800, lr = 0.001
I1128 17:24:43.997434 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:24:57.240731 26047 solver.cpp:330] Iteration 63000, Testing net (#0)
I1128 17:24:59.405042 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:24:59.496693 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8873
I1128 17:24:59.496714 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.3818 (* 1 = 0.3818 loss)
I1128 17:24:59.617130 26047 solver.cpp:218] Iteration 63000 (7.77632 iter/s, 25.7191s/200 iters), loss = 0.0348994
I1128 17:24:59.617173 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0348994 (* 1 = 0.0348994 loss)
I1128 17:24:59.617179 26047 sgd_solver.cpp:105] Iteration 63000, lr = 0.001
I1128 17:25:23.066454 26047 solver.cpp:218] Iteration 63200 (8.52931 iter/s, 23.4485s/200 iters), loss = 0.0663765
I1128 17:25:23.066602 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0663765 (* 1 = 0.0663765 loss)
I1128 17:25:23.066610 26047 sgd_solver.cpp:105] Iteration 63200, lr = 0.001
I1128 17:25:32.100504 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:25:46.512843 26047 solver.cpp:218] Iteration 63400 (8.53042 iter/s, 23.4455s/200 iters), loss = 0.0275502
I1128 17:25:46.512887 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0275502 (* 1 = 0.0275502 loss)
I1128 17:25:46.512893 26047 sgd_solver.cpp:105] Iteration 63400, lr = 0.001
I1128 17:26:09.962249 26047 solver.cpp:218] Iteration 63600 (8.52928 iter/s, 23.4486s/200 iters), loss = 0.0147009
I1128 17:26:09.962375 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0147009 (* 1 = 0.0147009 loss)
I1128 17:26:09.962383 26047 sgd_solver.cpp:105] Iteration 63600, lr = 0.001
I1128 17:26:17.829960 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:26:33.427940 26047 solver.cpp:218] Iteration 63800 (8.52354 iter/s, 23.4644s/200 iters), loss = 0.0307812
I1128 17:26:33.427969 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0307812 (* 1 = 0.0307812 loss)
I1128 17:26:33.427974 26047 sgd_solver.cpp:105] Iteration 63800, lr = 0.001
I1128 17:26:56.778959 26047 solver.cpp:330] Iteration 64000, Testing net (#0)
I1128 17:26:58.937824 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:26:59.028187 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8866
I1128 17:26:59.028221 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.382498 (* 1 = 0.382498 loss)
I1128 17:26:59.148075 26047 solver.cpp:218] Iteration 64000 (7.77641 iter/s, 25.7188s/200 iters), loss = 0.019593
I1128 17:26:59.148103 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0195929 (* 1 = 0.0195929 loss)
I1128 17:26:59.148109 26047 sgd_solver.cpp:105] Iteration 64000, lr = 0.001
I1128 17:27:05.962888 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:27:22.817832 26047 solver.cpp:218] Iteration 64200 (8.45004 iter/s, 23.6685s/200 iters), loss = 0.02546
I1128 17:27:22.817862 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0254599 (* 1 = 0.0254599 loss)
I1128 17:27:22.817867 26047 sgd_solver.cpp:105] Iteration 64200, lr = 0.001
I1128 17:27:46.293773 26047 solver.cpp:218] Iteration 64400 (8.51979 iter/s, 23.4748s/200 iters), loss = 0.0380537
I1128 17:27:46.293915 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0380536 (* 1 = 0.0380536 loss)
I1128 17:27:46.293921 26047 sgd_solver.cpp:105] Iteration 64400, lr = 0.001
I1128 17:27:52.042609 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:28:09.756878 26047 solver.cpp:218] Iteration 64600 (8.52449 iter/s, 23.4618s/200 iters), loss = 0.0237031
I1128 17:28:09.756922 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.023703 (* 1 = 0.023703 loss)
I1128 17:28:09.756927 26047 sgd_solver.cpp:105] Iteration 64600, lr = 0.001
I1128 17:28:33.202674 26047 solver.cpp:218] Iteration 64800 (8.53074 iter/s, 23.4446s/200 iters), loss = 0.0193698
I1128 17:28:33.202772 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0193697 (* 1 = 0.0193697 loss)
I1128 17:28:33.202796 26047 sgd_solver.cpp:105] Iteration 64800, lr = 0.001
I1128 17:28:37.780552 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:28:56.536810 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_65000.caffemodel
I1128 17:28:56.543376 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_65000.solverstate
I1128 17:28:56.591477 26047 solver.cpp:330] Iteration 65000, Testing net (#0)
I1128 17:28:58.749536 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:28:58.840519 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8875
I1128 17:28:58.840554 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.38202 (* 1 = 0.38202 loss)
I1128 17:28:58.960450 26047 solver.cpp:218] Iteration 65000 (7.76504 iter/s, 25.7565s/200 iters), loss = 0.0306689
I1128 17:28:58.960496 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0306688 (* 1 = 0.0306688 loss)
I1128 17:28:58.960502 26047 sgd_solver.cpp:105] Iteration 65000, lr = 0.001
I1128 17:29:22.510911 26047 solver.cpp:218] Iteration 65200 (8.49282 iter/s, 23.5493s/200 iters), loss = 0.043138
I1128 17:29:22.511044 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0431379 (* 1 = 0.0431379 loss)
I1128 17:29:22.511052 26047 sgd_solver.cpp:105] Iteration 65200, lr = 0.001
I1128 17:29:26.082783 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:29:46.068604 26047 solver.cpp:218] Iteration 65400 (8.49023 iter/s, 23.5565s/200 iters), loss = 0.0251244
I1128 17:29:46.068647 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0251244 (* 1 = 0.0251244 loss)
I1128 17:29:46.068652 26047 sgd_solver.cpp:105] Iteration 65400, lr = 0.001
I1128 17:30:09.692812 26047 solver.cpp:218] Iteration 65600 (8.46629 iter/s, 23.6231s/200 iters), loss = 0.0233116
I1128 17:30:09.692940 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0233115 (* 1 = 0.0233115 loss)
I1128 17:30:09.692948 26047 sgd_solver.cpp:105] Iteration 65600, lr = 0.001
I1128 17:30:12.041604 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:30:33.219158 26047 solver.cpp:218] Iteration 65800 (8.50154 iter/s, 23.5252s/200 iters), loss = 0.0417457
I1128 17:30:33.219192 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0417456 (* 1 = 0.0417456 loss)
I1128 17:30:33.219199 26047 sgd_solver.cpp:105] Iteration 65800, lr = 0.001
I1128 17:30:56.674473 26047 solver.cpp:330] Iteration 66000, Testing net (#0)
I1128 17:30:58.854729 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:30:58.943104 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.887
I1128 17:30:58.943125 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.383183 (* 1 = 0.383183 loss)
I1128 17:30:59.062976 26047 solver.cpp:218] Iteration 66000 (7.73915 iter/s, 25.8426s/200 iters), loss = 0.0189676
I1128 17:30:59.063007 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0189676 (* 1 = 0.0189676 loss)
I1128 17:30:59.063014 26047 sgd_solver.cpp:105] Iteration 66000, lr = 0.001
I1128 17:31:00.356766 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:31:22.915561 26047 solver.cpp:218] Iteration 66200 (8.38522 iter/s, 23.8515s/200 iters), loss = 0.015637
I1128 17:31:22.915596 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.015637 (* 1 = 0.015637 loss)
I1128 17:31:22.915618 26047 sgd_solver.cpp:105] Iteration 66200, lr = 0.001
I1128 17:31:46.434293 26047 solver.cpp:218] Iteration 66400 (8.50424 iter/s, 23.5177s/200 iters), loss = 0.0275771
I1128 17:31:46.434376 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0275771 (* 1 = 0.0275771 loss)
I1128 17:31:46.434382 26047 sgd_solver.cpp:105] Iteration 66400, lr = 0.001
I1128 17:31:46.672384 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:32:10.047499 26047 solver.cpp:218] Iteration 66600 (8.47023 iter/s, 23.6121s/200 iters), loss = 0.031072
I1128 17:32:10.047529 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.031072 (* 1 = 0.031072 loss)
I1128 17:32:10.047534 26047 sgd_solver.cpp:105] Iteration 66600, lr = 0.001
I1128 17:32:32.555831 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:32:33.493278 26047 solver.cpp:218] Iteration 66800 (8.5307 iter/s, 23.4447s/200 iters), loss = 0.0313203
I1128 17:32:33.493309 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0313203 (* 1 = 0.0313203 loss)
I1128 17:32:33.493314 26047 sgd_solver.cpp:105] Iteration 66800, lr = 0.001
I1128 17:32:56.816922 26047 solver.cpp:330] Iteration 67000, Testing net (#0)
I1128 17:32:58.980485 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:32:59.071938 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8867
I1128 17:32:59.071961 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.383722 (* 1 = 0.383722 loss)
I1128 17:32:59.192420 26047 solver.cpp:218] Iteration 67000 (7.7827 iter/s, 25.698s/200 iters), loss = 0.0191034
I1128 17:32:59.192450 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0191033 (* 1 = 0.0191033 loss)
I1128 17:32:59.192456 26047 sgd_solver.cpp:105] Iteration 67000, lr = 0.001
I1128 17:33:20.641708 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:33:22.633848 26047 solver.cpp:218] Iteration 67200 (8.53227 iter/s, 23.4404s/200 iters), loss = 0.0327118
I1128 17:33:22.633877 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0327118 (* 1 = 0.0327118 loss)
I1128 17:33:22.633882 26047 sgd_solver.cpp:105] Iteration 67200, lr = 0.001
I1128 17:33:46.081984 26047 solver.cpp:218] Iteration 67400 (8.52983 iter/s, 23.4471s/200 iters), loss = 0.0466332
I1128 17:33:46.082026 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0466332 (* 1 = 0.0466332 loss)
I1128 17:33:46.082032 26047 sgd_solver.cpp:105] Iteration 67400, lr = 0.001
I1128 17:33:57.682883 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_67500.caffemodel
I1128 17:33:57.711886 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_67500.solverstate
I1128 17:34:06.541050 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:34:09.593119 26047 solver.cpp:218] Iteration 67600 (8.50697 iter/s, 23.5101s/200 iters), loss = 0.0144482
I1128 17:34:09.593164 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0144481 (* 1 = 0.0144481 loss)
I1128 17:34:09.593170 26047 sgd_solver.cpp:105] Iteration 67600, lr = 0.001
I1128 17:34:33.049665 26047 solver.cpp:218] Iteration 67800 (8.52677 iter/s, 23.4555s/200 iters), loss = 0.0205651
I1128 17:34:33.049804 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0205651 (* 1 = 0.0205651 loss)
I1128 17:34:33.049813 26047 sgd_solver.cpp:105] Iteration 67800, lr = 0.001
I1128 17:34:52.284379 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:34:56.390645 26047 solver.cpp:330] Iteration 68000, Testing net (#0)
I1128 17:34:58.560297 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:34:58.651521 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8865
I1128 17:34:58.651543 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.383826 (* 1 = 0.383826 loss)
I1128 17:34:58.771553 26047 solver.cpp:218] Iteration 68000 (7.77584 iter/s, 25.7207s/200 iters), loss = 0.0249163
I1128 17:34:58.771586 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0249163 (* 1 = 0.0249163 loss)
I1128 17:34:58.771594 26047 sgd_solver.cpp:105] Iteration 68000, lr = 0.001
I1128 17:35:22.242252 26047 solver.cpp:218] Iteration 68200 (8.52162 iter/s, 23.4697s/200 iters), loss = 0.0229353
I1128 17:35:22.242430 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0229353 (* 1 = 0.0229353 loss)
I1128 17:35:22.242439 26047 sgd_solver.cpp:105] Iteration 68200, lr = 0.001
I1128 17:35:40.426443 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:35:45.705287 26047 solver.cpp:218] Iteration 68400 (8.52445 iter/s, 23.4619s/200 iters), loss = 0.021109
I1128 17:35:45.705317 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.021109 (* 1 = 0.021109 loss)
I1128 17:35:45.705322 26047 sgd_solver.cpp:105] Iteration 68400, lr = 0.001
I1128 17:36:09.176743 26047 solver.cpp:218] Iteration 68600 (8.52134 iter/s, 23.4705s/200 iters), loss = 0.0423526
I1128 17:36:09.176879 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0423525 (* 1 = 0.0423525 loss)
I1128 17:36:09.176887 26047 sgd_solver.cpp:105] Iteration 68600, lr = 0.001
I1128 17:36:26.189891 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:36:32.641355 26047 solver.cpp:218] Iteration 68800 (8.52386 iter/s, 23.4636s/200 iters), loss = 0.0130142
I1128 17:36:32.641398 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0130142 (* 1 = 0.0130142 loss)
I1128 17:36:32.641404 26047 sgd_solver.cpp:105] Iteration 68800, lr = 0.001
I1128 17:36:55.989593 26047 solver.cpp:330] Iteration 69000, Testing net (#0)
I1128 17:36:58.150488 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:36:58.242547 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8863
I1128 17:36:58.242583 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.385256 (* 1 = 0.385256 loss)
I1128 17:36:58.362406 26047 solver.cpp:218] Iteration 69000 (7.77605 iter/s, 25.72s/200 iters), loss = 0.0192735
I1128 17:36:58.362450 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0192734 (* 1 = 0.0192734 loss)
I1128 17:36:58.362457 26047 sgd_solver.cpp:105] Iteration 69000, lr = 0.001
I1128 17:37:14.321676 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:37:21.822124 26047 solver.cpp:218] Iteration 69200 (8.5256 iter/s, 23.4588s/200 iters), loss = 0.0363018
I1128 17:37:21.822167 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0363018 (* 1 = 0.0363018 loss)
I1128 17:37:21.822172 26047 sgd_solver.cpp:105] Iteration 69200, lr = 0.001
I1128 17:37:45.282912 26047 solver.cpp:218] Iteration 69400 (8.52521 iter/s, 23.4598s/200 iters), loss = 0.0313774
I1128 17:37:45.283069 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0313774 (* 1 = 0.0313774 loss)
I1128 17:37:45.283077 26047 sgd_solver.cpp:105] Iteration 69400, lr = 0.001
I1128 17:38:00.183871 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:38:08.745409 26047 solver.cpp:218] Iteration 69600 (8.52463 iter/s, 23.4614s/200 iters), loss = 0.0152373
I1128 17:38:08.745440 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0152373 (* 1 = 0.0152373 loss)
I1128 17:38:08.745460 26047 sgd_solver.cpp:105] Iteration 69600, lr = 0.001
I1128 17:38:32.201478 26047 solver.cpp:218] Iteration 69800 (8.52692 iter/s, 23.4551s/200 iters), loss = 0.0392053
I1128 17:38:32.201598 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0392053 (* 1 = 0.0392053 loss)
I1128 17:38:32.201604 26047 sgd_solver.cpp:105] Iteration 69800, lr = 0.001
I1128 17:38:45.917960 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:38:55.533105 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_70000.caffemodel
I1128 17:38:55.539753 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_70000.solverstate
I1128 17:38:55.560370 26047 solver.cpp:330] Iteration 70000, Testing net (#0)
I1128 17:38:57.708003 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:38:57.800228 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8862
I1128 17:38:57.800251 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.383893 (* 1 = 0.383893 loss)
I1128 17:38:57.920018 26047 solver.cpp:218] Iteration 70000 (7.77682 iter/s, 25.7174s/200 iters), loss = 0.0158964
I1128 17:38:57.920064 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0158963 (* 1 = 0.0158963 loss)
I1128 17:38:57.920069 26047 sgd_solver.cpp:105] Iteration 70000, lr = 0.001
I1128 17:39:21.370162 26047 solver.cpp:218] Iteration 70200 (8.52907 iter/s, 23.4492s/200 iters), loss = 0.0117554
I1128 17:39:21.370328 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0117554 (* 1 = 0.0117554 loss)
I1128 17:39:21.370335 26047 sgd_solver.cpp:105] Iteration 70200, lr = 0.001
I1128 17:39:34.036428 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:39:44.817427 26047 solver.cpp:218] Iteration 70400 (8.53016 iter/s, 23.4462s/200 iters), loss = 0.0113949
I1128 17:39:44.817471 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0113949 (* 1 = 0.0113949 loss)
I1128 17:39:44.817476 26047 sgd_solver.cpp:105] Iteration 70400, lr = 0.001
I1128 17:40:08.258021 26047 solver.cpp:218] Iteration 70600 (8.53254 iter/s, 23.4397s/200 iters), loss = 0.0413028
I1128 17:40:08.258157 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0413028 (* 1 = 0.0413028 loss)
I1128 17:40:08.258164 26047 sgd_solver.cpp:105] Iteration 70600, lr = 0.001
I1128 17:40:19.860638 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:40:31.698735 26047 solver.cpp:218] Iteration 70800 (8.53253 iter/s, 23.4397s/200 iters), loss = 0.0250346
I1128 17:40:31.698778 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0250346 (* 1 = 0.0250346 loss)
I1128 17:40:31.698783 26047 sgd_solver.cpp:105] Iteration 70800, lr = 0.001
I1128 17:40:55.025976 26047 solver.cpp:330] Iteration 71000, Testing net (#0)
I1128 17:40:57.190687 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:40:57.281286 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8859
I1128 17:40:57.281307 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.384308 (* 1 = 0.384308 loss)
I1128 17:40:57.400640 26047 solver.cpp:218] Iteration 71000 (7.78183 iter/s, 25.7009s/200 iters), loss = 0.0357355
I1128 17:40:57.400671 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0357355 (* 1 = 0.0357355 loss)
I1128 17:40:57.400691 26047 sgd_solver.cpp:105] Iteration 71000, lr = 0.001
I1128 17:41:07.838809 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:41:20.851636 26047 solver.cpp:218] Iteration 71200 (8.52875 iter/s, 23.4501s/200 iters), loss = 0.0184679
I1128 17:41:20.851666 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0184679 (* 1 = 0.0184679 loss)
I1128 17:41:20.851672 26047 sgd_solver.cpp:105] Iteration 71200, lr = 0.001
I1128 17:41:44.284751 26047 solver.cpp:218] Iteration 71400 (8.53526 iter/s, 23.4322s/200 iters), loss = 0.042322
I1128 17:41:44.284939 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.042322 (* 1 = 0.042322 loss)
I1128 17:41:44.284946 26047 sgd_solver.cpp:105] Iteration 71400, lr = 0.001
I1128 17:41:53.666908 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:42:07.732730 26047 solver.cpp:218] Iteration 71600 (8.5299 iter/s, 23.4469s/200 iters), loss = 0.0387361
I1128 17:42:07.732774 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0387361 (* 1 = 0.0387361 loss)
I1128 17:42:07.732779 26047 sgd_solver.cpp:105] Iteration 71600, lr = 0.001
I1128 17:42:31.439551 26047 solver.cpp:218] Iteration 71800 (8.43672 iter/s, 23.7059s/200 iters), loss = 0.0171949
I1128 17:42:31.439728 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0171949 (* 1 = 0.0171949 loss)
I1128 17:42:31.439736 26047 sgd_solver.cpp:105] Iteration 71800, lr = 0.001
I1128 17:42:39.752996 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:42:54.884728 26047 solver.cpp:330] Iteration 72000, Testing net (#0)
I1128 17:42:57.045502 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:42:57.138504 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8868
I1128 17:42:57.138526 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.383205 (* 1 = 0.383205 loss)
I1128 17:42:57.258713 26047 solver.cpp:218] Iteration 72000 (7.74652 iter/s, 25.818s/200 iters), loss = 0.0318276
I1128 17:42:57.258757 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0318276 (* 1 = 0.0318276 loss)
I1128 17:42:57.258761 26047 sgd_solver.cpp:46] MultiStep Status: Iteration 72000, step = 3
I1128 17:42:57.258764 26047 sgd_solver.cpp:105] Iteration 72000, lr = 0.0001
I1128 17:43:20.753409 26047 solver.cpp:218] Iteration 72200 (8.51289 iter/s, 23.4938s/200 iters), loss = 0.0459992
I1128 17:43:20.753562 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0459992 (* 1 = 0.0459992 loss)
I1128 17:43:20.753569 26047 sgd_solver.cpp:105] Iteration 72200, lr = 0.0001
I1128 17:43:27.921578 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:43:44.340066 26047 solver.cpp:218] Iteration 72400 (8.47973 iter/s, 23.5857s/200 iters), loss = 0.0315002
I1128 17:43:44.340111 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0315002 (* 1 = 0.0315002 loss)
I1128 17:43:44.340116 26047 sgd_solver.cpp:105] Iteration 72400, lr = 0.0001
I1128 17:43:55.973124 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_72500.caffemodel
I1128 17:43:55.991585 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_72500.solverstate
I1128 17:44:07.875949 26047 solver.cpp:218] Iteration 72600 (8.49799 iter/s, 23.535s/200 iters), loss = 0.024798
I1128 17:44:07.875993 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.024798 (* 1 = 0.024798 loss)
I1128 17:44:07.875998 26047 sgd_solver.cpp:105] Iteration 72600, lr = 0.0001
I1128 17:44:13.994222 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:44:31.367171 26047 solver.cpp:218] Iteration 72800 (8.51414 iter/s, 23.4903s/200 iters), loss = 0.0276289
I1128 17:44:31.367276 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0276289 (* 1 = 0.0276289 loss)
I1128 17:44:31.367283 26047 sgd_solver.cpp:105] Iteration 72800, lr = 0.0001
I1128 17:44:54.730185 26047 solver.cpp:330] Iteration 73000, Testing net (#0)
I1128 17:44:56.896396 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:44:56.988087 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8868
I1128 17:44:56.988109 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.38484 (* 1 = 0.38484 loss)
I1128 17:44:57.108391 26047 solver.cpp:218] Iteration 73000 (7.76995 iter/s, 25.7402s/200 iters), loss = 0.0205146
I1128 17:44:57.108433 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0205146 (* 1 = 0.0205146 loss)
I1128 17:44:57.108440 26047 sgd_solver.cpp:105] Iteration 73000, lr = 0.0001
I1128 17:45:02.046550 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:45:20.588989 26047 solver.cpp:218] Iteration 73200 (8.51799 iter/s, 23.4797s/200 iters), loss = 0.0358037
I1128 17:45:20.589032 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0358037 (* 1 = 0.0358037 loss)
I1128 17:45:20.589038 26047 sgd_solver.cpp:105] Iteration 73200, lr = 0.0001
I1128 17:45:44.060797 26047 solver.cpp:218] Iteration 73400 (8.52118 iter/s, 23.4709s/200 iters), loss = 0.0391995
I1128 17:45:44.060925 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0391995 (* 1 = 0.0391995 loss)
I1128 17:45:44.060931 26047 sgd_solver.cpp:105] Iteration 73400, lr = 0.0001
I1128 17:45:47.939648 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:46:07.559229 26047 solver.cpp:218] Iteration 73600 (8.51156 iter/s, 23.4975s/200 iters), loss = 0.0218184
I1128 17:46:07.559274 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0218184 (* 1 = 0.0218184 loss)
I1128 17:46:07.559279 26047 sgd_solver.cpp:105] Iteration 73600, lr = 0.0001
I1128 17:46:31.028021 26047 solver.cpp:218] Iteration 73800 (8.52227 iter/s, 23.4679s/200 iters), loss = 0.0321102
I1128 17:46:31.028076 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0321102 (* 1 = 0.0321102 loss)
I1128 17:46:31.028095 26047 sgd_solver.cpp:105] Iteration 73800, lr = 0.0001
I1128 17:46:33.850064 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:46:54.390559 26047 solver.cpp:330] Iteration 74000, Testing net (#0)
I1128 17:46:56.554430 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:46:56.643461 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8866
I1128 17:46:56.643483 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.385552 (* 1 = 0.385552 loss)
I1128 17:46:56.763444 26047 solver.cpp:218] Iteration 74000 (7.77168 iter/s, 25.7345s/200 iters), loss = 0.0358077
I1128 17:46:56.763475 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0358076 (* 1 = 0.0358076 loss)
I1128 17:46:56.763496 26047 sgd_solver.cpp:105] Iteration 74000, lr = 0.0001
I1128 17:47:20.241077 26047 solver.cpp:218] Iteration 74200 (8.51906 iter/s, 23.4768s/200 iters), loss = 0.0250983
I1128 17:47:20.241214 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0250983 (* 1 = 0.0250983 loss)
I1128 17:47:20.241225 26047 sgd_solver.cpp:105] Iteration 74200, lr = 0.0001
I1128 17:47:21.889462 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:47:43.727919 26047 solver.cpp:218] Iteration 74400 (8.51576 iter/s, 23.4859s/200 iters), loss = 0.0178097
I1128 17:47:43.727962 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0178096 (* 1 = 0.0178096 loss)
I1128 17:47:43.727967 26047 sgd_solver.cpp:105] Iteration 74400, lr = 0.0001
I1128 17:48:07.213897 26047 solver.cpp:218] Iteration 74600 (8.51604 iter/s, 23.4851s/200 iters), loss = 0.0284296
I1128 17:48:07.214051 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0284295 (* 1 = 0.0284295 loss)
I1128 17:48:07.214061 26047 sgd_solver.cpp:105] Iteration 74600, lr = 0.0001
I1128 17:48:07.804385 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:48:30.694164 26047 solver.cpp:218] Iteration 74800 (8.51815 iter/s, 23.4793s/200 iters), loss = 0.0250951
I1128 17:48:30.694208 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0250951 (* 1 = 0.0250951 loss)
I1128 17:48:30.694214 26047 sgd_solver.cpp:105] Iteration 74800, lr = 0.0001
I1128 17:48:53.579159 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:48:54.049250 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_75000.caffemodel
I1128 17:48:54.055912 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_75000.solverstate
I1128 17:48:54.059164 26047 solver.cpp:330] Iteration 75000, Testing net (#0)
I1128 17:48:56.219434 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:48:56.310634 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8869
I1128 17:48:56.310668 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.385732 (* 1 = 0.385732 loss)
I1128 17:48:56.430673 26047 solver.cpp:218] Iteration 75000 (7.77135 iter/s, 25.7356s/200 iters), loss = 0.0118284
I1128 17:48:56.430701 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0118283 (* 1 = 0.0118283 loss)
I1128 17:48:56.430708 26047 sgd_solver.cpp:105] Iteration 75000, lr = 0.0001
I1128 17:49:19.884052 26047 solver.cpp:218] Iteration 75200 (8.52787 iter/s, 23.4525s/200 iters), loss = 0.022961
I1128 17:49:19.884094 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0229609 (* 1 = 0.0229609 loss)
I1128 17:49:19.884099 26047 sgd_solver.cpp:105] Iteration 75200, lr = 0.0001
I1128 17:49:41.702893 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:49:43.344642 26047 solver.cpp:218] Iteration 75400 (8.52525 iter/s, 23.4597s/200 iters), loss = 0.0154308
I1128 17:49:43.344686 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0154307 (* 1 = 0.0154307 loss)
I1128 17:49:43.344691 26047 sgd_solver.cpp:105] Iteration 75400, lr = 0.0001
I1128 17:50:06.795241 26047 solver.cpp:218] Iteration 75600 (8.52888 iter/s, 23.4497s/200 iters), loss = 0.0250497
I1128 17:50:06.795285 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0250497 (* 1 = 0.0250497 loss)
I1128 17:50:06.795290 26047 sgd_solver.cpp:105] Iteration 75600, lr = 0.0001
I1128 17:50:27.567333 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:50:30.266199 26047 solver.cpp:218] Iteration 75800 (8.52148 iter/s, 23.4701s/200 iters), loss = 0.0168615
I1128 17:50:30.266228 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0168614 (* 1 = 0.0168614 loss)
I1128 17:50:30.266233 26047 sgd_solver.cpp:105] Iteration 75800, lr = 0.0001
I1128 17:50:53.613431 26047 solver.cpp:330] Iteration 76000, Testing net (#0)
I1128 17:50:55.768231 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:50:55.860088 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8866
I1128 17:50:55.860124 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386014 (* 1 = 0.386014 loss)
I1128 17:50:55.980265 26047 solver.cpp:218] Iteration 76000 (7.77812 iter/s, 25.7131s/200 iters), loss = 0.0136306
I1128 17:50:55.980294 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0136305 (* 1 = 0.0136305 loss)
I1128 17:50:55.980300 26047 sgd_solver.cpp:105] Iteration 76000, lr = 0.0001
I1128 17:51:15.568184 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:51:19.437633 26047 solver.cpp:218] Iteration 76200 (8.52641 iter/s, 23.4565s/200 iters), loss = 0.0202965
I1128 17:51:19.437675 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0202964 (* 1 = 0.0202964 loss)
I1128 17:51:19.437681 26047 sgd_solver.cpp:105] Iteration 76200, lr = 0.0001
I1128 17:51:42.888715 26047 solver.cpp:218] Iteration 76400 (8.5287 iter/s, 23.4502s/200 iters), loss = 0.0352518
I1128 17:51:42.888758 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0352517 (* 1 = 0.0352517 loss)
I1128 17:51:42.888764 26047 sgd_solver.cpp:105] Iteration 76400, lr = 0.0001
I1128 17:52:01.426336 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:52:06.356266 26047 solver.cpp:218] Iteration 76600 (8.52272 iter/s, 23.4667s/200 iters), loss = 0.0155601
I1128 17:52:06.356297 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.01556 (* 1 = 0.01556 loss)
I1128 17:52:06.356317 26047 sgd_solver.cpp:105] Iteration 76600, lr = 0.0001
I1128 17:52:29.796543 26047 solver.cpp:218] Iteration 76800 (8.53263 iter/s, 23.4394s/200 iters), loss = 0.0302799
I1128 17:52:29.796571 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0302798 (* 1 = 0.0302798 loss)
I1128 17:52:29.796591 26047 sgd_solver.cpp:105] Iteration 76800, lr = 0.0001
I1128 17:52:47.278143 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:52:53.147460 26047 solver.cpp:330] Iteration 77000, Testing net (#0)
I1128 17:52:55.307400 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:52:55.401015 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8864
I1128 17:52:55.401051 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386022 (* 1 = 0.386022 loss)
I1128 17:52:55.521445 26047 solver.cpp:218] Iteration 77000 (7.77485 iter/s, 25.724s/200 iters), loss = 0.0267588
I1128 17:52:55.521476 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0267588 (* 1 = 0.0267588 loss)
I1128 17:52:55.521481 26047 sgd_solver.cpp:105] Iteration 77000, lr = 0.0001
I1128 17:53:18.971882 26047 solver.cpp:218] Iteration 77200 (8.52893 iter/s, 23.4496s/200 iters), loss = 0.0271349
I1128 17:53:18.972021 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0271349 (* 1 = 0.0271349 loss)
I1128 17:53:18.972029 26047 sgd_solver.cpp:105] Iteration 77200, lr = 0.0001
I1128 17:53:35.284582 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:53:42.446557 26047 solver.cpp:218] Iteration 77400 (8.52016 iter/s, 23.4737s/200 iters), loss = 0.026351
I1128 17:53:42.446600 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0263509 (* 1 = 0.0263509 loss)
I1128 17:53:42.446605 26047 sgd_solver.cpp:105] Iteration 77400, lr = 0.0001
I1128 17:53:54.152673 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_77500.caffemodel
I1128 17:53:54.238276 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_77500.solverstate
I1128 17:54:06.472470 26047 solver.cpp:218] Iteration 77600 (8.32465 iter/s, 24.025s/200 iters), loss = 0.0325027
I1128 17:54:06.472522 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0325027 (* 1 = 0.0325027 loss)
I1128 17:54:06.472529 26047 sgd_solver.cpp:105] Iteration 77600, lr = 0.0001
I1128 17:54:22.054709 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:54:30.424492 26047 solver.cpp:218] Iteration 77800 (8.35039 iter/s, 23.951s/200 iters), loss = 0.01661
I1128 17:54:30.424674 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.01661 (* 1 = 0.01661 loss)
I1128 17:54:30.424684 26047 sgd_solver.cpp:105] Iteration 77800, lr = 0.0001
I1128 17:54:53.950726 26047 solver.cpp:330] Iteration 78000, Testing net (#0)
I1128 17:54:56.115690 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:54:56.206290 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8867
I1128 17:54:56.206311 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386109 (* 1 = 0.386109 loss)
I1128 17:54:56.326118 26047 solver.cpp:218] Iteration 78000 (7.72183 iter/s, 25.9006s/200 iters), loss = 0.0238398
I1128 17:54:56.326148 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0238397 (* 1 = 0.0238397 loss)
I1128 17:54:56.326156 26047 sgd_solver.cpp:105] Iteration 78000, lr = 0.0001
I1128 17:55:10.391683 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:55:19.761543 26047 solver.cpp:218] Iteration 78200 (8.53439 iter/s, 23.4346s/200 iters), loss = 0.0387729
I1128 17:55:19.761575 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0387729 (* 1 = 0.0387729 loss)
I1128 17:55:19.761595 26047 sgd_solver.cpp:105] Iteration 78200, lr = 0.0001
I1128 17:55:43.195746 26047 solver.cpp:218] Iteration 78400 (8.53484 iter/s, 23.4334s/200 iters), loss = 0.0220812
I1128 17:55:43.195925 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0220812 (* 1 = 0.0220812 loss)
I1128 17:55:43.195937 26047 sgd_solver.cpp:105] Iteration 78400, lr = 0.0001
I1128 17:55:56.207823 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:56:06.631736 26047 solver.cpp:218] Iteration 78600 (8.53424 iter/s, 23.435s/200 iters), loss = 0.024062
I1128 17:56:06.631768 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.024062 (* 1 = 0.024062 loss)
I1128 17:56:06.631790 26047 sgd_solver.cpp:105] Iteration 78600, lr = 0.0001
I1128 17:56:30.062381 26047 solver.cpp:218] Iteration 78800 (8.53614 iter/s, 23.4298s/200 iters), loss = 0.0261973
I1128 17:56:30.062477 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0261973 (* 1 = 0.0261973 loss)
I1128 17:56:30.062502 26047 sgd_solver.cpp:105] Iteration 78800, lr = 0.0001
I1128 17:56:42.016737 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:56:53.381714 26047 solver.cpp:330] Iteration 79000, Testing net (#0)
I1128 17:56:55.548656 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:56:55.641543 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8866
I1128 17:56:55.641567 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.385897 (* 1 = 0.385897 loss)
I1128 17:56:55.762171 26047 solver.cpp:218] Iteration 79000 (7.78246 iter/s, 25.6988s/200 iters), loss = 0.0187733
I1128 17:56:55.762202 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0187732 (* 1 = 0.0187732 loss)
I1128 17:56:55.762209 26047 sgd_solver.cpp:105] Iteration 79000, lr = 0.0001
I1128 17:57:19.214329 26047 solver.cpp:218] Iteration 79200 (8.5283 iter/s, 23.4513s/200 iters), loss = 0.0199101
I1128 17:57:19.214465 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0199101 (* 1 = 0.0199101 loss)
I1128 17:57:19.214473 26047 sgd_solver.cpp:105] Iteration 79200, lr = 0.0001
I1128 17:57:29.995206 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:57:42.647847 26047 solver.cpp:218] Iteration 79400 (8.53512 iter/s, 23.4326s/200 iters), loss = 0.0336371
I1128 17:57:42.647879 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.033637 (* 1 = 0.033637 loss)
I1128 17:57:42.647900 26047 sgd_solver.cpp:105] Iteration 79400, lr = 0.0001
I1128 17:58:06.084164 26047 solver.cpp:218] Iteration 79600 (8.53407 iter/s, 23.4355s/200 iters), loss = 0.031219
I1128 17:58:06.084290 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0312189 (* 1 = 0.0312189 loss)
I1128 17:58:06.084301 26047 sgd_solver.cpp:105] Iteration 79600, lr = 0.0001
I1128 17:58:15.814414 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:58:29.526845 26047 solver.cpp:218] Iteration 79800 (8.53179 iter/s, 23.4418s/200 iters), loss = 0.0296325
I1128 17:58:29.526875 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0296324 (* 1 = 0.0296324 loss)
I1128 17:58:29.526883 26047 sgd_solver.cpp:105] Iteration 79800, lr = 0.0001
I1128 17:58:52.849947 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_80000.caffemodel
I1128 17:58:52.880543 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_80000.solverstate
I1128 17:58:52.885251 26047 solver.cpp:330] Iteration 80000, Testing net (#0)
I1128 17:58:55.037384 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:58:55.131697 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8865
I1128 17:58:55.131721 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.385835 (* 1 = 0.385835 loss)
I1128 17:58:55.251299 26047 solver.cpp:218] Iteration 80000 (7.77498 iter/s, 25.7235s/200 iters), loss = 0.0275547
I1128 17:58:55.251330 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0275546 (* 1 = 0.0275546 loss)
I1128 17:58:55.251349 26047 sgd_solver.cpp:105] Iteration 80000, lr = 0.0001
I1128 17:59:03.932510 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:59:18.710433 26047 solver.cpp:218] Iteration 80200 (8.52577 iter/s, 23.4583s/200 iters), loss = 0.0367073
I1128 17:59:18.710476 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0367073 (* 1 = 0.0367073 loss)
I1128 17:59:18.710481 26047 sgd_solver.cpp:105] Iteration 80200, lr = 0.0001
I1128 17:59:42.160729 26047 solver.cpp:218] Iteration 80400 (8.52898 iter/s, 23.4495s/200 iters), loss = 0.0257587
I1128 17:59:42.160840 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0257587 (* 1 = 0.0257587 loss)
I1128 17:59:42.160861 26047 sgd_solver.cpp:105] Iteration 80400, lr = 0.0001
I1128 17:59:49.669584 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:00:05.623261 26047 solver.cpp:218] Iteration 80600 (8.52456 iter/s, 23.4616s/200 iters), loss = 0.0102432
I1128 18:00:05.623291 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0102432 (* 1 = 0.0102432 loss)
I1128 18:00:05.623296 26047 sgd_solver.cpp:105] Iteration 80600, lr = 0.0001
I1128 18:00:29.077955 26047 solver.cpp:218] Iteration 80800 (8.52772 iter/s, 23.4529s/200 iters), loss = 0.0276833
I1128 18:00:29.078033 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0276833 (* 1 = 0.0276833 loss)
I1128 18:00:29.078039 26047 sgd_solver.cpp:105] Iteration 80800, lr = 0.0001
I1128 18:00:35.531569 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:00:52.411671 26047 solver.cpp:330] Iteration 81000, Testing net (#0)
I1128 18:00:54.580251 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:00:54.673502 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8864
I1128 18:00:54.673524 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.385838 (* 1 = 0.385838 loss)
I1128 18:00:54.793393 26047 solver.cpp:218] Iteration 81000 (7.7787 iter/s, 25.7112s/200 iters), loss = 0.0194267
I1128 18:00:54.793424 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0194267 (* 1 = 0.0194267 loss)
I1128 18:00:54.793431 26047 sgd_solver.cpp:105] Iteration 81000, lr = 0.0001
I1128 18:01:18.263555 26047 solver.cpp:218] Iteration 81200 (8.52279 iter/s, 23.4665s/200 iters), loss = 0.0362996
I1128 18:01:18.263674 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0362995 (* 1 = 0.0362995 loss)
I1128 18:01:18.263681 26047 sgd_solver.cpp:105] Iteration 81200, lr = 0.0001
I1128 18:01:23.548836 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:01:41.752396 26047 solver.cpp:218] Iteration 81400 (8.51601 iter/s, 23.4852s/200 iters), loss = 0.0361862
I1128 18:01:41.752441 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0361862 (* 1 = 0.0361862 loss)
I1128 18:01:41.752447 26047 sgd_solver.cpp:105] Iteration 81400, lr = 0.0001
I1128 18:02:05.313694 26047 solver.cpp:218] Iteration 81600 (8.48975 iter/s, 23.5578s/200 iters), loss = 0.0210744
I1128 18:02:05.313809 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0210744 (* 1 = 0.0210744 loss)
I1128 18:02:05.313832 26047 sgd_solver.cpp:105] Iteration 81600, lr = 0.0001
I1128 18:02:09.546205 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:02:28.814551 26047 solver.cpp:218] Iteration 81800 (8.51157 iter/s, 23.4974s/200 iters), loss = 0.0491696
I1128 18:02:28.814581 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0491695 (* 1 = 0.0491695 loss)
I1128 18:02:28.814602 26047 sgd_solver.cpp:105] Iteration 81800, lr = 0.0001
I1128 18:02:52.186720 26047 solver.cpp:330] Iteration 82000, Testing net (#0)
I1128 18:02:54.349611 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:02:54.439728 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8871
I1128 18:02:54.439751 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.385798 (* 1 = 0.385798 loss)
I1128 18:02:54.559794 26047 solver.cpp:218] Iteration 82000 (7.7695 iter/s, 25.7417s/200 iters), loss = 0.0265472
I1128 18:02:54.559840 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0265471 (* 1 = 0.0265471 loss)
I1128 18:02:54.559846 26047 sgd_solver.cpp:105] Iteration 82000, lr = 0.0001
I1128 18:02:57.734638 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:03:18.047178 26047 solver.cpp:218] Iteration 82200 (8.51636 iter/s, 23.4842s/200 iters), loss = 0.0149312
I1128 18:03:18.047220 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0149312 (* 1 = 0.0149312 loss)
I1128 18:03:18.047226 26047 sgd_solver.cpp:105] Iteration 82200, lr = 0.0001
I1128 18:03:41.533385 26047 solver.cpp:218] Iteration 82400 (8.51675 iter/s, 23.4831s/200 iters), loss = 0.0110889
I1128 18:03:41.533464 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0110889 (* 1 = 0.0110889 loss)
I1128 18:03:41.533470 26047 sgd_solver.cpp:105] Iteration 82400, lr = 0.0001
I1128 18:03:43.535313 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:03:53.168659 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_82500.caffemodel
I1128 18:03:53.175356 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_82500.solverstate
I1128 18:04:05.070382 26047 solver.cpp:218] Iteration 82600 (8.49836 iter/s, 23.534s/200 iters), loss = 0.0168053
I1128 18:04:05.070425 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0168053 (* 1 = 0.0168053 loss)
I1128 18:04:05.070431 26047 sgd_solver.cpp:105] Iteration 82600, lr = 0.0001
I1128 18:04:28.827491 26047 solver.cpp:218] Iteration 82800 (8.41958 iter/s, 23.7542s/200 iters), loss = 0.0198915
I1128 18:04:28.827636 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0198915 (* 1 = 0.0198915 loss)
I1128 18:04:28.827646 26047 sgd_solver.cpp:105] Iteration 82800, lr = 0.0001
I1128 18:04:29.774329 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:04:52.773978 26047 solver.cpp:330] Iteration 83000, Testing net (#0)
I1128 18:04:55.044015 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:04:55.137660 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8872
I1128 18:04:55.137683 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.385956 (* 1 = 0.385956 loss)
I1128 18:04:55.258020 26047 solver.cpp:218] Iteration 83000 (7.56795 iter/s, 26.4272s/200 iters), loss = 0.0149849
I1128 18:04:55.258051 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0149848 (* 1 = 0.0149848 loss)
I1128 18:04:55.258057 26047 sgd_solver.cpp:105] Iteration 83000, lr = 0.0001
I1128 18:05:18.916501 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:05:19.036581 26047 solver.cpp:218] Iteration 83200 (8.41192 iter/s, 23.7758s/200 iters), loss = 0.0121546
I1128 18:05:19.036617 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0121546 (* 1 = 0.0121546 loss)
I1128 18:05:19.036623 26047 sgd_solver.cpp:105] Iteration 83200, lr = 0.0001
I1128 18:05:43.015276 26047 solver.cpp:218] Iteration 83400 (8.34169 iter/s, 23.976s/200 iters), loss = 0.031229
I1128 18:05:43.015321 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.031229 (* 1 = 0.031229 loss)
I1128 18:05:43.015327 26047 sgd_solver.cpp:105] Iteration 83400, lr = 0.0001
I1128 18:06:05.564761 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:06:06.873683 26047 solver.cpp:218] Iteration 83600 (8.38373 iter/s, 23.8557s/200 iters), loss = 0.037539
I1128 18:06:06.873723 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0375389 (* 1 = 0.0375389 loss)
I1128 18:06:06.873728 26047 sgd_solver.cpp:105] Iteration 83600, lr = 0.0001
I1128 18:06:30.925611 26047 solver.cpp:218] Iteration 83800 (8.31631 iter/s, 24.0491s/200 iters), loss = 0.0482983
I1128 18:06:30.925662 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0482982 (* 1 = 0.0482982 loss)
I1128 18:06:30.925669 26047 sgd_solver.cpp:105] Iteration 83800, lr = 0.0001
I1128 18:06:52.547703 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:06:54.822579 26047 solver.cpp:330] Iteration 84000, Testing net (#0)
I1128 18:06:57.107009 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:06:57.204354 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8871
I1128 18:06:57.204380 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386074 (* 1 = 0.386074 loss)
I1128 18:06:57.334738 26047 solver.cpp:218] Iteration 84000 (7.57396 iter/s, 26.4063s/200 iters), loss = 0.0166263
I1128 18:06:57.334810 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0166263 (* 1 = 0.0166263 loss)
I1128 18:06:57.334818 26047 sgd_solver.cpp:105] Iteration 84000, lr = 0.0001
I1128 18:07:21.316754 26047 solver.cpp:218] Iteration 84200 (8.34046 iter/s, 23.9795s/200 iters), loss = 0.0105874
I1128 18:07:21.316805 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0105874 (* 1 = 0.0105874 loss)
I1128 18:07:21.316812 26047 sgd_solver.cpp:105] Iteration 84200, lr = 0.0001
I1128 18:07:41.686635 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:07:45.290119 26047 solver.cpp:218] Iteration 84400 (8.34343 iter/s, 23.9709s/200 iters), loss = 0.0162945
I1128 18:07:45.290164 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0162945 (* 1 = 0.0162945 loss)
I1128 18:07:45.290170 26047 sgd_solver.cpp:105] Iteration 84400, lr = 0.0001
I1128 18:08:09.294392 26047 solver.cpp:218] Iteration 84600 (8.33267 iter/s, 24.0019s/200 iters), loss = 0.0335504
I1128 18:08:09.294443 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0335503 (* 1 = 0.0335503 loss)
I1128 18:08:09.294450 26047 sgd_solver.cpp:105] Iteration 84600, lr = 0.0001
I1128 18:08:28.734813 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:08:33.348139 26047 solver.cpp:218] Iteration 84800 (8.31551 iter/s, 24.0514s/200 iters), loss = 0.0394488
I1128 18:08:33.348182 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0394488 (* 1 = 0.0394488 loss)
I1128 18:08:33.348188 26047 sgd_solver.cpp:105] Iteration 84800, lr = 0.0001
I1128 18:08:57.183046 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_85000.caffemodel
I1128 18:08:57.189760 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_85000.solverstate
I1128 18:08:57.193102 26047 solver.cpp:330] Iteration 85000, Testing net (#0)
I1128 18:08:59.518347 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:08:59.619200 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8878
I1128 18:08:59.619225 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386037 (* 1 = 0.386037 loss)
I1128 18:08:59.741567 26047 solver.cpp:218] Iteration 85000 (7.57835 iter/s, 26.391s/200 iters), loss = 0.0484568
I1128 18:08:59.741612 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0484567 (* 1 = 0.0484567 loss)
I1128 18:08:59.741619 26047 sgd_solver.cpp:105] Iteration 85000, lr = 0.0001
I1128 18:09:17.839375 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:09:23.624855 26047 solver.cpp:218] Iteration 85200 (8.37483 iter/s, 23.8811s/200 iters), loss = 0.0213309
I1128 18:09:23.624907 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0213308 (* 1 = 0.0213308 loss)
I1128 18:09:23.624914 26047 sgd_solver.cpp:105] Iteration 85200, lr = 0.0001
I1128 18:09:47.626739 26047 solver.cpp:218] Iteration 85400 (8.33343 iter/s, 23.9997s/200 iters), loss = 0.0171461
I1128 18:09:47.626829 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0171461 (* 1 = 0.0171461 loss)
I1128 18:09:47.626849 26047 sgd_solver.cpp:105] Iteration 85400, lr = 0.0001
I1128 18:10:04.426967 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:10:11.234210 26047 solver.cpp:218] Iteration 85600 (8.47266 iter/s, 23.6053s/200 iters), loss = 0.0529669
I1128 18:10:11.234253 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0529668 (* 1 = 0.0529668 loss)
I1128 18:10:11.234258 26047 sgd_solver.cpp:105] Iteration 85600, lr = 0.0001
I1128 18:10:34.709044 26047 solver.cpp:218] Iteration 85800 (8.5205 iter/s, 23.4728s/200 iters), loss = 0.0307627
I1128 18:10:34.709132 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0307627 (* 1 = 0.0307627 loss)
I1128 18:10:34.709139 26047 sgd_solver.cpp:105] Iteration 85800, lr = 0.0001
I1128 18:10:50.349019 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:10:58.122277 26047 solver.cpp:330] Iteration 86000, Testing net (#0)
I1128 18:11:00.398958 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:11:00.494992 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.888
I1128 18:11:00.495028 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386169 (* 1 = 0.386169 loss)
I1128 18:11:00.615362 26047 solver.cpp:218] Iteration 86000 (7.72079 iter/s, 25.9041s/200 iters), loss = 0.0307222
I1128 18:11:00.615411 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0307222 (* 1 = 0.0307222 loss)
I1128 18:11:00.615417 26047 sgd_solver.cpp:105] Iteration 86000, lr = 0.0001
I1128 18:11:24.351544 26047 solver.cpp:218] Iteration 86200 (8.42666 iter/s, 23.7342s/200 iters), loss = 0.0309253
I1128 18:11:24.351657 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0309253 (* 1 = 0.0309253 loss)
I1128 18:11:24.351677 26047 sgd_solver.cpp:105] Iteration 86200, lr = 0.0001
I1128 18:11:39.043893 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:11:47.958015 26047 solver.cpp:218] Iteration 86400 (8.47297 iter/s, 23.6045s/200 iters), loss = 0.0231779
I1128 18:11:47.958058 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0231778 (* 1 = 0.0231778 loss)
I1128 18:11:47.958065 26047 sgd_solver.cpp:105] Iteration 86400, lr = 0.0001
I1128 18:12:11.426914 26047 solver.cpp:218] Iteration 86600 (8.5226 iter/s, 23.467s/200 iters), loss = 0.0198182
I1128 18:12:11.426975 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0198182 (* 1 = 0.0198182 loss)
I1128 18:12:11.426981 26047 sgd_solver.cpp:105] Iteration 86600, lr = 0.0001
I1128 18:12:24.807371 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:12:34.898205 26047 solver.cpp:218] Iteration 86800 (8.52173 iter/s, 23.4694s/200 iters), loss = 0.0294034
I1128 18:12:34.898234 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0294033 (* 1 = 0.0294033 loss)
I1128 18:12:34.898239 26047 sgd_solver.cpp:105] Iteration 86800, lr = 0.0001
I1128 18:12:58.244587 26047 solver.cpp:330] Iteration 87000, Testing net (#0)
I1128 18:13:00.506868 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:13:00.602561 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8875
I1128 18:13:00.602583 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386157 (* 1 = 0.386157 loss)
I1128 18:13:00.722911 26047 solver.cpp:218] Iteration 87000 (7.74512 iter/s, 25.8227s/200 iters), loss = 0.0273669
I1128 18:13:00.722941 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0273669 (* 1 = 0.0273669 loss)
I1128 18:13:00.722946 26047 sgd_solver.cpp:105] Iteration 87000, lr = 0.0001
I1128 18:13:13.048271 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:13:24.192729 26047 solver.cpp:218] Iteration 87200 (8.52223 iter/s, 23.468s/200 iters), loss = 0.033533
I1128 18:13:24.192772 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.033533 (* 1 = 0.033533 loss)
I1128 18:13:24.192777 26047 sgd_solver.cpp:105] Iteration 87200, lr = 0.0001
I1128 18:13:47.641944 26047 solver.cpp:218] Iteration 87400 (8.52971 iter/s, 23.4475s/200 iters), loss = 0.0262459
I1128 18:13:47.642057 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0262458 (* 1 = 0.0262458 loss)
I1128 18:13:47.642063 26047 sgd_solver.cpp:105] Iteration 87400, lr = 0.0001
I1128 18:13:58.778760 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:13:59.246904 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_87500.caffemodel
I1128 18:13:59.253640 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_87500.solverstate
I1128 18:14:11.167971 26047 solver.cpp:218] Iteration 87600 (8.50188 iter/s, 23.5242s/200 iters), loss = 0.0159305
I1128 18:14:11.168014 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0159304 (* 1 = 0.0159304 loss)
I1128 18:14:11.168020 26047 sgd_solver.cpp:105] Iteration 87600, lr = 0.0001
I1128 18:14:34.628546 26047 solver.cpp:218] Iteration 87800 (8.52556 iter/s, 23.4589s/200 iters), loss = 0.0260546
I1128 18:14:34.628685 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0260546 (* 1 = 0.0260546 loss)
I1128 18:14:34.628692 26047 sgd_solver.cpp:105] Iteration 87800, lr = 0.0001
I1128 18:14:44.727715 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:14:58.012843 26047 solver.cpp:330] Iteration 88000, Testing net (#0)
I1128 18:15:00.281661 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:15:00.376926 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8874
I1128 18:15:00.376947 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386126 (* 1 = 0.386126 loss)
I1128 18:15:00.497898 26047 solver.cpp:218] Iteration 88000 (7.73174 iter/s, 25.8674s/200 iters), loss = 0.0318651
I1128 18:15:00.497941 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0318651 (* 1 = 0.0318651 loss)
I1128 18:15:00.497946 26047 sgd_solver.cpp:105] Iteration 88000, lr = 0.0001
I1128 18:15:23.955459 26047 solver.cpp:218] Iteration 88200 (8.52663 iter/s, 23.4559s/200 iters), loss = 0.0454516
I1128 18:15:23.955545 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0454516 (* 1 = 0.0454516 loss)
I1128 18:15:23.955551 26047 sgd_solver.cpp:105] Iteration 88200, lr = 0.0001
I1128 18:15:32.995581 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:15:47.408069 26047 solver.cpp:218] Iteration 88400 (8.52845 iter/s, 23.4509s/200 iters), loss = 0.042085
I1128 18:15:47.408113 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.042085 (* 1 = 0.042085 loss)
I1128 18:15:47.408119 26047 sgd_solver.cpp:105] Iteration 88400, lr = 0.0001
I1128 18:16:10.859977 26047 solver.cpp:218] Iteration 88600 (8.52868 iter/s, 23.4503s/200 iters), loss = 0.0188659
I1128 18:16:10.860124 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0188659 (* 1 = 0.0188659 loss)
I1128 18:16:10.860132 26047 sgd_solver.cpp:105] Iteration 88600, lr = 0.0001
I1128 18:16:18.726598 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:16:34.312783 26047 solver.cpp:218] Iteration 88800 (8.52838 iter/s, 23.4511s/200 iters), loss = 0.0273541
I1128 18:16:34.312824 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0273541 (* 1 = 0.0273541 loss)
I1128 18:16:34.312830 26047 sgd_solver.cpp:105] Iteration 88800, lr = 0.0001
I1128 18:16:57.758405 26047 solver.cpp:330] Iteration 89000, Testing net (#0)
I1128 18:17:00.024430 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:17:00.120649 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8878
I1128 18:17:00.120674 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386106 (* 1 = 0.386106 loss)
I1128 18:17:00.241092 26047 solver.cpp:218] Iteration 89000 (7.71409 iter/s, 25.9266s/200 iters), loss = 0.0180645
I1128 18:17:00.241124 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0180645 (* 1 = 0.0180645 loss)
I1128 18:17:00.241145 26047 sgd_solver.cpp:105] Iteration 89000, lr = 0.0001
I1128 18:17:07.056630 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:17:23.710669 26047 solver.cpp:218] Iteration 89200 (8.52223 iter/s, 23.468s/200 iters), loss = 0.0522991
I1128 18:17:23.710712 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0522991 (* 1 = 0.0522991 loss)
I1128 18:17:23.710717 26047 sgd_solver.cpp:105] Iteration 89200, lr = 0.0001
I1128 18:17:47.174013 26047 solver.cpp:218] Iteration 89400 (8.52449 iter/s, 23.4618s/200 iters), loss = 0.0215201
I1128 18:17:47.174160 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0215201 (* 1 = 0.0215201 loss)
I1128 18:17:47.174168 26047 sgd_solver.cpp:105] Iteration 89400, lr = 0.0001
I1128 18:17:52.928306 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:18:10.647167 26047 solver.cpp:218] Iteration 89600 (8.52096 iter/s, 23.4715s/200 iters), loss = 0.0465665
I1128 18:18:10.647212 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0465665 (* 1 = 0.0465665 loss)
I1128 18:18:10.647217 26047 sgd_solver.cpp:105] Iteration 89600, lr = 0.0001
I1128 18:18:34.125396 26047 solver.cpp:218] Iteration 89800 (8.51908 iter/s, 23.4767s/200 iters), loss = 0.016848
I1128 18:18:34.125510 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.016848 (* 1 = 0.016848 loss)
I1128 18:18:34.125517 26047 sgd_solver.cpp:105] Iteration 89800, lr = 0.0001
I1128 18:18:38.709877 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:18:57.500250 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_90000.caffemodel
I1128 18:18:57.506893 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_90000.solverstate
I1128 18:18:57.510282 26047 solver.cpp:330] Iteration 90000, Testing net (#0)
I1128 18:18:59.778017 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:18:59.873524 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8881
I1128 18:18:59.873548 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386231 (* 1 = 0.386231 loss)
I1128 18:18:59.994912 26047 solver.cpp:218] Iteration 90000 (7.73162 iter/s, 25.8678s/200 iters), loss = 0.0347982
I1128 18:18:59.994945 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0347982 (* 1 = 0.0347982 loss)
I1128 18:18:59.994949 26047 sgd_solver.cpp:46] MultiStep Status: Iteration 90000, step = 4
I1128 18:18:59.994968 26047 sgd_solver.cpp:105] Iteration 90000, lr = 1e-05
I1128 18:19:23.512420 26047 solver.cpp:218] Iteration 90200 (8.50484 iter/s, 23.516s/200 iters), loss = 0.0533369
I1128 18:19:23.512548 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0533369 (* 1 = 0.0533369 loss)
I1128 18:19:23.512555 26047 sgd_solver.cpp:105] Iteration 90200, lr = 1e-05
I1128 18:19:27.036783 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:19:46.996006 26047 solver.cpp:218] Iteration 90400 (8.51715 iter/s, 23.482s/200 iters), loss = 0.040458
I1128 18:19:46.996048 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0404579 (* 1 = 0.0404579 loss)
I1128 18:19:46.996053 26047 sgd_solver.cpp:105] Iteration 90400, lr = 1e-05
I1128 18:20:10.530767 26047 solver.cpp:218] Iteration 90600 (8.4986 iter/s, 23.5333s/200 iters), loss = 0.0205922
I1128 18:20:10.530921 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0205922 (* 1 = 0.0205922 loss)
I1128 18:20:10.530928 26047 sgd_solver.cpp:105] Iteration 90600, lr = 1e-05
I1128 18:20:12.920898 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:20:34.287137 26047 solver.cpp:218] Iteration 90800 (8.41935 iter/s, 23.7548s/200 iters), loss = 0.0375403
I1128 18:20:34.287164 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0375402 (* 1 = 0.0375402 loss)
I1128 18:20:34.287169 26047 sgd_solver.cpp:105] Iteration 90800, lr = 1e-05
I1128 18:20:57.815430 26047 solver.cpp:330] Iteration 91000, Testing net (#0)
I1128 18:21:00.078167 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:21:00.173080 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8878
I1128 18:21:00.173115 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386292 (* 1 = 0.386292 loss)
I1128 18:21:00.293517 26047 solver.cpp:218] Iteration 91000 (7.69088 iter/s, 26.0048s/200 iters), loss = 0.0269982
I1128 18:21:00.293547 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0269981 (* 1 = 0.0269981 loss)
I1128 18:21:00.293552 26047 sgd_solver.cpp:105] Iteration 91000, lr = 1e-05
I1128 18:21:01.586001 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:21:23.724606 26047 solver.cpp:218] Iteration 91200 (8.53618 iter/s, 23.4297s/200 iters), loss = 0.0337136
I1128 18:21:23.724649 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0337135 (* 1 = 0.0337135 loss)
I1128 18:21:23.724655 26047 sgd_solver.cpp:105] Iteration 91200, lr = 1e-05
I1128 18:21:47.158622 26047 solver.cpp:218] Iteration 91400 (8.53511 iter/s, 23.4326s/200 iters), loss = 0.0158907
I1128 18:21:47.158738 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0158907 (* 1 = 0.0158907 loss)
I1128 18:21:47.158746 26047 sgd_solver.cpp:105] Iteration 91400, lr = 1e-05
I1128 18:21:47.397244 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:22:10.590183 26047 solver.cpp:218] Iteration 91600 (8.53603 iter/s, 23.4301s/200 iters), loss = 0.0285284
I1128 18:22:10.590226 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0285283 (* 1 = 0.0285283 loss)
I1128 18:22:10.590232 26047 sgd_solver.cpp:105] Iteration 91600, lr = 1e-05
I1128 18:22:33.087335 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:22:34.026842 26047 solver.cpp:218] Iteration 91800 (8.53415 iter/s, 23.4353s/200 iters), loss = 0.0310613
I1128 18:22:34.026871 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0310613 (* 1 = 0.0310613 loss)
I1128 18:22:34.026892 26047 sgd_solver.cpp:105] Iteration 91800, lr = 1e-05
I1128 18:22:57.340512 26047 solver.cpp:330] Iteration 92000, Testing net (#0)
I1128 18:22:59.600656 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:22:59.695976 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8877
I1128 18:22:59.696012 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386364 (* 1 = 0.386364 loss)
I1128 18:22:59.816951 26047 solver.cpp:218] Iteration 92000 (7.75536 iter/s, 25.7886s/200 iters), loss = 0.0235337
I1128 18:22:59.816980 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0235337 (* 1 = 0.0235337 loss)
I1128 18:22:59.816985 26047 sgd_solver.cpp:105] Iteration 92000, lr = 1e-05
I1128 18:23:21.260360 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:23:23.253288 26047 solver.cpp:218] Iteration 92200 (8.53425 iter/s, 23.435s/200 iters), loss = 0.026882
I1128 18:23:23.253331 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.026882 (* 1 = 0.026882 loss)
I1128 18:23:23.253336 26047 sgd_solver.cpp:105] Iteration 92200, lr = 1e-05
I1128 18:23:46.697597 26047 solver.cpp:218] Iteration 92400 (8.53135 iter/s, 23.443s/200 iters), loss = 0.0351054
I1128 18:23:46.697639 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0351054 (* 1 = 0.0351054 loss)
I1128 18:23:46.697644 26047 sgd_solver.cpp:105] Iteration 92400, lr = 1e-05
I1128 18:23:58.310325 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_92500.caffemodel
I1128 18:23:58.325914 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_92500.solverstate
I1128 18:24:07.136742 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:24:10.188773 26047 solver.cpp:218] Iteration 92600 (8.51432 iter/s, 23.4898s/200 iters), loss = 0.0310697
I1128 18:24:10.188817 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0310696 (* 1 = 0.0310696 loss)
I1128 18:24:10.188824 26047 sgd_solver.cpp:105] Iteration 92600, lr = 1e-05
I1128 18:24:33.636131 26047 solver.cpp:218] Iteration 92800 (8.53023 iter/s, 23.446s/200 iters), loss = 0.0295172
I1128 18:24:33.636209 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0295172 (* 1 = 0.0295172 loss)
I1128 18:24:33.636214 26047 sgd_solver.cpp:105] Iteration 92800, lr = 1e-05
I1128 18:24:52.863050 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:24:56.971109 26047 solver.cpp:330] Iteration 93000, Testing net (#0)
I1128 18:24:59.236449 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:24:59.332079 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8873
I1128 18:24:59.332118 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386346 (* 1 = 0.386346 loss)
I1128 18:24:59.452342 26047 solver.cpp:218] Iteration 93000 (7.74752 iter/s, 25.8147s/200 iters), loss = 0.0209007
I1128 18:24:59.452370 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0209006 (* 1 = 0.0209006 loss)
I1128 18:24:59.452375 26047 sgd_solver.cpp:105] Iteration 93000, lr = 1e-05
I1128 18:25:22.881330 26047 solver.cpp:218] Iteration 93200 (8.53691 iter/s, 23.4277s/200 iters), loss = 0.0273357
I1128 18:25:22.881464 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0273357 (* 1 = 0.0273357 loss)
I1128 18:25:22.881472 26047 sgd_solver.cpp:105] Iteration 93200, lr = 1e-05
I1128 18:25:41.055701 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:25:46.332401 26047 solver.cpp:218] Iteration 93400 (8.52891 iter/s, 23.4497s/200 iters), loss = 0.0243015
I1128 18:25:46.332444 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0243015 (* 1 = 0.0243015 loss)
I1128 18:25:46.332449 26047 sgd_solver.cpp:105] Iteration 93400, lr = 1e-05
I1128 18:26:09.772650 26047 solver.cpp:218] Iteration 93600 (8.53281 iter/s, 23.4389s/200 iters), loss = 0.0252637
I1128 18:26:09.772769 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0252637 (* 1 = 0.0252637 loss)
I1128 18:26:09.772776 26047 sgd_solver.cpp:105] Iteration 93600, lr = 1e-05
I1128 18:26:26.780537 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:26:33.229038 26047 solver.cpp:218] Iteration 93800 (8.52697 iter/s, 23.455s/200 iters), loss = 0.0208494
I1128 18:26:33.229082 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0208493 (* 1 = 0.0208493 loss)
I1128 18:26:33.229087 26047 sgd_solver.cpp:105] Iteration 93800, lr = 1e-05
I1128 18:26:56.555649 26047 solver.cpp:330] Iteration 94000, Testing net (#0)
I1128 18:26:58.821310 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:26:58.916512 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8875
I1128 18:26:58.916533 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386355 (* 1 = 0.386355 loss)
I1128 18:26:59.039258 26047 solver.cpp:218] Iteration 94000 (7.7493 iter/s, 25.8088s/200 iters), loss = 0.0211319
I1128 18:26:59.039289 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0211319 (* 1 = 0.0211319 loss)
I1128 18:26:59.039309 26047 sgd_solver.cpp:105] Iteration 94000, lr = 1e-05
I1128 18:27:14.987830 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:27:22.496811 26047 solver.cpp:218] Iteration 94200 (8.52651 iter/s, 23.4563s/200 iters), loss = 0.0287441
I1128 18:27:22.496840 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0287441 (* 1 = 0.0287441 loss)
I1128 18:27:22.496845 26047 sgd_solver.cpp:105] Iteration 94200, lr = 1e-05
I1128 18:27:45.940300 26047 solver.cpp:218] Iteration 94400 (8.53162 iter/s, 23.4422s/200 iters), loss = 0.0476559
I1128 18:27:45.940381 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0476559 (* 1 = 0.0476559 loss)
I1128 18:27:45.940387 26047 sgd_solver.cpp:105] Iteration 94400, lr = 1e-05
I1128 18:28:00.846951 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:28:09.415448 26047 solver.cpp:218] Iteration 94600 (8.52013 iter/s, 23.4738s/200 iters), loss = 0.0220656
I1128 18:28:09.415478 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0220655 (* 1 = 0.0220655 loss)
I1128 18:28:09.415484 26047 sgd_solver.cpp:105] Iteration 94600, lr = 1e-05
I1128 18:28:32.855927 26047 solver.cpp:218] Iteration 94800 (8.53271 iter/s, 23.4392s/200 iters), loss = 0.0111389
I1128 18:28:32.856040 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0111388 (* 1 = 0.0111388 loss)
I1128 18:28:32.856046 26047 sgd_solver.cpp:105] Iteration 94800, lr = 1e-05
I1128 18:28:46.586032 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:28:56.206804 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_95000.caffemodel
I1128 18:28:56.235769 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_95000.solverstate
I1128 18:28:56.253414 26047 solver.cpp:330] Iteration 95000, Testing net (#0)
I1128 18:28:58.518919 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:28:58.614519 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8876
I1128 18:28:58.614542 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386486 (* 1 = 0.386486 loss)
I1128 18:28:58.734742 26047 solver.cpp:218] Iteration 95000 (7.72877 iter/s, 25.8773s/200 iters), loss = 0.0386368
I1128 18:28:58.734788 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0386367 (* 1 = 0.0386367 loss)
I1128 18:28:58.734796 26047 sgd_solver.cpp:105] Iteration 95000, lr = 1e-05
I1128 18:29:22.199312 26047 solver.cpp:218] Iteration 95200 (8.52395 iter/s, 23.4633s/200 iters), loss = 0.016087
I1128 18:29:22.199440 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0160869 (* 1 = 0.0160869 loss)
I1128 18:29:22.199447 26047 sgd_solver.cpp:105] Iteration 95200, lr = 1e-05
I1128 18:29:34.873234 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:29:45.661940 26047 solver.cpp:218] Iteration 95400 (8.52468 iter/s, 23.4613s/200 iters), loss = 0.033311
I1128 18:29:45.661983 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.033311 (* 1 = 0.033311 loss)
I1128 18:29:45.661988 26047 sgd_solver.cpp:105] Iteration 95400, lr = 1e-05
I1128 18:30:09.104133 26047 solver.cpp:218] Iteration 95600 (8.53208 iter/s, 23.4409s/200 iters), loss = 0.0175026
I1128 18:30:09.104182 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0175025 (* 1 = 0.0175025 loss)
I1128 18:30:09.104187 26047 sgd_solver.cpp:105] Iteration 95600, lr = 1e-05
I1128 18:30:20.726080 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:30:32.581437 26047 solver.cpp:218] Iteration 95800 (8.51933 iter/s, 23.476s/200 iters), loss = 0.0302084
I1128 18:30:32.581466 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0302083 (* 1 = 0.0302083 loss)
I1128 18:30:32.581485 26047 sgd_solver.cpp:105] Iteration 95800, lr = 1e-05
I1128 18:30:55.913349 26047 solver.cpp:330] Iteration 96000, Testing net (#0)
I1128 18:30:58.180655 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:30:58.276342 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8876
I1128 18:30:58.276365 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386406 (* 1 = 0.386406 loss)
I1128 18:30:58.398015 26047 solver.cpp:218] Iteration 96000 (7.74737 iter/s, 25.8152s/200 iters), loss = 0.0274133
I1128 18:30:58.398046 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0274132 (* 1 = 0.0274132 loss)
I1128 18:30:58.398052 26047 sgd_solver.cpp:105] Iteration 96000, lr = 1e-05
I1128 18:31:08.844795 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:31:21.870302 26047 solver.cpp:218] Iteration 96200 (8.52114 iter/s, 23.471s/200 iters), loss = 0.0196017
I1128 18:31:21.870332 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0196017 (* 1 = 0.0196017 loss)
I1128 18:31:21.870338 26047 sgd_solver.cpp:105] Iteration 96200, lr = 1e-05
I1128 18:31:45.321426 26047 solver.cpp:218] Iteration 96400 (8.52883 iter/s, 23.4499s/200 iters), loss = 0.0128033
I1128 18:31:45.321563 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0128032 (* 1 = 0.0128032 loss)
I1128 18:31:45.321568 26047 sgd_solver.cpp:105] Iteration 96400, lr = 1e-05
I1128 18:31:54.712004 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:32:08.771733 26047 solver.cpp:218] Iteration 96600 (8.52916 iter/s, 23.449s/200 iters), loss = 0.0201962
I1128 18:32:08.771777 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0201962 (* 1 = 0.0201962 loss)
I1128 18:32:08.771783 26047 sgd_solver.cpp:105] Iteration 96600, lr = 1e-05
I1128 18:32:32.206317 26047 solver.cpp:218] Iteration 96800 (8.53485 iter/s, 23.4333s/200 iters), loss = 0.0384458
I1128 18:32:32.206449 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0384457 (* 1 = 0.0384457 loss)
I1128 18:32:32.206455 26047 sgd_solver.cpp:105] Iteration 96800, lr = 1e-05
I1128 18:32:40.419417 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:32:55.533694 26047 solver.cpp:330] Iteration 97000, Testing net (#0)
I1128 18:32:57.797190 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:32:57.892554 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8876
I1128 18:32:57.892575 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386442 (* 1 = 0.386442 loss)
I1128 18:32:58.014551 26047 solver.cpp:218] Iteration 97000 (7.7499 iter/s, 25.8068s/200 iters), loss = 0.0324138
I1128 18:32:58.014583 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0324138 (* 1 = 0.0324138 loss)
I1128 18:32:58.014590 26047 sgd_solver.cpp:105] Iteration 97000, lr = 1e-05
I1128 18:33:21.458329 26047 solver.cpp:218] Iteration 97200 (8.5315 iter/s, 23.4425s/200 iters), loss = 0.0449566
I1128 18:33:21.458454 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0449565 (* 1 = 0.0449565 loss)
I1128 18:33:21.458461 26047 sgd_solver.cpp:105] Iteration 97200, lr = 1e-05
I1128 18:33:28.614861 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:33:44.906664 26047 solver.cpp:218] Iteration 97400 (8.52987 iter/s, 23.447s/200 iters), loss = 0.0586018
I1128 18:33:44.906708 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0586017 (* 1 = 0.0586017 loss)
I1128 18:33:44.906713 26047 sgd_solver.cpp:105] Iteration 97400, lr = 1e-05
I1128 18:33:56.515799 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_97500.caffemodel
I1128 18:33:56.560904 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_97500.solverstate
I1128 18:34:08.414201 26047 solver.cpp:218] Iteration 97600 (8.50836 iter/s, 23.5063s/200 iters), loss = 0.0223885
I1128 18:34:08.414245 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0223884 (* 1 = 0.0223884 loss)
I1128 18:34:08.414252 26047 sgd_solver.cpp:105] Iteration 97600, lr = 1e-05
I1128 18:34:14.521023 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:34:31.871526 26047 solver.cpp:218] Iteration 97800 (8.52653 iter/s, 23.4562s/200 iters), loss = 0.0245347
I1128 18:34:31.871621 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0245346 (* 1 = 0.0245346 loss)
I1128 18:34:31.871628 26047 sgd_solver.cpp:105] Iteration 97800, lr = 1e-05
I1128 18:34:55.205497 26047 solver.cpp:330] Iteration 98000, Testing net (#0)
I1128 18:34:57.470904 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:34:57.566864 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8875
I1128 18:34:57.566887 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386449 (* 1 = 0.386449 loss)
I1128 18:34:57.687881 26047 solver.cpp:218] Iteration 98000 (7.74653 iter/s, 25.818s/200 iters), loss = 0.0200353
I1128 18:34:57.687911 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0200352 (* 1 = 0.0200352 loss)
I1128 18:34:57.687917 26047 sgd_solver.cpp:105] Iteration 98000, lr = 1e-05
I1128 18:35:02.624500 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:35:21.142817 26047 solver.cpp:218] Iteration 98200 (8.52646 iter/s, 23.4564s/200 iters), loss = 0.019104
I1128 18:35:21.142844 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0191039 (* 1 = 0.0191039 loss)
I1128 18:35:21.142850 26047 sgd_solver.cpp:105] Iteration 98200, lr = 1e-05
I1128 18:35:44.587040 26047 solver.cpp:218] Iteration 98400 (8.53039 iter/s, 23.4456s/200 iters), loss = 0.0306132
I1128 18:35:44.587167 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0306131 (* 1 = 0.0306131 loss)
I1128 18:35:44.587174 26047 sgd_solver.cpp:105] Iteration 98400, lr = 1e-05
I1128 18:35:48.461505 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:36:08.029232 26047 solver.cpp:218] Iteration 98600 (8.5312 iter/s, 23.4434s/200 iters), loss = 0.0333879
I1128 18:36:08.029276 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0333878 (* 1 = 0.0333878 loss)
I1128 18:36:08.029283 26047 sgd_solver.cpp:105] Iteration 98600, lr = 1e-05
I1128 18:36:31.473621 26047 solver.cpp:218] Iteration 98800 (8.5304 iter/s, 23.4456s/200 iters), loss = 0.0494137
I1128 18:36:31.473752 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0494136 (* 1 = 0.0494136 loss)
I1128 18:36:31.473759 26047 sgd_solver.cpp:105] Iteration 98800, lr = 1e-05
I1128 18:36:34.289607 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:36:54.782641 26047 solver.cpp:330] Iteration 99000, Testing net (#0)
I1128 18:36:57.040055 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:36:57.134963 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8876
I1128 18:36:57.134985 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386467 (* 1 = 0.386467 loss)
I1128 18:36:57.255928 26047 solver.cpp:218] Iteration 99000 (7.75693 iter/s, 25.7834s/200 iters), loss = 0.0205195
I1128 18:36:57.255973 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0205194 (* 1 = 0.0205194 loss)
I1128 18:36:57.255980 26047 sgd_solver.cpp:105] Iteration 99000, lr = 1e-05
I1128 18:37:20.673712 26047 solver.cpp:218] Iteration 99200 (8.54016 iter/s, 23.4188s/200 iters), loss = 0.0252251
I1128 18:37:20.673857 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.025225 (* 1 = 0.025225 loss)
I1128 18:37:20.673866 26047 sgd_solver.cpp:105] Iteration 99200, lr = 1e-05
I1128 18:37:22.316829 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:37:44.094074 26047 solver.cpp:218] Iteration 99400 (8.53928 iter/s, 23.4212s/200 iters), loss = 0.00986783
I1128 18:37:44.094102 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00986776 (* 1 = 0.00986776 loss)
I1128 18:37:44.094107 26047 sgd_solver.cpp:105] Iteration 99400, lr = 1e-05
I1128 18:38:07.522838 26047 solver.cpp:218] Iteration 99600 (8.53621 iter/s, 23.4296s/200 iters), loss = 0.0198638
I1128 18:38:07.522923 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0198638 (* 1 = 0.0198638 loss)
I1128 18:38:07.522929 26047 sgd_solver.cpp:105] Iteration 99600, lr = 1e-05
I1128 18:38:08.112717 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:38:30.969907 26047 solver.cpp:218] Iteration 99800 (8.52959 iter/s, 23.4478s/200 iters), loss = 0.0201622
I1128 18:38:30.969951 26047 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0201621 (* 1 = 0.0201621 loss)
I1128 18:38:30.969956 26047 sgd_solver.cpp:105] Iteration 99800, lr = 1e-05
I1128 18:38:53.844326 26052 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:38:54.312602 26047 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_100000.caffemodel
I1128 18:38:54.340828 26047 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_mobilenet/model_save/caffe_ljftest_train_iter_100000.solverstate
I1128 18:38:54.385423 26047 solver.cpp:310] Iteration 100000, loss = 0.018054
I1128 18:38:54.385445 26047 solver.cpp:330] Iteration 100000, Testing net (#0)
I1128 18:38:56.651265 26055 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:38:56.746884 26047 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8875
I1128 18:38:56.746920 26047 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.386356 (* 1 = 0.386356 loss)
I1128 18:38:56.746923 26047 solver.cpp:315] Optimization Done.
I1128 18:38:56.746925 26047 caffe.cpp:259] Optimization Done.
