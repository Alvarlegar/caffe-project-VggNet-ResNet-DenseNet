Log file created at: 2017/11/28 13:09:49
Running on machine: ljf-ubuntu
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1128 13:09:49.487519  8649 caffe.cpp:218] Using GPUs 0
I1128 13:09:49.503422  8649 caffe.cpp:223] GPU 0: GeForce GTX 650
I1128 13:09:49.644649  8649 solver.cpp:44] Initializing solver from parameters: 
train_net: "/home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/train.prototxt"
test_net: "/home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/test.prototxt"
test_iter: 1000
test_interval: 1000
base_lr: 0.1
display: 200
max_iter: 100000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0001
stepsize: 200000
snapshot: 2500
snapshot_prefix: "/home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train"
solver_mode: GPU
device_id: 0
random_seed: 831486
train_state {
  level: 0
  stage: ""
}
stepvalue: 32000
stepvalue: 48000
stepvalue: 72000
stepvalue: 96000
iter_size: 4
type: "Nesterov"
I1128 13:09:49.644776  8649 solver.cpp:77] Creating training net from train_net file: /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/train.prototxt
I1128 13:09:49.645886  8649 net.cpp:51] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  transform_param {
    mirror: true
    crop_size: 28
  }
  data_param {
    source: "/home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/train_lmdb"
    batch_size: 32
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution3"
  bottom: "Convolution1"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Convolution5"
  bottom: "Eltwise1"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Convolution7"
  bottom: "Eltwise2"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise3"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution10"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution9"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Convolution12"
  bottom: "Eltwise4"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Convolution14"
  bottom: "Eltwise5"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution15"
  top: "Convolution15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Convolution15"
  top: "Convolution16"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Eltwise6"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution17"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution16"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Convolution19"
  bottom: "Eltwise7"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution20"
  top: "Convolution20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Convolution21"
  bottom: "Eltwise8"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution22"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution22"
  top: "Convolution22"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Convolution22"
  top: "Convolution23"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Pooling3"
  top: "Convolution24"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Convolution24"
  top: "Convolution24"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Convolution23"
  bottom: "Convolution24"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution25"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution25"
  top: "Convolution25"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Convolution25"
  top: "Convolution26"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Convolution26"
  top: "Convolution26"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "Convolution26"
  top: "Convolution26"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Convolution26"
  bottom: "Eltwise10"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution27"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "Convolution27"
  top: "Convolution27"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Convolution27"
  top: "Convolution28"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm28"
  type: "BatchNorm"
  bottom: "Convolution28"
  top: "Convolution28"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale28"
  type: "Scale"
  bottom: "Convolution28"
  top: "Convolution28"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Convolution28"
  bottom: "Eltwise11"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "Eltwise12"
  top: "Eltwise12"
}
layer {
  name: "Convolution29"
  type: "Convolution"
  bottom: "Eltwise12"
  top: "Convolution29"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm29"
  type: "BatchNorm"
  bottom: "Convolution29"
  top: "Convolution29"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale29"
  type: "Scale"
  bottom: "Convolution29"
  top: "Convolution29"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "Convolution29"
  top: "Convolution29"
}
layer {
  name: "Convolution30"
  type: "Convolution"
  bottom: "Convolution29"
  top: "Convolution30"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm30"
  type: "BatchNorm"
  bottom: "Convolution30"
  top: "Convolution30"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale30"
  type: "Scale"
  bottom: "Convolution30"
  top: "Convolution30"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Pooling4"
  type: "Pooling"
  bottom: "Eltwise12"
  top: "Pooling4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "Convolution31"
  type: "Convolution"
  bottom: "Pooling4"
  top: "Convolution31"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm31"
  type: "BatchNorm"
  bottom: "Convolution31"
  top: "Convolution31"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale31"
  type: "Scale"
  bottom: "Convolution31"
  top: "Convolution31"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise13"
  type: "Eltwise"
  bottom: "Convolution30"
  bottom: "Convolution31"
  top: "Eltwise13"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU27"
  type: "ReLU"
  bottom: "Eltwise13"
  top: "Eltwise13"
}
layer {
  name: "Convolution32"
  type: "Convolution"
  bottom: "Eltwise13"
  top: "Convolution32"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm32"
  type: "BatchNorm"
  bottom: "Convolution32"
  top: "Convolution32"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale32"
  type: "Scale"
  bottom: "Convolution32"
  top: "Convolution32"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU28"
  type: "ReLU"
  bottom: "Convolution32"
  top: "Convolution32"
}
layer {
  name: "Convolution33"
  type: "Convolution"
  bottom: "Convolution32"
  top: "Convolution33"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm33"
  type: "BatchNorm"
  bottom: "Convolution33"
  top: "Convolution33"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale33"
  type: "Scale"
  bottom: "Convolution33"
  top: "Convolution33"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise14"
  type: "Eltwise"
  bottom: "Convolution33"
  bottom: "Eltwise13"
  top: "Eltwise14"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU29"
  type: "ReLU"
  bottom: "Eltwise14"
  top: "Eltwise14"
}
layer {
  name: "Convolution34"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution34"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm34"
  type: "BatchNorm"
  bottom: "Convolution34"
  top: "Convolution34"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale34"
  type: "Scale"
  bottom: "Convolution34"
  top: "Convolution34"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU30"
  type: "ReLU"
  bottom: "Convolution34"
  top: "Convolution34"
}
layer {
  name: "Convolution35"
  type: "Convolution"
  bottom: "Convolution34"
  top: "Convolution35"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm35"
  type: "BatchNorm"
  bottom: "Convolution35"
  top: "Convolution35"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale35"
  type: "Scale"
  bottom: "Convolution35"
  top: "Convolution35"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise15"
  type: "Eltwise"
  bottom: "Convolution35"
  bottom: "Eltwise14"
  top: "Eltwise15"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU31"
  type: "ReLU"
  bottom: "Eltwise15"
  top: "Eltwise15"
}
layer {
  name: "Pooling5"
  type: "Pooling"
  bottom: "Eltwise15"
  top: "Pooling5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling5"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
I1128 13:09:49.646383  8649 layer_factory.hpp:77] Creating layer Data1
I1128 13:09:49.646499  8649 db_lmdb.cpp:35] Opened lmdb /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/train_lmdb
I1128 13:09:49.646534  8649 net.cpp:84] Creating Layer Data1
I1128 13:09:49.646546  8649 net.cpp:380] Data1 -> Data1
I1128 13:09:49.646569  8649 net.cpp:380] Data1 -> Data2
I1128 13:09:49.647178  8649 data_layer.cpp:45] output data size: 32,3,28,28
I1128 13:09:49.649022  8649 net.cpp:122] Setting up Data1
I1128 13:09:49.649051  8649 net.cpp:129] Top shape: 32 3 28 28 (75264)
I1128 13:09:49.649073  8649 net.cpp:129] Top shape: 32 (32)
I1128 13:09:49.649077  8649 net.cpp:137] Memory required for data: 301184
I1128 13:09:49.649088  8649 layer_factory.hpp:77] Creating layer Convolution1
I1128 13:09:49.649111  8649 net.cpp:84] Creating Layer Convolution1
I1128 13:09:49.649116  8649 net.cpp:406] Convolution1 <- Data1
I1128 13:09:49.649127  8649 net.cpp:380] Convolution1 -> Convolution1
I1128 13:09:49.990348  8649 net.cpp:122] Setting up Convolution1
I1128 13:09:49.990391  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.990396  8649 net.cpp:137] Memory required for data: 1906816
I1128 13:09:49.990427  8649 layer_factory.hpp:77] Creating layer BatchNorm1
I1128 13:09:49.990444  8649 net.cpp:84] Creating Layer BatchNorm1
I1128 13:09:49.990450  8649 net.cpp:406] BatchNorm1 <- Convolution1
I1128 13:09:49.990458  8649 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1128 13:09:49.990615  8649 net.cpp:122] Setting up BatchNorm1
I1128 13:09:49.990624  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.990628  8649 net.cpp:137] Memory required for data: 3512448
I1128 13:09:49.990636  8649 layer_factory.hpp:77] Creating layer Scale1
I1128 13:09:49.990645  8649 net.cpp:84] Creating Layer Scale1
I1128 13:09:49.990650  8649 net.cpp:406] Scale1 <- Convolution1
I1128 13:09:49.990656  8649 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1128 13:09:49.990696  8649 layer_factory.hpp:77] Creating layer Scale1
I1128 13:09:49.990787  8649 net.cpp:122] Setting up Scale1
I1128 13:09:49.990794  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.990798  8649 net.cpp:137] Memory required for data: 5118080
I1128 13:09:49.990804  8649 layer_factory.hpp:77] Creating layer ReLU1
I1128 13:09:49.990811  8649 net.cpp:84] Creating Layer ReLU1
I1128 13:09:49.990816  8649 net.cpp:406] ReLU1 <- Convolution1
I1128 13:09:49.990821  8649 net.cpp:367] ReLU1 -> Convolution1 (in-place)
I1128 13:09:49.991178  8649 net.cpp:122] Setting up ReLU1
I1128 13:09:49.991189  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.991192  8649 net.cpp:137] Memory required for data: 6723712
I1128 13:09:49.991196  8649 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I1128 13:09:49.991206  8649 net.cpp:84] Creating Layer Convolution1_ReLU1_0_split
I1128 13:09:49.991211  8649 net.cpp:406] Convolution1_ReLU1_0_split <- Convolution1
I1128 13:09:49.991216  8649 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I1128 13:09:49.991225  8649 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I1128 13:09:49.991258  8649 net.cpp:122] Setting up Convolution1_ReLU1_0_split
I1128 13:09:49.991266  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.991271  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.991273  8649 net.cpp:137] Memory required for data: 9934976
I1128 13:09:49.991276  8649 layer_factory.hpp:77] Creating layer Convolution2
I1128 13:09:49.991289  8649 net.cpp:84] Creating Layer Convolution2
I1128 13:09:49.991294  8649 net.cpp:406] Convolution2 <- Convolution1_ReLU1_0_split_0
I1128 13:09:49.991300  8649 net.cpp:380] Convolution2 -> Convolution2
I1128 13:09:49.993325  8649 net.cpp:122] Setting up Convolution2
I1128 13:09:49.993368  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.993372  8649 net.cpp:137] Memory required for data: 11540608
I1128 13:09:49.993388  8649 layer_factory.hpp:77] Creating layer BatchNorm2
I1128 13:09:49.993398  8649 net.cpp:84] Creating Layer BatchNorm2
I1128 13:09:49.993403  8649 net.cpp:406] BatchNorm2 <- Convolution2
I1128 13:09:49.993410  8649 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1128 13:09:49.993594  8649 net.cpp:122] Setting up BatchNorm2
I1128 13:09:49.993602  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.993605  8649 net.cpp:137] Memory required for data: 13146240
I1128 13:09:49.993613  8649 layer_factory.hpp:77] Creating layer Scale2
I1128 13:09:49.993624  8649 net.cpp:84] Creating Layer Scale2
I1128 13:09:49.993639  8649 net.cpp:406] Scale2 <- Convolution2
I1128 13:09:49.993645  8649 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1128 13:09:49.993680  8649 layer_factory.hpp:77] Creating layer Scale2
I1128 13:09:49.993772  8649 net.cpp:122] Setting up Scale2
I1128 13:09:49.993780  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.993784  8649 net.cpp:137] Memory required for data: 14751872
I1128 13:09:49.993789  8649 layer_factory.hpp:77] Creating layer ReLU2
I1128 13:09:49.993796  8649 net.cpp:84] Creating Layer ReLU2
I1128 13:09:49.993800  8649 net.cpp:406] ReLU2 <- Convolution2
I1128 13:09:49.993806  8649 net.cpp:367] ReLU2 -> Convolution2 (in-place)
I1128 13:09:49.994055  8649 net.cpp:122] Setting up ReLU2
I1128 13:09:49.994076  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.994081  8649 net.cpp:137] Memory required for data: 16357504
I1128 13:09:49.994084  8649 layer_factory.hpp:77] Creating layer Convolution3
I1128 13:09:49.994099  8649 net.cpp:84] Creating Layer Convolution3
I1128 13:09:49.994107  8649 net.cpp:406] Convolution3 <- Convolution2
I1128 13:09:49.994115  8649 net.cpp:380] Convolution3 -> Convolution3
I1128 13:09:49.995627  8649 net.cpp:122] Setting up Convolution3
I1128 13:09:49.995652  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.995656  8649 net.cpp:137] Memory required for data: 17963136
I1128 13:09:49.995664  8649 layer_factory.hpp:77] Creating layer BatchNorm3
I1128 13:09:49.995674  8649 net.cpp:84] Creating Layer BatchNorm3
I1128 13:09:49.995682  8649 net.cpp:406] BatchNorm3 <- Convolution3
I1128 13:09:49.995687  8649 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1128 13:09:49.995839  8649 net.cpp:122] Setting up BatchNorm3
I1128 13:09:49.995846  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.995849  8649 net.cpp:137] Memory required for data: 19568768
I1128 13:09:49.995859  8649 layer_factory.hpp:77] Creating layer Scale3
I1128 13:09:49.995867  8649 net.cpp:84] Creating Layer Scale3
I1128 13:09:49.995869  8649 net.cpp:406] Scale3 <- Convolution3
I1128 13:09:49.995875  8649 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1128 13:09:49.995909  8649 layer_factory.hpp:77] Creating layer Scale3
I1128 13:09:49.995999  8649 net.cpp:122] Setting up Scale3
I1128 13:09:49.996008  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.996012  8649 net.cpp:137] Memory required for data: 21174400
I1128 13:09:49.996021  8649 layer_factory.hpp:77] Creating layer Eltwise1
I1128 13:09:49.996028  8649 net.cpp:84] Creating Layer Eltwise1
I1128 13:09:49.996034  8649 net.cpp:406] Eltwise1 <- Convolution3
I1128 13:09:49.996040  8649 net.cpp:406] Eltwise1 <- Convolution1_ReLU1_0_split_1
I1128 13:09:49.996049  8649 net.cpp:380] Eltwise1 -> Eltwise1
I1128 13:09:49.996084  8649 net.cpp:122] Setting up Eltwise1
I1128 13:09:49.996095  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.996100  8649 net.cpp:137] Memory required for data: 22780032
I1128 13:09:49.996105  8649 layer_factory.hpp:77] Creating layer ReLU3
I1128 13:09:49.996114  8649 net.cpp:84] Creating Layer ReLU3
I1128 13:09:49.996119  8649 net.cpp:406] ReLU3 <- Eltwise1
I1128 13:09:49.996126  8649 net.cpp:367] ReLU3 -> Eltwise1 (in-place)
I1128 13:09:49.996469  8649 net.cpp:122] Setting up ReLU3
I1128 13:09:49.996481  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.996486  8649 net.cpp:137] Memory required for data: 24385664
I1128 13:09:49.996491  8649 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I1128 13:09:49.996500  8649 net.cpp:84] Creating Layer Eltwise1_ReLU3_0_split
I1128 13:09:49.996520  8649 net.cpp:406] Eltwise1_ReLU3_0_split <- Eltwise1
I1128 13:09:49.996541  8649 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I1128 13:09:49.996562  8649 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I1128 13:09:49.996626  8649 net.cpp:122] Setting up Eltwise1_ReLU3_0_split
I1128 13:09:49.996636  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.996642  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.996659  8649 net.cpp:137] Memory required for data: 27596928
I1128 13:09:49.996666  8649 layer_factory.hpp:77] Creating layer Convolution4
I1128 13:09:49.996685  8649 net.cpp:84] Creating Layer Convolution4
I1128 13:09:49.996690  8649 net.cpp:406] Convolution4 <- Eltwise1_ReLU3_0_split_0
I1128 13:09:49.996700  8649 net.cpp:380] Convolution4 -> Convolution4
I1128 13:09:49.998026  8649 net.cpp:122] Setting up Convolution4
I1128 13:09:49.998088  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.998105  8649 net.cpp:137] Memory required for data: 29202560
I1128 13:09:49.998126  8649 layer_factory.hpp:77] Creating layer BatchNorm4
I1128 13:09:49.998150  8649 net.cpp:84] Creating Layer BatchNorm4
I1128 13:09:49.998167  8649 net.cpp:406] BatchNorm4 <- Convolution4
I1128 13:09:49.998185  8649 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1128 13:09:49.998402  8649 net.cpp:122] Setting up BatchNorm4
I1128 13:09:49.998427  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.998443  8649 net.cpp:137] Memory required for data: 30808192
I1128 13:09:49.998464  8649 layer_factory.hpp:77] Creating layer Scale4
I1128 13:09:49.998476  8649 net.cpp:84] Creating Layer Scale4
I1128 13:09:49.998481  8649 net.cpp:406] Scale4 <- Convolution4
I1128 13:09:49.998488  8649 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1128 13:09:49.998546  8649 layer_factory.hpp:77] Creating layer Scale4
I1128 13:09:49.998677  8649 net.cpp:122] Setting up Scale4
I1128 13:09:49.998688  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.998694  8649 net.cpp:137] Memory required for data: 32413824
I1128 13:09:49.998703  8649 layer_factory.hpp:77] Creating layer ReLU4
I1128 13:09:49.998724  8649 net.cpp:84] Creating Layer ReLU4
I1128 13:09:49.998741  8649 net.cpp:406] ReLU4 <- Convolution4
I1128 13:09:49.998760  8649 net.cpp:367] ReLU4 -> Convolution4 (in-place)
I1128 13:09:49.998987  8649 net.cpp:122] Setting up ReLU4
I1128 13:09:49.998999  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:49.999004  8649 net.cpp:137] Memory required for data: 34019456
I1128 13:09:49.999009  8649 layer_factory.hpp:77] Creating layer Convolution5
I1128 13:09:49.999034  8649 net.cpp:84] Creating Layer Convolution5
I1128 13:09:49.999052  8649 net.cpp:406] Convolution5 <- Convolution4
I1128 13:09:49.999063  8649 net.cpp:380] Convolution5 -> Convolution5
I1128 13:09:50.000216  8649 net.cpp:122] Setting up Convolution5
I1128 13:09:50.000269  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.000274  8649 net.cpp:137] Memory required for data: 35625088
I1128 13:09:50.000286  8649 layer_factory.hpp:77] Creating layer BatchNorm5
I1128 13:09:50.000301  8649 net.cpp:84] Creating Layer BatchNorm5
I1128 13:09:50.000325  8649 net.cpp:406] BatchNorm5 <- Convolution5
I1128 13:09:50.000339  8649 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1128 13:09:50.000522  8649 net.cpp:122] Setting up BatchNorm5
I1128 13:09:50.000531  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.000536  8649 net.cpp:137] Memory required for data: 37230720
I1128 13:09:50.000553  8649 layer_factory.hpp:77] Creating layer Scale5
I1128 13:09:50.000563  8649 net.cpp:84] Creating Layer Scale5
I1128 13:09:50.000569  8649 net.cpp:406] Scale5 <- Convolution5
I1128 13:09:50.000581  8649 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1128 13:09:50.000622  8649 layer_factory.hpp:77] Creating layer Scale5
I1128 13:09:50.000716  8649 net.cpp:122] Setting up Scale5
I1128 13:09:50.000737  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.000742  8649 net.cpp:137] Memory required for data: 38836352
I1128 13:09:50.000751  8649 layer_factory.hpp:77] Creating layer Eltwise2
I1128 13:09:50.000767  8649 net.cpp:84] Creating Layer Eltwise2
I1128 13:09:50.000773  8649 net.cpp:406] Eltwise2 <- Convolution5
I1128 13:09:50.000780  8649 net.cpp:406] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I1128 13:09:50.000790  8649 net.cpp:380] Eltwise2 -> Eltwise2
I1128 13:09:50.000820  8649 net.cpp:122] Setting up Eltwise2
I1128 13:09:50.000836  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.000841  8649 net.cpp:137] Memory required for data: 40441984
I1128 13:09:50.000847  8649 layer_factory.hpp:77] Creating layer ReLU5
I1128 13:09:50.000856  8649 net.cpp:84] Creating Layer ReLU5
I1128 13:09:50.000862  8649 net.cpp:406] ReLU5 <- Eltwise2
I1128 13:09:50.000869  8649 net.cpp:367] ReLU5 -> Eltwise2 (in-place)
I1128 13:09:50.001232  8649 net.cpp:122] Setting up ReLU5
I1128 13:09:50.001246  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.001251  8649 net.cpp:137] Memory required for data: 42047616
I1128 13:09:50.001256  8649 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I1128 13:09:50.001266  8649 net.cpp:84] Creating Layer Eltwise2_ReLU5_0_split
I1128 13:09:50.001271  8649 net.cpp:406] Eltwise2_ReLU5_0_split <- Eltwise2
I1128 13:09:50.001281  8649 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I1128 13:09:50.001293  8649 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I1128 13:09:50.001338  8649 net.cpp:122] Setting up Eltwise2_ReLU5_0_split
I1128 13:09:50.001346  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.001353  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.001358  8649 net.cpp:137] Memory required for data: 45258880
I1128 13:09:50.001363  8649 layer_factory.hpp:77] Creating layer Convolution6
I1128 13:09:50.001379  8649 net.cpp:84] Creating Layer Convolution6
I1128 13:09:50.001384  8649 net.cpp:406] Convolution6 <- Eltwise2_ReLU5_0_split_0
I1128 13:09:50.001392  8649 net.cpp:380] Convolution6 -> Convolution6
I1128 13:09:50.003005  8649 net.cpp:122] Setting up Convolution6
I1128 13:09:50.003062  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.003088  8649 net.cpp:137] Memory required for data: 46864512
I1128 13:09:50.003113  8649 layer_factory.hpp:77] Creating layer BatchNorm6
I1128 13:09:50.003135  8649 net.cpp:84] Creating Layer BatchNorm6
I1128 13:09:50.003154  8649 net.cpp:406] BatchNorm6 <- Convolution6
I1128 13:09:50.003173  8649 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1128 13:09:50.003435  8649 net.cpp:122] Setting up BatchNorm6
I1128 13:09:50.003464  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.003479  8649 net.cpp:137] Memory required for data: 48470144
I1128 13:09:50.003496  8649 layer_factory.hpp:77] Creating layer Scale6
I1128 13:09:50.003517  8649 net.cpp:84] Creating Layer Scale6
I1128 13:09:50.003532  8649 net.cpp:406] Scale6 <- Convolution6
I1128 13:09:50.003547  8649 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1128 13:09:50.003617  8649 layer_factory.hpp:77] Creating layer Scale6
I1128 13:09:50.003772  8649 net.cpp:122] Setting up Scale6
I1128 13:09:50.003798  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.003813  8649 net.cpp:137] Memory required for data: 50075776
I1128 13:09:50.003832  8649 layer_factory.hpp:77] Creating layer ReLU6
I1128 13:09:50.003851  8649 net.cpp:84] Creating Layer ReLU6
I1128 13:09:50.003867  8649 net.cpp:406] ReLU6 <- Convolution6
I1128 13:09:50.003886  8649 net.cpp:367] ReLU6 -> Convolution6 (in-place)
I1128 13:09:50.004525  8649 net.cpp:122] Setting up ReLU6
I1128 13:09:50.004542  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.004549  8649 net.cpp:137] Memory required for data: 51681408
I1128 13:09:50.004554  8649 layer_factory.hpp:77] Creating layer Convolution7
I1128 13:09:50.004570  8649 net.cpp:84] Creating Layer Convolution7
I1128 13:09:50.004578  8649 net.cpp:406] Convolution7 <- Convolution6
I1128 13:09:50.004590  8649 net.cpp:380] Convolution7 -> Convolution7
I1128 13:09:50.005698  8649 net.cpp:122] Setting up Convolution7
I1128 13:09:50.005712  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.005715  8649 net.cpp:137] Memory required for data: 53287040
I1128 13:09:50.005726  8649 layer_factory.hpp:77] Creating layer BatchNorm7
I1128 13:09:50.005738  8649 net.cpp:84] Creating Layer BatchNorm7
I1128 13:09:50.005744  8649 net.cpp:406] BatchNorm7 <- Convolution7
I1128 13:09:50.005769  8649 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1128 13:09:50.005934  8649 net.cpp:122] Setting up BatchNorm7
I1128 13:09:50.005942  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.005949  8649 net.cpp:137] Memory required for data: 54892672
I1128 13:09:50.005961  8649 layer_factory.hpp:77] Creating layer Scale7
I1128 13:09:50.005978  8649 net.cpp:84] Creating Layer Scale7
I1128 13:09:50.005985  8649 net.cpp:406] Scale7 <- Convolution7
I1128 13:09:50.005995  8649 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1128 13:09:50.006041  8649 layer_factory.hpp:77] Creating layer Scale7
I1128 13:09:50.006145  8649 net.cpp:122] Setting up Scale7
I1128 13:09:50.006153  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.006158  8649 net.cpp:137] Memory required for data: 56498304
I1128 13:09:50.006167  8649 layer_factory.hpp:77] Creating layer Eltwise3
I1128 13:09:50.006180  8649 net.cpp:84] Creating Layer Eltwise3
I1128 13:09:50.006186  8649 net.cpp:406] Eltwise3 <- Convolution7
I1128 13:09:50.006196  8649 net.cpp:406] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I1128 13:09:50.006206  8649 net.cpp:380] Eltwise3 -> Eltwise3
I1128 13:09:50.006237  8649 net.cpp:122] Setting up Eltwise3
I1128 13:09:50.006243  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.006249  8649 net.cpp:137] Memory required for data: 58103936
I1128 13:09:50.006255  8649 layer_factory.hpp:77] Creating layer ReLU7
I1128 13:09:50.006265  8649 net.cpp:84] Creating Layer ReLU7
I1128 13:09:50.006273  8649 net.cpp:406] ReLU7 <- Eltwise3
I1128 13:09:50.006283  8649 net.cpp:367] ReLU7 -> Eltwise3 (in-place)
I1128 13:09:50.006438  8649 net.cpp:122] Setting up ReLU7
I1128 13:09:50.006446  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.006453  8649 net.cpp:137] Memory required for data: 59709568
I1128 13:09:50.006458  8649 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I1128 13:09:50.006469  8649 net.cpp:84] Creating Layer Eltwise3_ReLU7_0_split
I1128 13:09:50.006477  8649 net.cpp:406] Eltwise3_ReLU7_0_split <- Eltwise3
I1128 13:09:50.006489  8649 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I1128 13:09:50.006500  8649 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I1128 13:09:50.006542  8649 net.cpp:122] Setting up Eltwise3_ReLU7_0_split
I1128 13:09:50.006551  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.006556  8649 net.cpp:129] Top shape: 32 16 28 28 (401408)
I1128 13:09:50.006562  8649 net.cpp:137] Memory required for data: 62920832
I1128 13:09:50.006569  8649 layer_factory.hpp:77] Creating layer Convolution8
I1128 13:09:50.006585  8649 net.cpp:84] Creating Layer Convolution8
I1128 13:09:50.006593  8649 net.cpp:406] Convolution8 <- Eltwise3_ReLU7_0_split_0
I1128 13:09:50.006606  8649 net.cpp:380] Convolution8 -> Convolution8
I1128 13:09:50.008011  8649 net.cpp:122] Setting up Convolution8
I1128 13:09:50.008028  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.008031  8649 net.cpp:137] Memory required for data: 63723648
I1128 13:09:50.008041  8649 layer_factory.hpp:77] Creating layer BatchNorm8
I1128 13:09:50.008051  8649 net.cpp:84] Creating Layer BatchNorm8
I1128 13:09:50.008059  8649 net.cpp:406] BatchNorm8 <- Convolution8
I1128 13:09:50.008070  8649 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1128 13:09:50.008255  8649 net.cpp:122] Setting up BatchNorm8
I1128 13:09:50.008265  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.008268  8649 net.cpp:137] Memory required for data: 64526464
I1128 13:09:50.008275  8649 layer_factory.hpp:77] Creating layer Scale8
I1128 13:09:50.008283  8649 net.cpp:84] Creating Layer Scale8
I1128 13:09:50.008289  8649 net.cpp:406] Scale8 <- Convolution8
I1128 13:09:50.008296  8649 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1128 13:09:50.008340  8649 layer_factory.hpp:77] Creating layer Scale8
I1128 13:09:50.008438  8649 net.cpp:122] Setting up Scale8
I1128 13:09:50.008446  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.008463  8649 net.cpp:137] Memory required for data: 65329280
I1128 13:09:50.008476  8649 layer_factory.hpp:77] Creating layer ReLU8
I1128 13:09:50.008486  8649 net.cpp:84] Creating Layer ReLU8
I1128 13:09:50.008493  8649 net.cpp:406] ReLU8 <- Convolution8
I1128 13:09:50.008503  8649 net.cpp:367] ReLU8 -> Convolution8 (in-place)
I1128 13:09:50.008787  8649 net.cpp:122] Setting up ReLU8
I1128 13:09:50.008797  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.008803  8649 net.cpp:137] Memory required for data: 66132096
I1128 13:09:50.008810  8649 layer_factory.hpp:77] Creating layer Convolution9
I1128 13:09:50.008826  8649 net.cpp:84] Creating Layer Convolution9
I1128 13:09:50.008833  8649 net.cpp:406] Convolution9 <- Convolution8
I1128 13:09:50.008844  8649 net.cpp:380] Convolution9 -> Convolution9
I1128 13:09:50.009912  8649 net.cpp:122] Setting up Convolution9
I1128 13:09:50.009925  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.009929  8649 net.cpp:137] Memory required for data: 66934912
I1128 13:09:50.009937  8649 layer_factory.hpp:77] Creating layer BatchNorm9
I1128 13:09:50.009943  8649 net.cpp:84] Creating Layer BatchNorm9
I1128 13:09:50.009948  8649 net.cpp:406] BatchNorm9 <- Convolution9
I1128 13:09:50.009953  8649 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1128 13:09:50.010107  8649 net.cpp:122] Setting up BatchNorm9
I1128 13:09:50.010114  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.010118  8649 net.cpp:137] Memory required for data: 67737728
I1128 13:09:50.010125  8649 layer_factory.hpp:77] Creating layer Scale9
I1128 13:09:50.010131  8649 net.cpp:84] Creating Layer Scale9
I1128 13:09:50.010136  8649 net.cpp:406] Scale9 <- Convolution9
I1128 13:09:50.010140  8649 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1128 13:09:50.010172  8649 layer_factory.hpp:77] Creating layer Scale9
I1128 13:09:50.010262  8649 net.cpp:122] Setting up Scale9
I1128 13:09:50.010268  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.010273  8649 net.cpp:137] Memory required for data: 68540544
I1128 13:09:50.010278  8649 layer_factory.hpp:77] Creating layer Pooling1
I1128 13:09:50.010285  8649 net.cpp:84] Creating Layer Pooling1
I1128 13:09:50.010290  8649 net.cpp:406] Pooling1 <- Eltwise3_ReLU7_0_split_1
I1128 13:09:50.010295  8649 net.cpp:380] Pooling1 -> Pooling1
I1128 13:09:50.010335  8649 net.cpp:122] Setting up Pooling1
I1128 13:09:50.010342  8649 net.cpp:129] Top shape: 32 16 14 14 (100352)
I1128 13:09:50.010346  8649 net.cpp:137] Memory required for data: 68941952
I1128 13:09:50.010350  8649 layer_factory.hpp:77] Creating layer Convolution10
I1128 13:09:50.010360  8649 net.cpp:84] Creating Layer Convolution10
I1128 13:09:50.010365  8649 net.cpp:406] Convolution10 <- Pooling1
I1128 13:09:50.010370  8649 net.cpp:380] Convolution10 -> Convolution10
I1128 13:09:50.011243  8649 net.cpp:122] Setting up Convolution10
I1128 13:09:50.011260  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.011265  8649 net.cpp:137] Memory required for data: 69744768
I1128 13:09:50.011281  8649 layer_factory.hpp:77] Creating layer BatchNorm10
I1128 13:09:50.011291  8649 net.cpp:84] Creating Layer BatchNorm10
I1128 13:09:50.011296  8649 net.cpp:406] BatchNorm10 <- Convolution10
I1128 13:09:50.011301  8649 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1128 13:09:50.011451  8649 net.cpp:122] Setting up BatchNorm10
I1128 13:09:50.011458  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.011461  8649 net.cpp:137] Memory required for data: 70547584
I1128 13:09:50.011468  8649 layer_factory.hpp:77] Creating layer Scale10
I1128 13:09:50.011474  8649 net.cpp:84] Creating Layer Scale10
I1128 13:09:50.011478  8649 net.cpp:406] Scale10 <- Convolution10
I1128 13:09:50.011482  8649 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1128 13:09:50.011515  8649 layer_factory.hpp:77] Creating layer Scale10
I1128 13:09:50.011603  8649 net.cpp:122] Setting up Scale10
I1128 13:09:50.011610  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.011626  8649 net.cpp:137] Memory required for data: 71350400
I1128 13:09:50.011631  8649 layer_factory.hpp:77] Creating layer Eltwise4
I1128 13:09:50.011638  8649 net.cpp:84] Creating Layer Eltwise4
I1128 13:09:50.011643  8649 net.cpp:406] Eltwise4 <- Convolution9
I1128 13:09:50.011647  8649 net.cpp:406] Eltwise4 <- Convolution10
I1128 13:09:50.011653  8649 net.cpp:380] Eltwise4 -> Eltwise4
I1128 13:09:50.011677  8649 net.cpp:122] Setting up Eltwise4
I1128 13:09:50.011682  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.011685  8649 net.cpp:137] Memory required for data: 72153216
I1128 13:09:50.011689  8649 layer_factory.hpp:77] Creating layer ReLU9
I1128 13:09:50.011695  8649 net.cpp:84] Creating Layer ReLU9
I1128 13:09:50.011699  8649 net.cpp:406] ReLU9 <- Eltwise4
I1128 13:09:50.011704  8649 net.cpp:367] ReLU9 -> Eltwise4 (in-place)
I1128 13:09:50.011859  8649 net.cpp:122] Setting up ReLU9
I1128 13:09:50.011868  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.011871  8649 net.cpp:137] Memory required for data: 72956032
I1128 13:09:50.011875  8649 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I1128 13:09:50.011880  8649 net.cpp:84] Creating Layer Eltwise4_ReLU9_0_split
I1128 13:09:50.011885  8649 net.cpp:406] Eltwise4_ReLU9_0_split <- Eltwise4
I1128 13:09:50.011890  8649 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I1128 13:09:50.011898  8649 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I1128 13:09:50.011932  8649 net.cpp:122] Setting up Eltwise4_ReLU9_0_split
I1128 13:09:50.011940  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.011942  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.011946  8649 net.cpp:137] Memory required for data: 74561664
I1128 13:09:50.011950  8649 layer_factory.hpp:77] Creating layer Convolution11
I1128 13:09:50.011960  8649 net.cpp:84] Creating Layer Convolution11
I1128 13:09:50.011963  8649 net.cpp:406] Convolution11 <- Eltwise4_ReLU9_0_split_0
I1128 13:09:50.011968  8649 net.cpp:380] Convolution11 -> Convolution11
I1128 13:09:50.013080  8649 net.cpp:122] Setting up Convolution11
I1128 13:09:50.013097  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.013099  8649 net.cpp:137] Memory required for data: 75364480
I1128 13:09:50.013106  8649 layer_factory.hpp:77] Creating layer BatchNorm11
I1128 13:09:50.013113  8649 net.cpp:84] Creating Layer BatchNorm11
I1128 13:09:50.013119  8649 net.cpp:406] BatchNorm11 <- Convolution11
I1128 13:09:50.013124  8649 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1128 13:09:50.013284  8649 net.cpp:122] Setting up BatchNorm11
I1128 13:09:50.013293  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.013296  8649 net.cpp:137] Memory required for data: 76167296
I1128 13:09:50.013304  8649 layer_factory.hpp:77] Creating layer Scale11
I1128 13:09:50.013310  8649 net.cpp:84] Creating Layer Scale11
I1128 13:09:50.013315  8649 net.cpp:406] Scale11 <- Convolution11
I1128 13:09:50.013320  8649 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1128 13:09:50.013355  8649 layer_factory.hpp:77] Creating layer Scale11
I1128 13:09:50.013445  8649 net.cpp:122] Setting up Scale11
I1128 13:09:50.013453  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.013456  8649 net.cpp:137] Memory required for data: 76970112
I1128 13:09:50.013461  8649 layer_factory.hpp:77] Creating layer ReLU10
I1128 13:09:50.013468  8649 net.cpp:84] Creating Layer ReLU10
I1128 13:09:50.013473  8649 net.cpp:406] ReLU10 <- Convolution11
I1128 13:09:50.013478  8649 net.cpp:367] ReLU10 -> Convolution11 (in-place)
I1128 13:09:50.013824  8649 net.cpp:122] Setting up ReLU10
I1128 13:09:50.013842  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.013845  8649 net.cpp:137] Memory required for data: 77772928
I1128 13:09:50.013850  8649 layer_factory.hpp:77] Creating layer Convolution12
I1128 13:09:50.013864  8649 net.cpp:84] Creating Layer Convolution12
I1128 13:09:50.013870  8649 net.cpp:406] Convolution12 <- Convolution11
I1128 13:09:50.013890  8649 net.cpp:380] Convolution12 -> Convolution12
I1128 13:09:50.015467  8649 net.cpp:122] Setting up Convolution12
I1128 13:09:50.015492  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.015496  8649 net.cpp:137] Memory required for data: 78575744
I1128 13:09:50.015507  8649 layer_factory.hpp:77] Creating layer BatchNorm12
I1128 13:09:50.015522  8649 net.cpp:84] Creating Layer BatchNorm12
I1128 13:09:50.015529  8649 net.cpp:406] BatchNorm12 <- Convolution12
I1128 13:09:50.015539  8649 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I1128 13:09:50.015738  8649 net.cpp:122] Setting up BatchNorm12
I1128 13:09:50.015749  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.015772  8649 net.cpp:137] Memory required for data: 79378560
I1128 13:09:50.015792  8649 layer_factory.hpp:77] Creating layer Scale12
I1128 13:09:50.015810  8649 net.cpp:84] Creating Layer Scale12
I1128 13:09:50.015825  8649 net.cpp:406] Scale12 <- Convolution12
I1128 13:09:50.015841  8649 net.cpp:367] Scale12 -> Convolution12 (in-place)
I1128 13:09:50.015898  8649 layer_factory.hpp:77] Creating layer Scale12
I1128 13:09:50.015997  8649 net.cpp:122] Setting up Scale12
I1128 13:09:50.016008  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.016023  8649 net.cpp:137] Memory required for data: 80181376
I1128 13:09:50.016031  8649 layer_factory.hpp:77] Creating layer Eltwise5
I1128 13:09:50.016041  8649 net.cpp:84] Creating Layer Eltwise5
I1128 13:09:50.016047  8649 net.cpp:406] Eltwise5 <- Convolution12
I1128 13:09:50.016054  8649 net.cpp:406] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I1128 13:09:50.016062  8649 net.cpp:380] Eltwise5 -> Eltwise5
I1128 13:09:50.016093  8649 net.cpp:122] Setting up Eltwise5
I1128 13:09:50.016101  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.016114  8649 net.cpp:137] Memory required for data: 80984192
I1128 13:09:50.016120  8649 layer_factory.hpp:77] Creating layer ReLU11
I1128 13:09:50.016129  8649 net.cpp:84] Creating Layer ReLU11
I1128 13:09:50.016134  8649 net.cpp:406] ReLU11 <- Eltwise5
I1128 13:09:50.016144  8649 net.cpp:367] ReLU11 -> Eltwise5 (in-place)
I1128 13:09:50.016556  8649 net.cpp:122] Setting up ReLU11
I1128 13:09:50.016587  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.016602  8649 net.cpp:137] Memory required for data: 81787008
I1128 13:09:50.016616  8649 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I1128 13:09:50.016634  8649 net.cpp:84] Creating Layer Eltwise5_ReLU11_0_split
I1128 13:09:50.016649  8649 net.cpp:406] Eltwise5_ReLU11_0_split <- Eltwise5
I1128 13:09:50.016666  8649 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I1128 13:09:50.016686  8649 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I1128 13:09:50.016747  8649 net.cpp:122] Setting up Eltwise5_ReLU11_0_split
I1128 13:09:50.016767  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.016782  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.016794  8649 net.cpp:137] Memory required for data: 83392640
I1128 13:09:50.016806  8649 layer_factory.hpp:77] Creating layer Convolution13
I1128 13:09:50.016829  8649 net.cpp:84] Creating Layer Convolution13
I1128 13:09:50.016844  8649 net.cpp:406] Convolution13 <- Eltwise5_ReLU11_0_split_0
I1128 13:09:50.016863  8649 net.cpp:380] Convolution13 -> Convolution13
I1128 13:09:50.018779  8649 net.cpp:122] Setting up Convolution13
I1128 13:09:50.018838  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.018856  8649 net.cpp:137] Memory required for data: 84195456
I1128 13:09:50.018878  8649 layer_factory.hpp:77] Creating layer BatchNorm13
I1128 13:09:50.018901  8649 net.cpp:84] Creating Layer BatchNorm13
I1128 13:09:50.018916  8649 net.cpp:406] BatchNorm13 <- Convolution13
I1128 13:09:50.018934  8649 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I1128 13:09:50.019227  8649 net.cpp:122] Setting up BatchNorm13
I1128 13:09:50.019260  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.019282  8649 net.cpp:137] Memory required for data: 84998272
I1128 13:09:50.019314  8649 layer_factory.hpp:77] Creating layer Scale13
I1128 13:09:50.019335  8649 net.cpp:84] Creating Layer Scale13
I1128 13:09:50.019351  8649 net.cpp:406] Scale13 <- Convolution13
I1128 13:09:50.019366  8649 net.cpp:367] Scale13 -> Convolution13 (in-place)
I1128 13:09:50.019434  8649 layer_factory.hpp:77] Creating layer Scale13
I1128 13:09:50.019594  8649 net.cpp:122] Setting up Scale13
I1128 13:09:50.019608  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.019614  8649 net.cpp:137] Memory required for data: 85801088
I1128 13:09:50.019624  8649 layer_factory.hpp:77] Creating layer ReLU12
I1128 13:09:50.019635  8649 net.cpp:84] Creating Layer ReLU12
I1128 13:09:50.019642  8649 net.cpp:406] ReLU12 <- Convolution13
I1128 13:09:50.019651  8649 net.cpp:367] ReLU12 -> Convolution13 (in-place)
I1128 13:09:50.019922  8649 net.cpp:122] Setting up ReLU12
I1128 13:09:50.019937  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.019943  8649 net.cpp:137] Memory required for data: 86603904
I1128 13:09:50.019950  8649 layer_factory.hpp:77] Creating layer Convolution14
I1128 13:09:50.019973  8649 net.cpp:84] Creating Layer Convolution14
I1128 13:09:50.019980  8649 net.cpp:406] Convolution14 <- Convolution13
I1128 13:09:50.019990  8649 net.cpp:380] Convolution14 -> Convolution14
I1128 13:09:50.021718  8649 net.cpp:122] Setting up Convolution14
I1128 13:09:50.021739  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.021744  8649 net.cpp:137] Memory required for data: 87406720
I1128 13:09:50.021754  8649 layer_factory.hpp:77] Creating layer BatchNorm14
I1128 13:09:50.021761  8649 net.cpp:84] Creating Layer BatchNorm14
I1128 13:09:50.021767  8649 net.cpp:406] BatchNorm14 <- Convolution14
I1128 13:09:50.021775  8649 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I1128 13:09:50.021944  8649 net.cpp:122] Setting up BatchNorm14
I1128 13:09:50.021951  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.021955  8649 net.cpp:137] Memory required for data: 88209536
I1128 13:09:50.021965  8649 layer_factory.hpp:77] Creating layer Scale14
I1128 13:09:50.021975  8649 net.cpp:84] Creating Layer Scale14
I1128 13:09:50.021981  8649 net.cpp:406] Scale14 <- Convolution14
I1128 13:09:50.021988  8649 net.cpp:367] Scale14 -> Convolution14 (in-place)
I1128 13:09:50.022027  8649 layer_factory.hpp:77] Creating layer Scale14
I1128 13:09:50.022122  8649 net.cpp:122] Setting up Scale14
I1128 13:09:50.022130  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.022137  8649 net.cpp:137] Memory required for data: 89012352
I1128 13:09:50.022143  8649 layer_factory.hpp:77] Creating layer Eltwise6
I1128 13:09:50.022152  8649 net.cpp:84] Creating Layer Eltwise6
I1128 13:09:50.022157  8649 net.cpp:406] Eltwise6 <- Convolution14
I1128 13:09:50.022162  8649 net.cpp:406] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I1128 13:09:50.022172  8649 net.cpp:380] Eltwise6 -> Eltwise6
I1128 13:09:50.022199  8649 net.cpp:122] Setting up Eltwise6
I1128 13:09:50.022207  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.022213  8649 net.cpp:137] Memory required for data: 89815168
I1128 13:09:50.022218  8649 layer_factory.hpp:77] Creating layer ReLU13
I1128 13:09:50.022225  8649 net.cpp:84] Creating Layer ReLU13
I1128 13:09:50.022230  8649 net.cpp:406] ReLU13 <- Eltwise6
I1128 13:09:50.022234  8649 net.cpp:367] ReLU13 -> Eltwise6 (in-place)
I1128 13:09:50.022511  8649 net.cpp:122] Setting up ReLU13
I1128 13:09:50.022521  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.022526  8649 net.cpp:137] Memory required for data: 90617984
I1128 13:09:50.022529  8649 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I1128 13:09:50.022536  8649 net.cpp:84] Creating Layer Eltwise6_ReLU13_0_split
I1128 13:09:50.022539  8649 net.cpp:406] Eltwise6_ReLU13_0_split <- Eltwise6
I1128 13:09:50.022545  8649 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I1128 13:09:50.022552  8649 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I1128 13:09:50.022598  8649 net.cpp:122] Setting up Eltwise6_ReLU13_0_split
I1128 13:09:50.022603  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.022608  8649 net.cpp:129] Top shape: 32 32 14 14 (200704)
I1128 13:09:50.022611  8649 net.cpp:137] Memory required for data: 92223616
I1128 13:09:50.022614  8649 layer_factory.hpp:77] Creating layer Convolution15
I1128 13:09:50.022624  8649 net.cpp:84] Creating Layer Convolution15
I1128 13:09:50.022632  8649 net.cpp:406] Convolution15 <- Eltwise6_ReLU13_0_split_0
I1128 13:09:50.022640  8649 net.cpp:380] Convolution15 -> Convolution15
I1128 13:09:50.024075  8649 net.cpp:122] Setting up Convolution15
I1128 13:09:50.024091  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.024096  8649 net.cpp:137] Memory required for data: 92625024
I1128 13:09:50.024104  8649 layer_factory.hpp:77] Creating layer BatchNorm15
I1128 13:09:50.024116  8649 net.cpp:84] Creating Layer BatchNorm15
I1128 13:09:50.024123  8649 net.cpp:406] BatchNorm15 <- Convolution15
I1128 13:09:50.024134  8649 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I1128 13:09:50.024330  8649 net.cpp:122] Setting up BatchNorm15
I1128 13:09:50.024340  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.024345  8649 net.cpp:137] Memory required for data: 93026432
I1128 13:09:50.024354  8649 layer_factory.hpp:77] Creating layer Scale15
I1128 13:09:50.024363  8649 net.cpp:84] Creating Layer Scale15
I1128 13:09:50.024370  8649 net.cpp:406] Scale15 <- Convolution15
I1128 13:09:50.024379  8649 net.cpp:367] Scale15 -> Convolution15 (in-place)
I1128 13:09:50.024421  8649 layer_factory.hpp:77] Creating layer Scale15
I1128 13:09:50.024518  8649 net.cpp:122] Setting up Scale15
I1128 13:09:50.024528  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.024533  8649 net.cpp:137] Memory required for data: 93427840
I1128 13:09:50.024541  8649 layer_factory.hpp:77] Creating layer ReLU14
I1128 13:09:50.024551  8649 net.cpp:84] Creating Layer ReLU14
I1128 13:09:50.024559  8649 net.cpp:406] ReLU14 <- Convolution15
I1128 13:09:50.024570  8649 net.cpp:367] ReLU14 -> Convolution15 (in-place)
I1128 13:09:50.024729  8649 net.cpp:122] Setting up ReLU14
I1128 13:09:50.024739  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.024744  8649 net.cpp:137] Memory required for data: 93829248
I1128 13:09:50.024750  8649 layer_factory.hpp:77] Creating layer Convolution16
I1128 13:09:50.024765  8649 net.cpp:84] Creating Layer Convolution16
I1128 13:09:50.024770  8649 net.cpp:406] Convolution16 <- Convolution15
I1128 13:09:50.024780  8649 net.cpp:380] Convolution16 -> Convolution16
I1128 13:09:50.026041  8649 net.cpp:122] Setting up Convolution16
I1128 13:09:50.026053  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.026058  8649 net.cpp:137] Memory required for data: 94230656
I1128 13:09:50.026068  8649 layer_factory.hpp:77] Creating layer BatchNorm16
I1128 13:09:50.026077  8649 net.cpp:84] Creating Layer BatchNorm16
I1128 13:09:50.026085  8649 net.cpp:406] BatchNorm16 <- Convolution16
I1128 13:09:50.026093  8649 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I1128 13:09:50.026257  8649 net.cpp:122] Setting up BatchNorm16
I1128 13:09:50.026266  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.026271  8649 net.cpp:137] Memory required for data: 94632064
I1128 13:09:50.026283  8649 layer_factory.hpp:77] Creating layer Scale16
I1128 13:09:50.026291  8649 net.cpp:84] Creating Layer Scale16
I1128 13:09:50.026300  8649 net.cpp:406] Scale16 <- Convolution16
I1128 13:09:50.026309  8649 net.cpp:367] Scale16 -> Convolution16 (in-place)
I1128 13:09:50.026352  8649 layer_factory.hpp:77] Creating layer Scale16
I1128 13:09:50.026449  8649 net.cpp:122] Setting up Scale16
I1128 13:09:50.026458  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.026465  8649 net.cpp:137] Memory required for data: 95033472
I1128 13:09:50.026474  8649 layer_factory.hpp:77] Creating layer Pooling2
I1128 13:09:50.026484  8649 net.cpp:84] Creating Layer Pooling2
I1128 13:09:50.026501  8649 net.cpp:406] Pooling2 <- Eltwise6_ReLU13_0_split_1
I1128 13:09:50.026512  8649 net.cpp:380] Pooling2 -> Pooling2
I1128 13:09:50.026553  8649 net.cpp:122] Setting up Pooling2
I1128 13:09:50.026561  8649 net.cpp:129] Top shape: 32 32 7 7 (50176)
I1128 13:09:50.026566  8649 net.cpp:137] Memory required for data: 95234176
I1128 13:09:50.026573  8649 layer_factory.hpp:77] Creating layer Convolution17
I1128 13:09:50.026588  8649 net.cpp:84] Creating Layer Convolution17
I1128 13:09:50.026595  8649 net.cpp:406] Convolution17 <- Pooling2
I1128 13:09:50.026603  8649 net.cpp:380] Convolution17 -> Convolution17
I1128 13:09:50.027776  8649 net.cpp:122] Setting up Convolution17
I1128 13:09:50.027789  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.027794  8649 net.cpp:137] Memory required for data: 95635584
I1128 13:09:50.027803  8649 layer_factory.hpp:77] Creating layer BatchNorm17
I1128 13:09:50.027813  8649 net.cpp:84] Creating Layer BatchNorm17
I1128 13:09:50.027822  8649 net.cpp:406] BatchNorm17 <- Convolution17
I1128 13:09:50.027833  8649 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I1128 13:09:50.027997  8649 net.cpp:122] Setting up BatchNorm17
I1128 13:09:50.028007  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.028012  8649 net.cpp:137] Memory required for data: 96036992
I1128 13:09:50.028023  8649 layer_factory.hpp:77] Creating layer Scale17
I1128 13:09:50.028033  8649 net.cpp:84] Creating Layer Scale17
I1128 13:09:50.028040  8649 net.cpp:406] Scale17 <- Convolution17
I1128 13:09:50.028048  8649 net.cpp:367] Scale17 -> Convolution17 (in-place)
I1128 13:09:50.028089  8649 layer_factory.hpp:77] Creating layer Scale17
I1128 13:09:50.028187  8649 net.cpp:122] Setting up Scale17
I1128 13:09:50.028195  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.028200  8649 net.cpp:137] Memory required for data: 96438400
I1128 13:09:50.028209  8649 layer_factory.hpp:77] Creating layer Eltwise7
I1128 13:09:50.028220  8649 net.cpp:84] Creating Layer Eltwise7
I1128 13:09:50.028247  8649 net.cpp:406] Eltwise7 <- Convolution16
I1128 13:09:50.028254  8649 net.cpp:406] Eltwise7 <- Convolution17
I1128 13:09:50.028265  8649 net.cpp:380] Eltwise7 -> Eltwise7
I1128 13:09:50.028296  8649 net.cpp:122] Setting up Eltwise7
I1128 13:09:50.028306  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.028311  8649 net.cpp:137] Memory required for data: 96839808
I1128 13:09:50.028317  8649 layer_factory.hpp:77] Creating layer ReLU15
I1128 13:09:50.028327  8649 net.cpp:84] Creating Layer ReLU15
I1128 13:09:50.028334  8649 net.cpp:406] ReLU15 <- Eltwise7
I1128 13:09:50.028340  8649 net.cpp:367] ReLU15 -> Eltwise7 (in-place)
I1128 13:09:50.028621  8649 net.cpp:122] Setting up ReLU15
I1128 13:09:50.028632  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.028637  8649 net.cpp:137] Memory required for data: 97241216
I1128 13:09:50.028643  8649 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I1128 13:09:50.028653  8649 net.cpp:84] Creating Layer Eltwise7_ReLU15_0_split
I1128 13:09:50.028661  8649 net.cpp:406] Eltwise7_ReLU15_0_split <- Eltwise7
I1128 13:09:50.028671  8649 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I1128 13:09:50.028681  8649 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I1128 13:09:50.028723  8649 net.cpp:122] Setting up Eltwise7_ReLU15_0_split
I1128 13:09:50.028730  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.028738  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.028744  8649 net.cpp:137] Memory required for data: 98044032
I1128 13:09:50.028749  8649 layer_factory.hpp:77] Creating layer Convolution18
I1128 13:09:50.028764  8649 net.cpp:84] Creating Layer Convolution18
I1128 13:09:50.028770  8649 net.cpp:406] Convolution18 <- Eltwise7_ReLU15_0_split_0
I1128 13:09:50.028779  8649 net.cpp:380] Convolution18 -> Convolution18
I1128 13:09:50.030059  8649 net.cpp:122] Setting up Convolution18
I1128 13:09:50.030079  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.030102  8649 net.cpp:137] Memory required for data: 98445440
I1128 13:09:50.030113  8649 layer_factory.hpp:77] Creating layer BatchNorm18
I1128 13:09:50.030127  8649 net.cpp:84] Creating Layer BatchNorm18
I1128 13:09:50.030133  8649 net.cpp:406] BatchNorm18 <- Convolution18
I1128 13:09:50.030141  8649 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I1128 13:09:50.030320  8649 net.cpp:122] Setting up BatchNorm18
I1128 13:09:50.030330  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.030335  8649 net.cpp:137] Memory required for data: 98846848
I1128 13:09:50.030347  8649 layer_factory.hpp:77] Creating layer Scale18
I1128 13:09:50.030357  8649 net.cpp:84] Creating Layer Scale18
I1128 13:09:50.030364  8649 net.cpp:406] Scale18 <- Convolution18
I1128 13:09:50.030371  8649 net.cpp:367] Scale18 -> Convolution18 (in-place)
I1128 13:09:50.030416  8649 layer_factory.hpp:77] Creating layer Scale18
I1128 13:09:50.030519  8649 net.cpp:122] Setting up Scale18
I1128 13:09:50.030529  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.030536  8649 net.cpp:137] Memory required for data: 99248256
I1128 13:09:50.030544  8649 layer_factory.hpp:77] Creating layer ReLU16
I1128 13:09:50.030552  8649 net.cpp:84] Creating Layer ReLU16
I1128 13:09:50.030577  8649 net.cpp:406] ReLU16 <- Convolution18
I1128 13:09:50.030596  8649 net.cpp:367] ReLU16 -> Convolution18 (in-place)
I1128 13:09:50.031010  8649 net.cpp:122] Setting up ReLU16
I1128 13:09:50.031025  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.031030  8649 net.cpp:137] Memory required for data: 99649664
I1128 13:09:50.031035  8649 layer_factory.hpp:77] Creating layer Convolution19
I1128 13:09:50.031065  8649 net.cpp:84] Creating Layer Convolution19
I1128 13:09:50.031074  8649 net.cpp:406] Convolution19 <- Convolution18
I1128 13:09:50.031083  8649 net.cpp:380] Convolution19 -> Convolution19
I1128 13:09:50.033135  8649 net.cpp:122] Setting up Convolution19
I1128 13:09:50.033190  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.033195  8649 net.cpp:137] Memory required for data: 100051072
I1128 13:09:50.033211  8649 layer_factory.hpp:77] Creating layer BatchNorm19
I1128 13:09:50.033233  8649 net.cpp:84] Creating Layer BatchNorm19
I1128 13:09:50.033242  8649 net.cpp:406] BatchNorm19 <- Convolution19
I1128 13:09:50.033253  8649 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I1128 13:09:50.033638  8649 net.cpp:122] Setting up BatchNorm19
I1128 13:09:50.033740  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.033756  8649 net.cpp:137] Memory required for data: 100452480
I1128 13:09:50.033816  8649 layer_factory.hpp:77] Creating layer Scale19
I1128 13:09:50.033849  8649 net.cpp:84] Creating Layer Scale19
I1128 13:09:50.033866  8649 net.cpp:406] Scale19 <- Convolution19
I1128 13:09:50.033887  8649 net.cpp:367] Scale19 -> Convolution19 (in-place)
I1128 13:09:50.034072  8649 layer_factory.hpp:77] Creating layer Scale19
I1128 13:09:50.034322  8649 net.cpp:122] Setting up Scale19
I1128 13:09:50.034373  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.034391  8649 net.cpp:137] Memory required for data: 100853888
I1128 13:09:50.034417  8649 layer_factory.hpp:77] Creating layer Eltwise8
I1128 13:09:50.034447  8649 net.cpp:84] Creating Layer Eltwise8
I1128 13:09:50.034466  8649 net.cpp:406] Eltwise8 <- Convolution19
I1128 13:09:50.034487  8649 net.cpp:406] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I1128 13:09:50.034508  8649 net.cpp:380] Eltwise8 -> Eltwise8
I1128 13:09:50.034607  8649 net.cpp:122] Setting up Eltwise8
I1128 13:09:50.034631  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.034643  8649 net.cpp:137] Memory required for data: 101255296
I1128 13:09:50.034656  8649 layer_factory.hpp:77] Creating layer ReLU17
I1128 13:09:50.034670  8649 net.cpp:84] Creating Layer ReLU17
I1128 13:09:50.034682  8649 net.cpp:406] ReLU17 <- Eltwise8
I1128 13:09:50.034698  8649 net.cpp:367] ReLU17 -> Eltwise8 (in-place)
I1128 13:09:50.034958  8649 net.cpp:122] Setting up ReLU17
I1128 13:09:50.034971  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.034989  8649 net.cpp:137] Memory required for data: 101656704
I1128 13:09:50.035007  8649 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I1128 13:09:50.035025  8649 net.cpp:84] Creating Layer Eltwise8_ReLU17_0_split
I1128 13:09:50.035040  8649 net.cpp:406] Eltwise8_ReLU17_0_split <- Eltwise8
I1128 13:09:50.035063  8649 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I1128 13:09:50.035076  8649 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I1128 13:09:50.035132  8649 net.cpp:122] Setting up Eltwise8_ReLU17_0_split
I1128 13:09:50.035153  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.035171  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.035185  8649 net.cpp:137] Memory required for data: 102459520
I1128 13:09:50.035199  8649 layer_factory.hpp:77] Creating layer Convolution20
I1128 13:09:50.035224  8649 net.cpp:84] Creating Layer Convolution20
I1128 13:09:50.035233  8649 net.cpp:406] Convolution20 <- Eltwise8_ReLU17_0_split_0
I1128 13:09:50.035243  8649 net.cpp:380] Convolution20 -> Convolution20
I1128 13:09:50.037109  8649 net.cpp:122] Setting up Convolution20
I1128 13:09:50.037176  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.037192  8649 net.cpp:137] Memory required for data: 102860928
I1128 13:09:50.037214  8649 layer_factory.hpp:77] Creating layer BatchNorm20
I1128 13:09:50.037235  8649 net.cpp:84] Creating Layer BatchNorm20
I1128 13:09:50.037255  8649 net.cpp:406] BatchNorm20 <- Convolution20
I1128 13:09:50.037274  8649 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I1128 13:09:50.037544  8649 net.cpp:122] Setting up BatchNorm20
I1128 13:09:50.037566  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.037581  8649 net.cpp:137] Memory required for data: 103262336
I1128 13:09:50.037601  8649 layer_factory.hpp:77] Creating layer Scale20
I1128 13:09:50.037621  8649 net.cpp:84] Creating Layer Scale20
I1128 13:09:50.037636  8649 net.cpp:406] Scale20 <- Convolution20
I1128 13:09:50.037654  8649 net.cpp:367] Scale20 -> Convolution20 (in-place)
I1128 13:09:50.037722  8649 layer_factory.hpp:77] Creating layer Scale20
I1128 13:09:50.037883  8649 net.cpp:122] Setting up Scale20
I1128 13:09:50.037904  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.037919  8649 net.cpp:137] Memory required for data: 103663744
I1128 13:09:50.037935  8649 layer_factory.hpp:77] Creating layer ReLU18
I1128 13:09:50.037952  8649 net.cpp:84] Creating Layer ReLU18
I1128 13:09:50.037966  8649 net.cpp:406] ReLU18 <- Convolution20
I1128 13:09:50.037983  8649 net.cpp:367] ReLU18 -> Convolution20 (in-place)
I1128 13:09:50.038429  8649 net.cpp:122] Setting up ReLU18
I1128 13:09:50.038461  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.038477  8649 net.cpp:137] Memory required for data: 104065152
I1128 13:09:50.038492  8649 layer_factory.hpp:77] Creating layer Convolution21
I1128 13:09:50.038517  8649 net.cpp:84] Creating Layer Convolution21
I1128 13:09:50.038537  8649 net.cpp:406] Convolution21 <- Convolution20
I1128 13:09:50.038558  8649 net.cpp:380] Convolution21 -> Convolution21
I1128 13:09:50.040217  8649 net.cpp:122] Setting up Convolution21
I1128 13:09:50.040276  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.040282  8649 net.cpp:137] Memory required for data: 104466560
I1128 13:09:50.040293  8649 layer_factory.hpp:77] Creating layer BatchNorm21
I1128 13:09:50.040307  8649 net.cpp:84] Creating Layer BatchNorm21
I1128 13:09:50.040330  8649 net.cpp:406] BatchNorm21 <- Convolution21
I1128 13:09:50.040351  8649 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I1128 13:09:50.040555  8649 net.cpp:122] Setting up BatchNorm21
I1128 13:09:50.040565  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.040570  8649 net.cpp:137] Memory required for data: 104867968
I1128 13:09:50.040580  8649 layer_factory.hpp:77] Creating layer Scale21
I1128 13:09:50.040601  8649 net.cpp:84] Creating Layer Scale21
I1128 13:09:50.040622  8649 net.cpp:406] Scale21 <- Convolution21
I1128 13:09:50.040639  8649 net.cpp:367] Scale21 -> Convolution21 (in-place)
I1128 13:09:50.040696  8649 layer_factory.hpp:77] Creating layer Scale21
I1128 13:09:50.040812  8649 net.cpp:122] Setting up Scale21
I1128 13:09:50.040822  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.040827  8649 net.cpp:137] Memory required for data: 105269376
I1128 13:09:50.040834  8649 layer_factory.hpp:77] Creating layer Eltwise9
I1128 13:09:50.040845  8649 net.cpp:84] Creating Layer Eltwise9
I1128 13:09:50.040851  8649 net.cpp:406] Eltwise9 <- Convolution21
I1128 13:09:50.040858  8649 net.cpp:406] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I1128 13:09:50.040869  8649 net.cpp:380] Eltwise9 -> Eltwise9
I1128 13:09:50.040904  8649 net.cpp:122] Setting up Eltwise9
I1128 13:09:50.040910  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.040915  8649 net.cpp:137] Memory required for data: 105670784
I1128 13:09:50.040921  8649 layer_factory.hpp:77] Creating layer ReLU19
I1128 13:09:50.040935  8649 net.cpp:84] Creating Layer ReLU19
I1128 13:09:50.040941  8649 net.cpp:406] ReLU19 <- Eltwise9
I1128 13:09:50.040949  8649 net.cpp:367] ReLU19 -> Eltwise9 (in-place)
I1128 13:09:50.041132  8649 net.cpp:122] Setting up ReLU19
I1128 13:09:50.041142  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.041148  8649 net.cpp:137] Memory required for data: 106072192
I1128 13:09:50.041154  8649 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
I1128 13:09:50.041165  8649 net.cpp:84] Creating Layer Eltwise9_ReLU19_0_split
I1128 13:09:50.041172  8649 net.cpp:406] Eltwise9_ReLU19_0_split <- Eltwise9
I1128 13:09:50.041184  8649 net.cpp:380] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
I1128 13:09:50.041198  8649 net.cpp:380] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
I1128 13:09:50.041244  8649 net.cpp:122] Setting up Eltwise9_ReLU19_0_split
I1128 13:09:50.041250  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.041257  8649 net.cpp:129] Top shape: 32 64 7 7 (100352)
I1128 13:09:50.041265  8649 net.cpp:137] Memory required for data: 106875008
I1128 13:09:50.041272  8649 layer_factory.hpp:77] Creating layer Convolution22
I1128 13:09:50.041288  8649 net.cpp:84] Creating Layer Convolution22
I1128 13:09:50.041296  8649 net.cpp:406] Convolution22 <- Eltwise9_ReLU19_0_split_0
I1128 13:09:50.041306  8649 net.cpp:380] Convolution22 -> Convolution22
I1128 13:09:50.042565  8649 net.cpp:122] Setting up Convolution22
I1128 13:09:50.042577  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.042580  8649 net.cpp:137] Memory required for data: 107006080
I1128 13:09:50.042592  8649 layer_factory.hpp:77] Creating layer BatchNorm22
I1128 13:09:50.042603  8649 net.cpp:84] Creating Layer BatchNorm22
I1128 13:09:50.042610  8649 net.cpp:406] BatchNorm22 <- Convolution22
I1128 13:09:50.042620  8649 net.cpp:367] BatchNorm22 -> Convolution22 (in-place)
I1128 13:09:50.042798  8649 net.cpp:122] Setting up BatchNorm22
I1128 13:09:50.042805  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.042811  8649 net.cpp:137] Memory required for data: 107137152
I1128 13:09:50.042822  8649 layer_factory.hpp:77] Creating layer Scale22
I1128 13:09:50.042834  8649 net.cpp:84] Creating Layer Scale22
I1128 13:09:50.042840  8649 net.cpp:406] Scale22 <- Convolution22
I1128 13:09:50.042848  8649 net.cpp:367] Scale22 -> Convolution22 (in-place)
I1128 13:09:50.042898  8649 layer_factory.hpp:77] Creating layer Scale22
I1128 13:09:50.043004  8649 net.cpp:122] Setting up Scale22
I1128 13:09:50.043011  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.043017  8649 net.cpp:137] Memory required for data: 107268224
I1128 13:09:50.043025  8649 layer_factory.hpp:77] Creating layer ReLU20
I1128 13:09:50.043036  8649 net.cpp:84] Creating Layer ReLU20
I1128 13:09:50.043045  8649 net.cpp:406] ReLU20 <- Convolution22
I1128 13:09:50.043054  8649 net.cpp:367] ReLU20 -> Convolution22 (in-place)
I1128 13:09:50.043344  8649 net.cpp:122] Setting up ReLU20
I1128 13:09:50.043354  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.043370  8649 net.cpp:137] Memory required for data: 107399296
I1128 13:09:50.043380  8649 layer_factory.hpp:77] Creating layer Convolution23
I1128 13:09:50.043393  8649 net.cpp:84] Creating Layer Convolution23
I1128 13:09:50.043401  8649 net.cpp:406] Convolution23 <- Convolution22
I1128 13:09:50.043413  8649 net.cpp:380] Convolution23 -> Convolution23
I1128 13:09:50.045012  8649 net.cpp:122] Setting up Convolution23
I1128 13:09:50.045028  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.045032  8649 net.cpp:137] Memory required for data: 107530368
I1128 13:09:50.045042  8649 layer_factory.hpp:77] Creating layer BatchNorm23
I1128 13:09:50.045054  8649 net.cpp:84] Creating Layer BatchNorm23
I1128 13:09:50.045061  8649 net.cpp:406] BatchNorm23 <- Convolution23
I1128 13:09:50.045071  8649 net.cpp:367] BatchNorm23 -> Convolution23 (in-place)
I1128 13:09:50.045251  8649 net.cpp:122] Setting up BatchNorm23
I1128 13:09:50.045259  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.045262  8649 net.cpp:137] Memory required for data: 107661440
I1128 13:09:50.045274  8649 layer_factory.hpp:77] Creating layer Scale23
I1128 13:09:50.045284  8649 net.cpp:84] Creating Layer Scale23
I1128 13:09:50.045291  8649 net.cpp:406] Scale23 <- Convolution23
I1128 13:09:50.045301  8649 net.cpp:367] Scale23 -> Convolution23 (in-place)
I1128 13:09:50.045349  8649 layer_factory.hpp:77] Creating layer Scale23
I1128 13:09:50.045459  8649 net.cpp:122] Setting up Scale23
I1128 13:09:50.045467  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.045473  8649 net.cpp:137] Memory required for data: 107792512
I1128 13:09:50.045482  8649 layer_factory.hpp:77] Creating layer Pooling3
I1128 13:09:50.045496  8649 net.cpp:84] Creating Layer Pooling3
I1128 13:09:50.045503  8649 net.cpp:406] Pooling3 <- Eltwise9_ReLU19_0_split_1
I1128 13:09:50.045512  8649 net.cpp:380] Pooling3 -> Pooling3
I1128 13:09:50.045559  8649 net.cpp:122] Setting up Pooling3
I1128 13:09:50.045567  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.045573  8649 net.cpp:137] Memory required for data: 107923584
I1128 13:09:50.045578  8649 layer_factory.hpp:77] Creating layer Convolution24
I1128 13:09:50.045589  8649 net.cpp:84] Creating Layer Convolution24
I1128 13:09:50.045596  8649 net.cpp:406] Convolution24 <- Pooling3
I1128 13:09:50.045608  8649 net.cpp:380] Convolution24 -> Convolution24
I1128 13:09:50.046808  8649 net.cpp:122] Setting up Convolution24
I1128 13:09:50.046823  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.046826  8649 net.cpp:137] Memory required for data: 108054656
I1128 13:09:50.046836  8649 layer_factory.hpp:77] Creating layer BatchNorm24
I1128 13:09:50.046849  8649 net.cpp:84] Creating Layer BatchNorm24
I1128 13:09:50.046856  8649 net.cpp:406] BatchNorm24 <- Convolution24
I1128 13:09:50.046869  8649 net.cpp:367] BatchNorm24 -> Convolution24 (in-place)
I1128 13:09:50.047044  8649 net.cpp:122] Setting up BatchNorm24
I1128 13:09:50.047052  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.047058  8649 net.cpp:137] Memory required for data: 108185728
I1128 13:09:50.047068  8649 layer_factory.hpp:77] Creating layer Scale24
I1128 13:09:50.047080  8649 net.cpp:84] Creating Layer Scale24
I1128 13:09:50.047087  8649 net.cpp:406] Scale24 <- Convolution24
I1128 13:09:50.047098  8649 net.cpp:367] Scale24 -> Convolution24 (in-place)
I1128 13:09:50.047147  8649 layer_factory.hpp:77] Creating layer Scale24
I1128 13:09:50.047253  8649 net.cpp:122] Setting up Scale24
I1128 13:09:50.047261  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.047267  8649 net.cpp:137] Memory required for data: 108316800
I1128 13:09:50.047274  8649 layer_factory.hpp:77] Creating layer Eltwise10
I1128 13:09:50.047287  8649 net.cpp:84] Creating Layer Eltwise10
I1128 13:09:50.047293  8649 net.cpp:406] Eltwise10 <- Convolution23
I1128 13:09:50.047302  8649 net.cpp:406] Eltwise10 <- Convolution24
I1128 13:09:50.047312  8649 net.cpp:380] Eltwise10 -> Eltwise10
I1128 13:09:50.047358  8649 net.cpp:122] Setting up Eltwise10
I1128 13:09:50.047368  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.047372  8649 net.cpp:137] Memory required for data: 108447872
I1128 13:09:50.047377  8649 layer_factory.hpp:77] Creating layer ReLU21
I1128 13:09:50.047391  8649 net.cpp:84] Creating Layer ReLU21
I1128 13:09:50.047399  8649 net.cpp:406] ReLU21 <- Eltwise10
I1128 13:09:50.047407  8649 net.cpp:367] ReLU21 -> Eltwise10 (in-place)
I1128 13:09:50.047714  8649 net.cpp:122] Setting up ReLU21
I1128 13:09:50.047724  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.047731  8649 net.cpp:137] Memory required for data: 108578944
I1128 13:09:50.047736  8649 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
I1128 13:09:50.047747  8649 net.cpp:84] Creating Layer Eltwise10_ReLU21_0_split
I1128 13:09:50.047755  8649 net.cpp:406] Eltwise10_ReLU21_0_split <- Eltwise10
I1128 13:09:50.047765  8649 net.cpp:380] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
I1128 13:09:50.047778  8649 net.cpp:380] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
I1128 13:09:50.047827  8649 net.cpp:122] Setting up Eltwise10_ReLU21_0_split
I1128 13:09:50.047833  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.047840  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.047847  8649 net.cpp:137] Memory required for data: 108841088
I1128 13:09:50.047853  8649 layer_factory.hpp:77] Creating layer Convolution25
I1128 13:09:50.047869  8649 net.cpp:84] Creating Layer Convolution25
I1128 13:09:50.047876  8649 net.cpp:406] Convolution25 <- Eltwise10_ReLU21_0_split_0
I1128 13:09:50.047888  8649 net.cpp:380] Convolution25 -> Convolution25
I1128 13:09:50.049335  8649 net.cpp:122] Setting up Convolution25
I1128 13:09:50.049381  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.049398  8649 net.cpp:137] Memory required for data: 108972160
I1128 13:09:50.049418  8649 layer_factory.hpp:77] Creating layer BatchNorm25
I1128 13:09:50.049438  8649 net.cpp:84] Creating Layer BatchNorm25
I1128 13:09:50.049446  8649 net.cpp:406] BatchNorm25 <- Convolution25
I1128 13:09:50.049458  8649 net.cpp:367] BatchNorm25 -> Convolution25 (in-place)
I1128 13:09:50.049706  8649 net.cpp:122] Setting up BatchNorm25
I1128 13:09:50.049717  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.049724  8649 net.cpp:137] Memory required for data: 109103232
I1128 13:09:50.049738  8649 layer_factory.hpp:77] Creating layer Scale25
I1128 13:09:50.049747  8649 net.cpp:84] Creating Layer Scale25
I1128 13:09:50.049753  8649 net.cpp:406] Scale25 <- Convolution25
I1128 13:09:50.049762  8649 net.cpp:367] Scale25 -> Convolution25 (in-place)
I1128 13:09:50.049814  8649 layer_factory.hpp:77] Creating layer Scale25
I1128 13:09:50.049984  8649 net.cpp:122] Setting up Scale25
I1128 13:09:50.049994  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.050000  8649 net.cpp:137] Memory required for data: 109234304
I1128 13:09:50.050009  8649 layer_factory.hpp:77] Creating layer ReLU22
I1128 13:09:50.050016  8649 net.cpp:84] Creating Layer ReLU22
I1128 13:09:50.050024  8649 net.cpp:406] ReLU22 <- Convolution25
I1128 13:09:50.050031  8649 net.cpp:367] ReLU22 -> Convolution25 (in-place)
I1128 13:09:50.050252  8649 net.cpp:122] Setting up ReLU22
I1128 13:09:50.050264  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.050271  8649 net.cpp:137] Memory required for data: 109365376
I1128 13:09:50.050277  8649 layer_factory.hpp:77] Creating layer Convolution26
I1128 13:09:50.050290  8649 net.cpp:84] Creating Layer Convolution26
I1128 13:09:50.050297  8649 net.cpp:406] Convolution26 <- Convolution25
I1128 13:09:50.050307  8649 net.cpp:380] Convolution26 -> Convolution26
I1128 13:09:50.052364  8649 net.cpp:122] Setting up Convolution26
I1128 13:09:50.052435  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.052454  8649 net.cpp:137] Memory required for data: 109496448
I1128 13:09:50.052479  8649 layer_factory.hpp:77] Creating layer BatchNorm26
I1128 13:09:50.052513  8649 net.cpp:84] Creating Layer BatchNorm26
I1128 13:09:50.052541  8649 net.cpp:406] BatchNorm26 <- Convolution26
I1128 13:09:50.052564  8649 net.cpp:367] BatchNorm26 -> Convolution26 (in-place)
I1128 13:09:50.052832  8649 net.cpp:122] Setting up BatchNorm26
I1128 13:09:50.052846  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.052855  8649 net.cpp:137] Memory required for data: 109627520
I1128 13:09:50.052870  8649 layer_factory.hpp:77] Creating layer Scale26
I1128 13:09:50.052882  8649 net.cpp:84] Creating Layer Scale26
I1128 13:09:50.052891  8649 net.cpp:406] Scale26 <- Convolution26
I1128 13:09:50.052901  8649 net.cpp:367] Scale26 -> Convolution26 (in-place)
I1128 13:09:50.052954  8649 layer_factory.hpp:77] Creating layer Scale26
I1128 13:09:50.053110  8649 net.cpp:122] Setting up Scale26
I1128 13:09:50.053123  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.053130  8649 net.cpp:137] Memory required for data: 109758592
I1128 13:09:50.053139  8649 layer_factory.hpp:77] Creating layer Eltwise11
I1128 13:09:50.053153  8649 net.cpp:84] Creating Layer Eltwise11
I1128 13:09:50.053159  8649 net.cpp:406] Eltwise11 <- Convolution26
I1128 13:09:50.053169  8649 net.cpp:406] Eltwise11 <- Eltwise10_ReLU21_0_split_1
I1128 13:09:50.053179  8649 net.cpp:380] Eltwise11 -> Eltwise11
I1128 13:09:50.053215  8649 net.cpp:122] Setting up Eltwise11
I1128 13:09:50.053225  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.053246  8649 net.cpp:137] Memory required for data: 109889664
I1128 13:09:50.053261  8649 layer_factory.hpp:77] Creating layer ReLU23
I1128 13:09:50.053278  8649 net.cpp:84] Creating Layer ReLU23
I1128 13:09:50.053292  8649 net.cpp:406] ReLU23 <- Eltwise11
I1128 13:09:50.053309  8649 net.cpp:367] ReLU23 -> Eltwise11 (in-place)
I1128 13:09:50.053833  8649 net.cpp:122] Setting up ReLU23
I1128 13:09:50.053849  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.053856  8649 net.cpp:137] Memory required for data: 110020736
I1128 13:09:50.053864  8649 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
I1128 13:09:50.053875  8649 net.cpp:84] Creating Layer Eltwise11_ReLU23_0_split
I1128 13:09:50.053882  8649 net.cpp:406] Eltwise11_ReLU23_0_split <- Eltwise11
I1128 13:09:50.053892  8649 net.cpp:380] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
I1128 13:09:50.053915  8649 net.cpp:380] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
I1128 13:09:50.053975  8649 net.cpp:122] Setting up Eltwise11_ReLU23_0_split
I1128 13:09:50.053987  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.053994  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.054002  8649 net.cpp:137] Memory required for data: 110282880
I1128 13:09:50.054008  8649 layer_factory.hpp:77] Creating layer Convolution27
I1128 13:09:50.054035  8649 net.cpp:84] Creating Layer Convolution27
I1128 13:09:50.054042  8649 net.cpp:406] Convolution27 <- Eltwise11_ReLU23_0_split_0
I1128 13:09:50.054051  8649 net.cpp:380] Convolution27 -> Convolution27
I1128 13:09:50.055963  8649 net.cpp:122] Setting up Convolution27
I1128 13:09:50.055986  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.055991  8649 net.cpp:137] Memory required for data: 110413952
I1128 13:09:50.056004  8649 layer_factory.hpp:77] Creating layer BatchNorm27
I1128 13:09:50.056015  8649 net.cpp:84] Creating Layer BatchNorm27
I1128 13:09:50.056023  8649 net.cpp:406] BatchNorm27 <- Convolution27
I1128 13:09:50.056032  8649 net.cpp:367] BatchNorm27 -> Convolution27 (in-place)
I1128 13:09:50.056304  8649 net.cpp:122] Setting up BatchNorm27
I1128 13:09:50.056334  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.056349  8649 net.cpp:137] Memory required for data: 110545024
I1128 13:09:50.056366  8649 layer_factory.hpp:77] Creating layer Scale27
I1128 13:09:50.056376  8649 net.cpp:84] Creating Layer Scale27
I1128 13:09:50.056381  8649 net.cpp:406] Scale27 <- Convolution27
I1128 13:09:50.056390  8649 net.cpp:367] Scale27 -> Convolution27 (in-place)
I1128 13:09:50.056452  8649 layer_factory.hpp:77] Creating layer Scale27
I1128 13:09:50.056617  8649 net.cpp:122] Setting up Scale27
I1128 13:09:50.056628  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.056634  8649 net.cpp:137] Memory required for data: 110676096
I1128 13:09:50.056643  8649 layer_factory.hpp:77] Creating layer ReLU24
I1128 13:09:50.056653  8649 net.cpp:84] Creating Layer ReLU24
I1128 13:09:50.056658  8649 net.cpp:406] ReLU24 <- Convolution27
I1128 13:09:50.056668  8649 net.cpp:367] ReLU24 -> Convolution27 (in-place)
I1128 13:09:50.056906  8649 net.cpp:122] Setting up ReLU24
I1128 13:09:50.056917  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.056923  8649 net.cpp:137] Memory required for data: 110807168
I1128 13:09:50.056929  8649 layer_factory.hpp:77] Creating layer Convolution28
I1128 13:09:50.056942  8649 net.cpp:84] Creating Layer Convolution28
I1128 13:09:50.056949  8649 net.cpp:406] Convolution28 <- Convolution27
I1128 13:09:50.056958  8649 net.cpp:380] Convolution28 -> Convolution28
I1128 13:09:50.058774  8649 net.cpp:122] Setting up Convolution28
I1128 13:09:50.058797  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.058804  8649 net.cpp:137] Memory required for data: 110938240
I1128 13:09:50.058815  8649 layer_factory.hpp:77] Creating layer BatchNorm28
I1128 13:09:50.058825  8649 net.cpp:84] Creating Layer BatchNorm28
I1128 13:09:50.058850  8649 net.cpp:406] BatchNorm28 <- Convolution28
I1128 13:09:50.058871  8649 net.cpp:367] BatchNorm28 -> Convolution28 (in-place)
I1128 13:09:50.059077  8649 net.cpp:122] Setting up BatchNorm28
I1128 13:09:50.059087  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.059093  8649 net.cpp:137] Memory required for data: 111069312
I1128 13:09:50.059104  8649 layer_factory.hpp:77] Creating layer Scale28
I1128 13:09:50.059113  8649 net.cpp:84] Creating Layer Scale28
I1128 13:09:50.059119  8649 net.cpp:406] Scale28 <- Convolution28
I1128 13:09:50.059126  8649 net.cpp:367] Scale28 -> Convolution28 (in-place)
I1128 13:09:50.059173  8649 layer_factory.hpp:77] Creating layer Scale28
I1128 13:09:50.059289  8649 net.cpp:122] Setting up Scale28
I1128 13:09:50.059299  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.059304  8649 net.cpp:137] Memory required for data: 111200384
I1128 13:09:50.059314  8649 layer_factory.hpp:77] Creating layer Eltwise12
I1128 13:09:50.059324  8649 net.cpp:84] Creating Layer Eltwise12
I1128 13:09:50.059332  8649 net.cpp:406] Eltwise12 <- Convolution28
I1128 13:09:50.059339  8649 net.cpp:406] Eltwise12 <- Eltwise11_ReLU23_0_split_1
I1128 13:09:50.059347  8649 net.cpp:380] Eltwise12 -> Eltwise12
I1128 13:09:50.059382  8649 net.cpp:122] Setting up Eltwise12
I1128 13:09:50.059392  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.059397  8649 net.cpp:137] Memory required for data: 111331456
I1128 13:09:50.059403  8649 layer_factory.hpp:77] Creating layer ReLU25
I1128 13:09:50.059413  8649 net.cpp:84] Creating Layer ReLU25
I1128 13:09:50.059433  8649 net.cpp:406] ReLU25 <- Eltwise12
I1128 13:09:50.059449  8649 net.cpp:367] ReLU25 -> Eltwise12 (in-place)
I1128 13:09:50.059823  8649 net.cpp:122] Setting up ReLU25
I1128 13:09:50.059834  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.059839  8649 net.cpp:137] Memory required for data: 111462528
I1128 13:09:50.059844  8649 layer_factory.hpp:77] Creating layer Eltwise12_ReLU25_0_split
I1128 13:09:50.059854  8649 net.cpp:84] Creating Layer Eltwise12_ReLU25_0_split
I1128 13:09:50.059861  8649 net.cpp:406] Eltwise12_ReLU25_0_split <- Eltwise12
I1128 13:09:50.059875  8649 net.cpp:380] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_0
I1128 13:09:50.059886  8649 net.cpp:380] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_1
I1128 13:09:50.059934  8649 net.cpp:122] Setting up Eltwise12_ReLU25_0_split
I1128 13:09:50.059942  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.059949  8649 net.cpp:129] Top shape: 32 64 4 4 (32768)
I1128 13:09:50.059955  8649 net.cpp:137] Memory required for data: 111724672
I1128 13:09:50.059962  8649 layer_factory.hpp:77] Creating layer Convolution29
I1128 13:09:50.059988  8649 net.cpp:84] Creating Layer Convolution29
I1128 13:09:50.059996  8649 net.cpp:406] Convolution29 <- Eltwise12_ReLU25_0_split_0
I1128 13:09:50.060008  8649 net.cpp:380] Convolution29 -> Convolution29
I1128 13:09:50.061338  8649 net.cpp:122] Setting up Convolution29
I1128 13:09:50.061352  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.061355  8649 net.cpp:137] Memory required for data: 111757440
I1128 13:09:50.061365  8649 layer_factory.hpp:77] Creating layer BatchNorm29
I1128 13:09:50.061377  8649 net.cpp:84] Creating Layer BatchNorm29
I1128 13:09:50.061384  8649 net.cpp:406] BatchNorm29 <- Convolution29
I1128 13:09:50.061393  8649 net.cpp:367] BatchNorm29 -> Convolution29 (in-place)
I1128 13:09:50.061583  8649 net.cpp:122] Setting up BatchNorm29
I1128 13:09:50.061590  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.061594  8649 net.cpp:137] Memory required for data: 111790208
I1128 13:09:50.061604  8649 layer_factory.hpp:77] Creating layer Scale29
I1128 13:09:50.061616  8649 net.cpp:84] Creating Layer Scale29
I1128 13:09:50.061624  8649 net.cpp:406] Scale29 <- Convolution29
I1128 13:09:50.061631  8649 net.cpp:367] Scale29 -> Convolution29 (in-place)
I1128 13:09:50.061682  8649 layer_factory.hpp:77] Creating layer Scale29
I1128 13:09:50.061799  8649 net.cpp:122] Setting up Scale29
I1128 13:09:50.061806  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.061811  8649 net.cpp:137] Memory required for data: 111822976
I1128 13:09:50.061820  8649 layer_factory.hpp:77] Creating layer ReLU26
I1128 13:09:50.061832  8649 net.cpp:84] Creating Layer ReLU26
I1128 13:09:50.061839  8649 net.cpp:406] ReLU26 <- Convolution29
I1128 13:09:50.061848  8649 net.cpp:367] ReLU26 -> Convolution29 (in-place)
I1128 13:09:50.062141  8649 net.cpp:122] Setting up ReLU26
I1128 13:09:50.062151  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.062158  8649 net.cpp:137] Memory required for data: 111855744
I1128 13:09:50.062165  8649 layer_factory.hpp:77] Creating layer Convolution30
I1128 13:09:50.062180  8649 net.cpp:84] Creating Layer Convolution30
I1128 13:09:50.062187  8649 net.cpp:406] Convolution30 <- Convolution29
I1128 13:09:50.062197  8649 net.cpp:380] Convolution30 -> Convolution30
I1128 13:09:50.063413  8649 net.cpp:122] Setting up Convolution30
I1128 13:09:50.063423  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.063429  8649 net.cpp:137] Memory required for data: 111888512
I1128 13:09:50.063441  8649 layer_factory.hpp:77] Creating layer BatchNorm30
I1128 13:09:50.063452  8649 net.cpp:84] Creating Layer BatchNorm30
I1128 13:09:50.063459  8649 net.cpp:406] BatchNorm30 <- Convolution30
I1128 13:09:50.063469  8649 net.cpp:367] BatchNorm30 -> Convolution30 (in-place)
I1128 13:09:50.063652  8649 net.cpp:122] Setting up BatchNorm30
I1128 13:09:50.063659  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.063665  8649 net.cpp:137] Memory required for data: 111921280
I1128 13:09:50.063676  8649 layer_factory.hpp:77] Creating layer Scale30
I1128 13:09:50.063686  8649 net.cpp:84] Creating Layer Scale30
I1128 13:09:50.063694  8649 net.cpp:406] Scale30 <- Convolution30
I1128 13:09:50.063702  8649 net.cpp:367] Scale30 -> Convolution30 (in-place)
I1128 13:09:50.063752  8649 layer_factory.hpp:77] Creating layer Scale30
I1128 13:09:50.063865  8649 net.cpp:122] Setting up Scale30
I1128 13:09:50.063874  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.063879  8649 net.cpp:137] Memory required for data: 111954048
I1128 13:09:50.063887  8649 layer_factory.hpp:77] Creating layer Pooling4
I1128 13:09:50.063900  8649 net.cpp:84] Creating Layer Pooling4
I1128 13:09:50.063908  8649 net.cpp:406] Pooling4 <- Eltwise12_ReLU25_0_split_1
I1128 13:09:50.063918  8649 net.cpp:380] Pooling4 -> Pooling4
I1128 13:09:50.063966  8649 net.cpp:122] Setting up Pooling4
I1128 13:09:50.063973  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.063979  8649 net.cpp:137] Memory required for data: 111986816
I1128 13:09:50.063984  8649 layer_factory.hpp:77] Creating layer Convolution31
I1128 13:09:50.064010  8649 net.cpp:84] Creating Layer Convolution31
I1128 13:09:50.064018  8649 net.cpp:406] Convolution31 <- Pooling4
I1128 13:09:50.064029  8649 net.cpp:380] Convolution31 -> Convolution31
I1128 13:09:50.065006  8649 net.cpp:122] Setting up Convolution31
I1128 13:09:50.065017  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.065021  8649 net.cpp:137] Memory required for data: 112019584
I1128 13:09:50.065032  8649 layer_factory.hpp:77] Creating layer BatchNorm31
I1128 13:09:50.065042  8649 net.cpp:84] Creating Layer BatchNorm31
I1128 13:09:50.065050  8649 net.cpp:406] BatchNorm31 <- Convolution31
I1128 13:09:50.065062  8649 net.cpp:367] BatchNorm31 -> Convolution31 (in-place)
I1128 13:09:50.065244  8649 net.cpp:122] Setting up BatchNorm31
I1128 13:09:50.065251  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.065255  8649 net.cpp:137] Memory required for data: 112052352
I1128 13:09:50.065268  8649 layer_factory.hpp:77] Creating layer Scale31
I1128 13:09:50.065279  8649 net.cpp:84] Creating Layer Scale31
I1128 13:09:50.065286  8649 net.cpp:406] Scale31 <- Convolution31
I1128 13:09:50.065295  8649 net.cpp:367] Scale31 -> Convolution31 (in-place)
I1128 13:09:50.065342  8649 layer_factory.hpp:77] Creating layer Scale31
I1128 13:09:50.065454  8649 net.cpp:122] Setting up Scale31
I1128 13:09:50.065462  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.065467  8649 net.cpp:137] Memory required for data: 112085120
I1128 13:09:50.065477  8649 layer_factory.hpp:77] Creating layer Eltwise13
I1128 13:09:50.065488  8649 net.cpp:84] Creating Layer Eltwise13
I1128 13:09:50.065496  8649 net.cpp:406] Eltwise13 <- Convolution30
I1128 13:09:50.065506  8649 net.cpp:406] Eltwise13 <- Convolution31
I1128 13:09:50.065515  8649 net.cpp:380] Eltwise13 -> Eltwise13
I1128 13:09:50.065551  8649 net.cpp:122] Setting up Eltwise13
I1128 13:09:50.065558  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.065563  8649 net.cpp:137] Memory required for data: 112117888
I1128 13:09:50.065569  8649 layer_factory.hpp:77] Creating layer ReLU27
I1128 13:09:50.065579  8649 net.cpp:84] Creating Layer ReLU27
I1128 13:09:50.065587  8649 net.cpp:406] ReLU27 <- Eltwise13
I1128 13:09:50.065598  8649 net.cpp:367] ReLU27 -> Eltwise13 (in-place)
I1128 13:09:50.065758  8649 net.cpp:122] Setting up ReLU27
I1128 13:09:50.065768  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.065773  8649 net.cpp:137] Memory required for data: 112150656
I1128 13:09:50.065780  8649 layer_factory.hpp:77] Creating layer Eltwise13_ReLU27_0_split
I1128 13:09:50.065793  8649 net.cpp:84] Creating Layer Eltwise13_ReLU27_0_split
I1128 13:09:50.065800  8649 net.cpp:406] Eltwise13_ReLU27_0_split <- Eltwise13
I1128 13:09:50.065811  8649 net.cpp:380] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_0
I1128 13:09:50.065825  8649 net.cpp:380] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_1
I1128 13:09:50.065874  8649 net.cpp:122] Setting up Eltwise13_ReLU27_0_split
I1128 13:09:50.065881  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.065887  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.065892  8649 net.cpp:137] Memory required for data: 112216192
I1128 13:09:50.065899  8649 layer_factory.hpp:77] Creating layer Convolution32
I1128 13:09:50.065914  8649 net.cpp:84] Creating Layer Convolution32
I1128 13:09:50.065922  8649 net.cpp:406] Convolution32 <- Eltwise13_ReLU27_0_split_0
I1128 13:09:50.065934  8649 net.cpp:380] Convolution32 -> Convolution32
I1128 13:09:50.067562  8649 net.cpp:122] Setting up Convolution32
I1128 13:09:50.067620  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.067633  8649 net.cpp:137] Memory required for data: 112248960
I1128 13:09:50.067651  8649 layer_factory.hpp:77] Creating layer BatchNorm32
I1128 13:09:50.067670  8649 net.cpp:84] Creating Layer BatchNorm32
I1128 13:09:50.067685  8649 net.cpp:406] BatchNorm32 <- Convolution32
I1128 13:09:50.067704  8649 net.cpp:367] BatchNorm32 -> Convolution32 (in-place)
I1128 13:09:50.068009  8649 net.cpp:122] Setting up BatchNorm32
I1128 13:09:50.068045  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.068060  8649 net.cpp:137] Memory required for data: 112281728
I1128 13:09:50.068081  8649 layer_factory.hpp:77] Creating layer Scale32
I1128 13:09:50.068100  8649 net.cpp:84] Creating Layer Scale32
I1128 13:09:50.068116  8649 net.cpp:406] Scale32 <- Convolution32
I1128 13:09:50.068135  8649 net.cpp:367] Scale32 -> Convolution32 (in-place)
I1128 13:09:50.068207  8649 layer_factory.hpp:77] Creating layer Scale32
I1128 13:09:50.068351  8649 net.cpp:122] Setting up Scale32
I1128 13:09:50.068361  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.068367  8649 net.cpp:137] Memory required for data: 112314496
I1128 13:09:50.068375  8649 layer_factory.hpp:77] Creating layer ReLU28
I1128 13:09:50.068395  8649 net.cpp:84] Creating Layer ReLU28
I1128 13:09:50.068415  8649 net.cpp:406] ReLU28 <- Convolution32
I1128 13:09:50.068436  8649 net.cpp:367] ReLU28 -> Convolution32 (in-place)
I1128 13:09:50.068851  8649 net.cpp:122] Setting up ReLU28
I1128 13:09:50.068873  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.068878  8649 net.cpp:137] Memory required for data: 112347264
I1128 13:09:50.068884  8649 layer_factory.hpp:77] Creating layer Convolution33
I1128 13:09:50.068907  8649 net.cpp:84] Creating Layer Convolution33
I1128 13:09:50.068936  8649 net.cpp:406] Convolution33 <- Convolution32
I1128 13:09:50.068964  8649 net.cpp:380] Convolution33 -> Convolution33
I1128 13:09:50.071090  8649 net.cpp:122] Setting up Convolution33
I1128 13:09:50.071135  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.071141  8649 net.cpp:137] Memory required for data: 112380032
I1128 13:09:50.071162  8649 layer_factory.hpp:77] Creating layer BatchNorm33
I1128 13:09:50.071183  8649 net.cpp:84] Creating Layer BatchNorm33
I1128 13:09:50.071192  8649 net.cpp:406] BatchNorm33 <- Convolution33
I1128 13:09:50.071209  8649 net.cpp:367] BatchNorm33 -> Convolution33 (in-place)
I1128 13:09:50.071499  8649 net.cpp:122] Setting up BatchNorm33
I1128 13:09:50.071516  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.071521  8649 net.cpp:137] Memory required for data: 112412800
I1128 13:09:50.071533  8649 layer_factory.hpp:77] Creating layer Scale33
I1128 13:09:50.071545  8649 net.cpp:84] Creating Layer Scale33
I1128 13:09:50.071569  8649 net.cpp:406] Scale33 <- Convolution33
I1128 13:09:50.071588  8649 net.cpp:367] Scale33 -> Convolution33 (in-place)
I1128 13:09:50.071688  8649 layer_factory.hpp:77] Creating layer Scale33
I1128 13:09:50.071830  8649 net.cpp:122] Setting up Scale33
I1128 13:09:50.071842  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.071847  8649 net.cpp:137] Memory required for data: 112445568
I1128 13:09:50.071857  8649 layer_factory.hpp:77] Creating layer Eltwise14
I1128 13:09:50.071882  8649 net.cpp:84] Creating Layer Eltwise14
I1128 13:09:50.071898  8649 net.cpp:406] Eltwise14 <- Convolution33
I1128 13:09:50.071916  8649 net.cpp:406] Eltwise14 <- Eltwise13_ReLU27_0_split_1
I1128 13:09:50.071935  8649 net.cpp:380] Eltwise14 -> Eltwise14
I1128 13:09:50.071997  8649 net.cpp:122] Setting up Eltwise14
I1128 13:09:50.072010  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.072016  8649 net.cpp:137] Memory required for data: 112478336
I1128 13:09:50.072033  8649 layer_factory.hpp:77] Creating layer ReLU29
I1128 13:09:50.072052  8649 net.cpp:84] Creating Layer ReLU29
I1128 13:09:50.072069  8649 net.cpp:406] ReLU29 <- Eltwise14
I1128 13:09:50.072087  8649 net.cpp:367] ReLU29 -> Eltwise14 (in-place)
I1128 13:09:50.072409  8649 net.cpp:122] Setting up ReLU29
I1128 13:09:50.072427  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.072432  8649 net.cpp:137] Memory required for data: 112511104
I1128 13:09:50.072438  8649 layer_factory.hpp:77] Creating layer Eltwise14_ReLU29_0_split
I1128 13:09:50.072448  8649 net.cpp:84] Creating Layer Eltwise14_ReLU29_0_split
I1128 13:09:50.072468  8649 net.cpp:406] Eltwise14_ReLU29_0_split <- Eltwise14
I1128 13:09:50.072495  8649 net.cpp:380] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_0
I1128 13:09:50.072516  8649 net.cpp:380] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_1
I1128 13:09:50.072585  8649 net.cpp:122] Setting up Eltwise14_ReLU29_0_split
I1128 13:09:50.072593  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.072600  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.072605  8649 net.cpp:137] Memory required for data: 112576640
I1128 13:09:50.072623  8649 layer_factory.hpp:77] Creating layer Convolution34
I1128 13:09:50.072654  8649 net.cpp:84] Creating Layer Convolution34
I1128 13:09:50.072661  8649 net.cpp:406] Convolution34 <- Eltwise14_ReLU29_0_split_0
I1128 13:09:50.072674  8649 net.cpp:380] Convolution34 -> Convolution34
I1128 13:09:50.074750  8649 net.cpp:122] Setting up Convolution34
I1128 13:09:50.074775  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.074780  8649 net.cpp:137] Memory required for data: 112609408
I1128 13:09:50.074791  8649 layer_factory.hpp:77] Creating layer BatchNorm34
I1128 13:09:50.074805  8649 net.cpp:84] Creating Layer BatchNorm34
I1128 13:09:50.074828  8649 net.cpp:406] BatchNorm34 <- Convolution34
I1128 13:09:50.074846  8649 net.cpp:367] BatchNorm34 -> Convolution34 (in-place)
I1128 13:09:50.075053  8649 net.cpp:122] Setting up BatchNorm34
I1128 13:09:50.075063  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.075068  8649 net.cpp:137] Memory required for data: 112642176
I1128 13:09:50.075078  8649 layer_factory.hpp:77] Creating layer Scale34
I1128 13:09:50.075100  8649 net.cpp:84] Creating Layer Scale34
I1128 13:09:50.075106  8649 net.cpp:406] Scale34 <- Convolution34
I1128 13:09:50.075114  8649 net.cpp:367] Scale34 -> Convolution34 (in-place)
I1128 13:09:50.075175  8649 layer_factory.hpp:77] Creating layer Scale34
I1128 13:09:50.075299  8649 net.cpp:122] Setting up Scale34
I1128 13:09:50.075309  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.075314  8649 net.cpp:137] Memory required for data: 112674944
I1128 13:09:50.075335  8649 layer_factory.hpp:77] Creating layer ReLU30
I1128 13:09:50.075356  8649 net.cpp:84] Creating Layer ReLU30
I1128 13:09:50.075372  8649 net.cpp:406] ReLU30 <- Convolution34
I1128 13:09:50.075388  8649 net.cpp:367] ReLU30 -> Convolution34 (in-place)
I1128 13:09:50.075923  8649 net.cpp:122] Setting up ReLU30
I1128 13:09:50.075961  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.075974  8649 net.cpp:137] Memory required for data: 112707712
I1128 13:09:50.075989  8649 layer_factory.hpp:77] Creating layer Convolution35
I1128 13:09:50.076014  8649 net.cpp:84] Creating Layer Convolution35
I1128 13:09:50.076028  8649 net.cpp:406] Convolution35 <- Convolution34
I1128 13:09:50.076047  8649 net.cpp:380] Convolution35 -> Convolution35
I1128 13:09:50.078140  8649 net.cpp:122] Setting up Convolution35
I1128 13:09:50.078202  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.078217  8649 net.cpp:137] Memory required for data: 112740480
I1128 13:09:50.078238  8649 layer_factory.hpp:77] Creating layer BatchNorm35
I1128 13:09:50.078259  8649 net.cpp:84] Creating Layer BatchNorm35
I1128 13:09:50.078274  8649 net.cpp:406] BatchNorm35 <- Convolution35
I1128 13:09:50.078285  8649 net.cpp:367] BatchNorm35 -> Convolution35 (in-place)
I1128 13:09:50.078560  8649 net.cpp:122] Setting up BatchNorm35
I1128 13:09:50.078575  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.078580  8649 net.cpp:137] Memory required for data: 112773248
I1128 13:09:50.078591  8649 layer_factory.hpp:77] Creating layer Scale35
I1128 13:09:50.078601  8649 net.cpp:84] Creating Layer Scale35
I1128 13:09:50.078608  8649 net.cpp:406] Scale35 <- Convolution35
I1128 13:09:50.078615  8649 net.cpp:367] Scale35 -> Convolution35 (in-place)
I1128 13:09:50.078681  8649 layer_factory.hpp:77] Creating layer Scale35
I1128 13:09:50.078884  8649 net.cpp:122] Setting up Scale35
I1128 13:09:50.078897  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.078902  8649 net.cpp:137] Memory required for data: 112806016
I1128 13:09:50.078925  8649 layer_factory.hpp:77] Creating layer Eltwise15
I1128 13:09:50.078936  8649 net.cpp:84] Creating Layer Eltwise15
I1128 13:09:50.078943  8649 net.cpp:406] Eltwise15 <- Convolution35
I1128 13:09:50.078949  8649 net.cpp:406] Eltwise15 <- Eltwise14_ReLU29_0_split_1
I1128 13:09:50.078958  8649 net.cpp:380] Eltwise15 -> Eltwise15
I1128 13:09:50.079004  8649 net.cpp:122] Setting up Eltwise15
I1128 13:09:50.079013  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.079020  8649 net.cpp:137] Memory required for data: 112838784
I1128 13:09:50.079025  8649 layer_factory.hpp:77] Creating layer ReLU31
I1128 13:09:50.079035  8649 net.cpp:84] Creating Layer ReLU31
I1128 13:09:50.079041  8649 net.cpp:406] ReLU31 <- Eltwise15
I1128 13:09:50.079049  8649 net.cpp:367] ReLU31 -> Eltwise15 (in-place)
I1128 13:09:50.079584  8649 net.cpp:122] Setting up ReLU31
I1128 13:09:50.079602  8649 net.cpp:129] Top shape: 32 64 2 2 (8192)
I1128 13:09:50.079607  8649 net.cpp:137] Memory required for data: 112871552
I1128 13:09:50.079613  8649 layer_factory.hpp:77] Creating layer Pooling5
I1128 13:09:50.079625  8649 net.cpp:84] Creating Layer Pooling5
I1128 13:09:50.079632  8649 net.cpp:406] Pooling5 <- Eltwise15
I1128 13:09:50.079641  8649 net.cpp:380] Pooling5 -> Pooling5
I1128 13:09:50.079955  8649 net.cpp:122] Setting up Pooling5
I1128 13:09:50.079969  8649 net.cpp:129] Top shape: 32 64 1 1 (2048)
I1128 13:09:50.079974  8649 net.cpp:137] Memory required for data: 112879744
I1128 13:09:50.079982  8649 layer_factory.hpp:77] Creating layer InnerProduct1
I1128 13:09:50.079993  8649 net.cpp:84] Creating Layer InnerProduct1
I1128 13:09:50.079999  8649 net.cpp:406] InnerProduct1 <- Pooling5
I1128 13:09:50.080010  8649 net.cpp:380] InnerProduct1 -> InnerProduct1
I1128 13:09:50.080188  8649 net.cpp:122] Setting up InnerProduct1
I1128 13:09:50.080199  8649 net.cpp:129] Top shape: 32 10 (320)
I1128 13:09:50.080204  8649 net.cpp:137] Memory required for data: 112881024
I1128 13:09:50.080217  8649 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1128 13:09:50.080237  8649 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1128 13:09:50.080243  8649 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1
I1128 13:09:50.080250  8649 net.cpp:406] SoftmaxWithLoss1 <- Data2
I1128 13:09:50.080260  8649 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1128 13:09:50.080276  8649 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1128 13:09:50.080906  8649 net.cpp:122] Setting up SoftmaxWithLoss1
I1128 13:09:50.080922  8649 net.cpp:129] Top shape: (1)
I1128 13:09:50.080927  8649 net.cpp:132]     with loss weight 1
I1128 13:09:50.080948  8649 net.cpp:137] Memory required for data: 112881028
I1128 13:09:50.080953  8649 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1128 13:09:50.080963  8649 net.cpp:198] InnerProduct1 needs backward computation.
I1128 13:09:50.080970  8649 net.cpp:198] Pooling5 needs backward computation.
I1128 13:09:50.080976  8649 net.cpp:198] ReLU31 needs backward computation.
I1128 13:09:50.080981  8649 net.cpp:198] Eltwise15 needs backward computation.
I1128 13:09:50.080987  8649 net.cpp:198] Scale35 needs backward computation.
I1128 13:09:50.080991  8649 net.cpp:198] BatchNorm35 needs backward computation.
I1128 13:09:50.080997  8649 net.cpp:198] Convolution35 needs backward computation.
I1128 13:09:50.081002  8649 net.cpp:198] ReLU30 needs backward computation.
I1128 13:09:50.081008  8649 net.cpp:198] Scale34 needs backward computation.
I1128 13:09:50.081013  8649 net.cpp:198] BatchNorm34 needs backward computation.
I1128 13:09:50.081019  8649 net.cpp:198] Convolution34 needs backward computation.
I1128 13:09:50.081025  8649 net.cpp:198] Eltwise14_ReLU29_0_split needs backward computation.
I1128 13:09:50.081032  8649 net.cpp:198] ReLU29 needs backward computation.
I1128 13:09:50.081038  8649 net.cpp:198] Eltwise14 needs backward computation.
I1128 13:09:50.081045  8649 net.cpp:198] Scale33 needs backward computation.
I1128 13:09:50.081050  8649 net.cpp:198] BatchNorm33 needs backward computation.
I1128 13:09:50.081056  8649 net.cpp:198] Convolution33 needs backward computation.
I1128 13:09:50.081074  8649 net.cpp:198] ReLU28 needs backward computation.
I1128 13:09:50.081096  8649 net.cpp:198] Scale32 needs backward computation.
I1128 13:09:50.081110  8649 net.cpp:198] BatchNorm32 needs backward computation.
I1128 13:09:50.081122  8649 net.cpp:198] Convolution32 needs backward computation.
I1128 13:09:50.081136  8649 net.cpp:198] Eltwise13_ReLU27_0_split needs backward computation.
I1128 13:09:50.081151  8649 net.cpp:198] ReLU27 needs backward computation.
I1128 13:09:50.081163  8649 net.cpp:198] Eltwise13 needs backward computation.
I1128 13:09:50.081176  8649 net.cpp:198] Scale31 needs backward computation.
I1128 13:09:50.081189  8649 net.cpp:198] BatchNorm31 needs backward computation.
I1128 13:09:50.081203  8649 net.cpp:198] Convolution31 needs backward computation.
I1128 13:09:50.081218  8649 net.cpp:198] Pooling4 needs backward computation.
I1128 13:09:50.081243  8649 net.cpp:198] Scale30 needs backward computation.
I1128 13:09:50.081262  8649 net.cpp:198] BatchNorm30 needs backward computation.
I1128 13:09:50.081276  8649 net.cpp:198] Convolution30 needs backward computation.
I1128 13:09:50.081290  8649 net.cpp:198] ReLU26 needs backward computation.
I1128 13:09:50.081305  8649 net.cpp:198] Scale29 needs backward computation.
I1128 13:09:50.081318  8649 net.cpp:198] BatchNorm29 needs backward computation.
I1128 13:09:50.081332  8649 net.cpp:198] Convolution29 needs backward computation.
I1128 13:09:50.081347  8649 net.cpp:198] Eltwise12_ReLU25_0_split needs backward computation.
I1128 13:09:50.081362  8649 net.cpp:198] ReLU25 needs backward computation.
I1128 13:09:50.081377  8649 net.cpp:198] Eltwise12 needs backward computation.
I1128 13:09:50.081390  8649 net.cpp:198] Scale28 needs backward computation.
I1128 13:09:50.081403  8649 net.cpp:198] BatchNorm28 needs backward computation.
I1128 13:09:50.081415  8649 net.cpp:198] Convolution28 needs backward computation.
I1128 13:09:50.081430  8649 net.cpp:198] ReLU24 needs backward computation.
I1128 13:09:50.081445  8649 net.cpp:198] Scale27 needs backward computation.
I1128 13:09:50.081460  8649 net.cpp:198] BatchNorm27 needs backward computation.
I1128 13:09:50.081475  8649 net.cpp:198] Convolution27 needs backward computation.
I1128 13:09:50.081487  8649 net.cpp:198] Eltwise11_ReLU23_0_split needs backward computation.
I1128 13:09:50.081501  8649 net.cpp:198] ReLU23 needs backward computation.
I1128 13:09:50.081513  8649 net.cpp:198] Eltwise11 needs backward computation.
I1128 13:09:50.081523  8649 net.cpp:198] Scale26 needs backward computation.
I1128 13:09:50.081529  8649 net.cpp:198] BatchNorm26 needs backward computation.
I1128 13:09:50.081533  8649 net.cpp:198] Convolution26 needs backward computation.
I1128 13:09:50.081538  8649 net.cpp:198] ReLU22 needs backward computation.
I1128 13:09:50.081545  8649 net.cpp:198] Scale25 needs backward computation.
I1128 13:09:50.081552  8649 net.cpp:198] BatchNorm25 needs backward computation.
I1128 13:09:50.081557  8649 net.cpp:198] Convolution25 needs backward computation.
I1128 13:09:50.081562  8649 net.cpp:198] Eltwise10_ReLU21_0_split needs backward computation.
I1128 13:09:50.081569  8649 net.cpp:198] ReLU21 needs backward computation.
I1128 13:09:50.081575  8649 net.cpp:198] Eltwise10 needs backward computation.
I1128 13:09:50.081581  8649 net.cpp:198] Scale24 needs backward computation.
I1128 13:09:50.081588  8649 net.cpp:198] BatchNorm24 needs backward computation.
I1128 13:09:50.081594  8649 net.cpp:198] Convolution24 needs backward computation.
I1128 13:09:50.081601  8649 net.cpp:198] Pooling3 needs backward computation.
I1128 13:09:50.081609  8649 net.cpp:198] Scale23 needs backward computation.
I1128 13:09:50.081614  8649 net.cpp:198] BatchNorm23 needs backward computation.
I1128 13:09:50.081620  8649 net.cpp:198] Convolution23 needs backward computation.
I1128 13:09:50.081626  8649 net.cpp:198] ReLU20 needs backward computation.
I1128 13:09:50.081632  8649 net.cpp:198] Scale22 needs backward computation.
I1128 13:09:50.081647  8649 net.cpp:198] BatchNorm22 needs backward computation.
I1128 13:09:50.081653  8649 net.cpp:198] Convolution22 needs backward computation.
I1128 13:09:50.081660  8649 net.cpp:198] Eltwise9_ReLU19_0_split needs backward computation.
I1128 13:09:50.081666  8649 net.cpp:198] ReLU19 needs backward computation.
I1128 13:09:50.081672  8649 net.cpp:198] Eltwise9 needs backward computation.
I1128 13:09:50.081678  8649 net.cpp:198] Scale21 needs backward computation.
I1128 13:09:50.081684  8649 net.cpp:198] BatchNorm21 needs backward computation.
I1128 13:09:50.081691  8649 net.cpp:198] Convolution21 needs backward computation.
I1128 13:09:50.081696  8649 net.cpp:198] ReLU18 needs backward computation.
I1128 13:09:50.081702  8649 net.cpp:198] Scale20 needs backward computation.
I1128 13:09:50.081708  8649 net.cpp:198] BatchNorm20 needs backward computation.
I1128 13:09:50.081715  8649 net.cpp:198] Convolution20 needs backward computation.
I1128 13:09:50.081720  8649 net.cpp:198] Eltwise8_ReLU17_0_split needs backward computation.
I1128 13:09:50.081727  8649 net.cpp:198] ReLU17 needs backward computation.
I1128 13:09:50.081732  8649 net.cpp:198] Eltwise8 needs backward computation.
I1128 13:09:50.081738  8649 net.cpp:198] Scale19 needs backward computation.
I1128 13:09:50.081746  8649 net.cpp:198] BatchNorm19 needs backward computation.
I1128 13:09:50.081753  8649 net.cpp:198] Convolution19 needs backward computation.
I1128 13:09:50.081758  8649 net.cpp:198] ReLU16 needs backward computation.
I1128 13:09:50.081765  8649 net.cpp:198] Scale18 needs backward computation.
I1128 13:09:50.081771  8649 net.cpp:198] BatchNorm18 needs backward computation.
I1128 13:09:50.081778  8649 net.cpp:198] Convolution18 needs backward computation.
I1128 13:09:50.081785  8649 net.cpp:198] Eltwise7_ReLU15_0_split needs backward computation.
I1128 13:09:50.081791  8649 net.cpp:198] ReLU15 needs backward computation.
I1128 13:09:50.081799  8649 net.cpp:198] Eltwise7 needs backward computation.
I1128 13:09:50.081805  8649 net.cpp:198] Scale17 needs backward computation.
I1128 13:09:50.081811  8649 net.cpp:198] BatchNorm17 needs backward computation.
I1128 13:09:50.081817  8649 net.cpp:198] Convolution17 needs backward computation.
I1128 13:09:50.081825  8649 net.cpp:198] Pooling2 needs backward computation.
I1128 13:09:50.081830  8649 net.cpp:198] Scale16 needs backward computation.
I1128 13:09:50.081836  8649 net.cpp:198] BatchNorm16 needs backward computation.
I1128 13:09:50.081843  8649 net.cpp:198] Convolution16 needs backward computation.
I1128 13:09:50.081849  8649 net.cpp:198] ReLU14 needs backward computation.
I1128 13:09:50.081856  8649 net.cpp:198] Scale15 needs backward computation.
I1128 13:09:50.081861  8649 net.cpp:198] BatchNorm15 needs backward computation.
I1128 13:09:50.081866  8649 net.cpp:198] Convolution15 needs backward computation.
I1128 13:09:50.081872  8649 net.cpp:198] Eltwise6_ReLU13_0_split needs backward computation.
I1128 13:09:50.081878  8649 net.cpp:198] ReLU13 needs backward computation.
I1128 13:09:50.081884  8649 net.cpp:198] Eltwise6 needs backward computation.
I1128 13:09:50.081892  8649 net.cpp:198] Scale14 needs backward computation.
I1128 13:09:50.081897  8649 net.cpp:198] BatchNorm14 needs backward computation.
I1128 13:09:50.081903  8649 net.cpp:198] Convolution14 needs backward computation.
I1128 13:09:50.081909  8649 net.cpp:198] ReLU12 needs backward computation.
I1128 13:09:50.081915  8649 net.cpp:198] Scale13 needs backward computation.
I1128 13:09:50.081921  8649 net.cpp:198] BatchNorm13 needs backward computation.
I1128 13:09:50.081928  8649 net.cpp:198] Convolution13 needs backward computation.
I1128 13:09:50.081933  8649 net.cpp:198] Eltwise5_ReLU11_0_split needs backward computation.
I1128 13:09:50.081940  8649 net.cpp:198] ReLU11 needs backward computation.
I1128 13:09:50.081946  8649 net.cpp:198] Eltwise5 needs backward computation.
I1128 13:09:50.081954  8649 net.cpp:198] Scale12 needs backward computation.
I1128 13:09:50.081960  8649 net.cpp:198] BatchNorm12 needs backward computation.
I1128 13:09:50.081972  8649 net.cpp:198] Convolution12 needs backward computation.
I1128 13:09:50.081977  8649 net.cpp:198] ReLU10 needs backward computation.
I1128 13:09:50.081984  8649 net.cpp:198] Scale11 needs backward computation.
I1128 13:09:50.081990  8649 net.cpp:198] BatchNorm11 needs backward computation.
I1128 13:09:50.081995  8649 net.cpp:198] Convolution11 needs backward computation.
I1128 13:09:50.082002  8649 net.cpp:198] Eltwise4_ReLU9_0_split needs backward computation.
I1128 13:09:50.082008  8649 net.cpp:198] ReLU9 needs backward computation.
I1128 13:09:50.082015  8649 net.cpp:198] Eltwise4 needs backward computation.
I1128 13:09:50.082021  8649 net.cpp:198] Scale10 needs backward computation.
I1128 13:09:50.082027  8649 net.cpp:198] BatchNorm10 needs backward computation.
I1128 13:09:50.082032  8649 net.cpp:198] Convolution10 needs backward computation.
I1128 13:09:50.082038  8649 net.cpp:198] Pooling1 needs backward computation.
I1128 13:09:50.082046  8649 net.cpp:198] Scale9 needs backward computation.
I1128 13:09:50.082051  8649 net.cpp:198] BatchNorm9 needs backward computation.
I1128 13:09:50.082057  8649 net.cpp:198] Convolution9 needs backward computation.
I1128 13:09:50.082067  8649 net.cpp:198] ReLU8 needs backward computation.
I1128 13:09:50.082072  8649 net.cpp:198] Scale8 needs backward computation.
I1128 13:09:50.082077  8649 net.cpp:198] BatchNorm8 needs backward computation.
I1128 13:09:50.082082  8649 net.cpp:198] Convolution8 needs backward computation.
I1128 13:09:50.082088  8649 net.cpp:198] Eltwise3_ReLU7_0_split needs backward computation.
I1128 13:09:50.082094  8649 net.cpp:198] ReLU7 needs backward computation.
I1128 13:09:50.082100  8649 net.cpp:198] Eltwise3 needs backward computation.
I1128 13:09:50.082108  8649 net.cpp:198] Scale7 needs backward computation.
I1128 13:09:50.082114  8649 net.cpp:198] BatchNorm7 needs backward computation.
I1128 13:09:50.082120  8649 net.cpp:198] Convolution7 needs backward computation.
I1128 13:09:50.082126  8649 net.cpp:198] ReLU6 needs backward computation.
I1128 13:09:50.082134  8649 net.cpp:198] Scale6 needs backward computation.
I1128 13:09:50.082139  8649 net.cpp:198] BatchNorm6 needs backward computation.
I1128 13:09:50.082145  8649 net.cpp:198] Convolution6 needs backward computation.
I1128 13:09:50.082152  8649 net.cpp:198] Eltwise2_ReLU5_0_split needs backward computation.
I1128 13:09:50.082159  8649 net.cpp:198] ReLU5 needs backward computation.
I1128 13:09:50.082165  8649 net.cpp:198] Eltwise2 needs backward computation.
I1128 13:09:50.082172  8649 net.cpp:198] Scale5 needs backward computation.
I1128 13:09:50.082180  8649 net.cpp:198] BatchNorm5 needs backward computation.
I1128 13:09:50.082185  8649 net.cpp:198] Convolution5 needs backward computation.
I1128 13:09:50.082191  8649 net.cpp:198] ReLU4 needs backward computation.
I1128 13:09:50.082197  8649 net.cpp:198] Scale4 needs backward computation.
I1128 13:09:50.082202  8649 net.cpp:198] BatchNorm4 needs backward computation.
I1128 13:09:50.082208  8649 net.cpp:198] Convolution4 needs backward computation.
I1128 13:09:50.082213  8649 net.cpp:198] Eltwise1_ReLU3_0_split needs backward computation.
I1128 13:09:50.082221  8649 net.cpp:198] ReLU3 needs backward computation.
I1128 13:09:50.082227  8649 net.cpp:198] Eltwise1 needs backward computation.
I1128 13:09:50.082234  8649 net.cpp:198] Scale3 needs backward computation.
I1128 13:09:50.082240  8649 net.cpp:198] BatchNorm3 needs backward computation.
I1128 13:09:50.082247  8649 net.cpp:198] Convolution3 needs backward computation.
I1128 13:09:50.082252  8649 net.cpp:198] ReLU2 needs backward computation.
I1128 13:09:50.082258  8649 net.cpp:198] Scale2 needs backward computation.
I1128 13:09:50.082264  8649 net.cpp:198] BatchNorm2 needs backward computation.
I1128 13:09:50.082271  8649 net.cpp:198] Convolution2 needs backward computation.
I1128 13:09:50.082278  8649 net.cpp:198] Convolution1_ReLU1_0_split needs backward computation.
I1128 13:09:50.082284  8649 net.cpp:198] ReLU1 needs backward computation.
I1128 13:09:50.082293  8649 net.cpp:198] Scale1 needs backward computation.
I1128 13:09:50.082306  8649 net.cpp:198] BatchNorm1 needs backward computation.
I1128 13:09:50.082314  8649 net.cpp:198] Convolution1 needs backward computation.
I1128 13:09:50.082320  8649 net.cpp:200] Data1 does not need backward computation.
I1128 13:09:50.082326  8649 net.cpp:242] This network produces output SoftmaxWithLoss1
I1128 13:09:50.082433  8649 net.cpp:255] Network initialization done.
I1128 13:09:50.083653  8649 solver.cpp:172] Creating test net (#0) specified by test_net file: /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/test.prototxt
I1128 13:09:50.084455  8649 net.cpp:51] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  transform_param {
    crop_size: 28
  }
  data_param {
    source: "/home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/test_lmdb"
    batch_size: 10
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution3"
  bottom: "Convolution1"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Convolution5"
  bottom: "Eltwise1"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Convolution7"
  bottom: "Eltwise2"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise3"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution10"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution9"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Convolution12"
  bottom: "Eltwise4"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Convolution14"
  bottom: "Eltwise5"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution15"
  top: "Convolution15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Convolution15"
  top: "Convolution16"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Eltwise6"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution17"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution16"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Convolution19"
  bottom: "Eltwise7"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution20"
  top: "Convolution20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Convolution21"
  bottom: "Eltwise8"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution22"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution22"
  top: "Convolution22"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Convolution22"
  top: "Convolution23"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Pooling3"
  top: "Convolution24"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Convolution24"
  top: "Convolution24"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Convolution23"
  bottom: "Convolution24"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution25"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution25"
  top: "Convolution25"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Convolution25"
  top: "Convolution26"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Convolution26"
  top: "Convolution26"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "Convolution26"
  top: "Convolution26"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Convolution26"
  bottom: "Eltwise10"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution27"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "Convolution27"
  top: "Convolution27"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Convolution27"
  top: "Convolution28"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm28"
  type: "BatchNorm"
  bottom: "Convolution28"
  top: "Convolution28"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale28"
  type: "Scale"
  bottom: "Convolution28"
  top: "Convolution28"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Convolution28"
  bottom: "Eltwise11"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "Eltwise12"
  top: "Eltwise12"
}
layer {
  name: "Convolution29"
  type: "Convolution"
  bottom: "Eltwise12"
  top: "Convolution29"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm29"
  type: "BatchNorm"
  bottom: "Convolution29"
  top: "Convolution29"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale29"
  type: "Scale"
  bottom: "Convolution29"
  top: "Convolution29"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "Convolution29"
  top: "Convolution29"
}
layer {
  name: "Convolution30"
  type: "Convolution"
  bottom: "Convolution29"
  top: "Convolution30"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm30"
  type: "BatchNorm"
  bottom: "Convolution30"
  top: "Convolution30"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale30"
  type: "Scale"
  bottom: "Convolution30"
  top: "Convolution30"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Pooling4"
  type: "Pooling"
  bottom: "Eltwise12"
  top: "Pooling4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
    pad: 0
  }
}
layer {
  name: "Convolution31"
  type: "Convolution"
  bottom: "Pooling4"
  top: "Convolution31"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm31"
  type: "BatchNorm"
  bottom: "Convolution31"
  top: "Convolution31"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale31"
  type: "Scale"
  bottom: "Convolution31"
  top: "Convolution31"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise13"
  type: "Eltwise"
  bottom: "Convolution30"
  bottom: "Convolution31"
  top: "Eltwise13"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU27"
  type: "ReLU"
  bottom: "Eltwise13"
  top: "Eltwise13"
}
layer {
  name: "Convolution32"
  type: "Convolution"
  bottom: "Eltwise13"
  top: "Convolution32"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm32"
  type: "BatchNorm"
  bottom: "Convolution32"
  top: "Convolution32"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale32"
  type: "Scale"
  bottom: "Convolution32"
  top: "Convolution32"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU28"
  type: "ReLU"
  bottom: "Convolution32"
  top: "Convolution32"
}
layer {
  name: "Convolution33"
  type: "Convolution"
  bottom: "Convolution32"
  top: "Convolution33"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm33"
  type: "BatchNorm"
  bottom: "Convolution33"
  top: "Convolution33"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale33"
  type: "Scale"
  bottom: "Convolution33"
  top: "Convolution33"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise14"
  type: "Eltwise"
  bottom: "Convolution33"
  bottom: "Eltwise13"
  top: "Eltwise14"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU29"
  type: "ReLU"
  bottom: "Eltwise14"
  top: "Eltwise14"
}
layer {
  name: "Convolution34"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution34"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm34"
  type: "BatchNorm"
  bottom: "Convolution34"
  top: "Convolution34"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale34"
  type: "Scale"
  bottom: "Convolution34"
  top: "Convolution34"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU30"
  type: "ReLU"
  bottom: "Convolution34"
  top: "Convolution34"
}
layer {
  name: "Convolution35"
  type: "Convolution"
  bottom: "Convolution34"
  top: "Convolution35"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm35"
  type: "BatchNorm"
  bottom: "Convolution35"
  top: "Convolution35"
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "Scale35"
  type: "Scale"
  bottom: "Convolution35"
  top: "Convolution35"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise15"
  type: "Eltwise"
  bottom: "Convolution35"
  bottom: "Eltwise14"
  top: "Eltwise15"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU31"
  type: "ReLU"
  bottom: "Eltwise15"
  top: "Eltwise15"
}
layer {
  name: "Pooling5"
  type: "Pooling"
  bottom: "Eltwise15"
  top: "Pooling5"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling5"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Accuracy1"
  type: "Accuracy"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "Accuracy1"
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
I1128 13:09:50.085187  8649 layer_factory.hpp:77] Creating layer Data1
I1128 13:09:50.085260  8649 db_lmdb.cpp:35] Opened lmdb /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/test_lmdb
I1128 13:09:50.085276  8649 net.cpp:84] Creating Layer Data1
I1128 13:09:50.085283  8649 net.cpp:380] Data1 -> Data1
I1128 13:09:50.085295  8649 net.cpp:380] Data1 -> Data2
I1128 13:09:50.085486  8649 data_layer.cpp:45] output data size: 10,3,28,28
I1128 13:09:50.086261  8649 net.cpp:122] Setting up Data1
I1128 13:09:50.086278  8649 net.cpp:129] Top shape: 10 3 28 28 (23520)
I1128 13:09:50.086284  8649 net.cpp:129] Top shape: 10 (10)
I1128 13:09:50.086288  8649 net.cpp:137] Memory required for data: 94120
I1128 13:09:50.086295  8649 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I1128 13:09:50.086318  8649 net.cpp:84] Creating Layer Data2_Data1_1_split
I1128 13:09:50.086325  8649 net.cpp:406] Data2_Data1_1_split <- Data2
I1128 13:09:50.086334  8649 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_0
I1128 13:09:50.086349  8649 net.cpp:380] Data2_Data1_1_split -> Data2_Data1_1_split_1
I1128 13:09:50.086431  8649 net.cpp:122] Setting up Data2_Data1_1_split
I1128 13:09:50.086441  8649 net.cpp:129] Top shape: 10 (10)
I1128 13:09:50.086447  8649 net.cpp:129] Top shape: 10 (10)
I1128 13:09:50.086452  8649 net.cpp:137] Memory required for data: 94200
I1128 13:09:50.086457  8649 layer_factory.hpp:77] Creating layer Convolution1
I1128 13:09:50.086472  8649 net.cpp:84] Creating Layer Convolution1
I1128 13:09:50.086478  8649 net.cpp:406] Convolution1 <- Data1
I1128 13:09:50.086488  8649 net.cpp:380] Convolution1 -> Convolution1
I1128 13:09:50.088390  8649 net.cpp:122] Setting up Convolution1
I1128 13:09:50.088412  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.088418  8649 net.cpp:137] Memory required for data: 595960
I1128 13:09:50.088433  8649 layer_factory.hpp:77] Creating layer BatchNorm1
I1128 13:09:50.088451  8649 net.cpp:84] Creating Layer BatchNorm1
I1128 13:09:50.088457  8649 net.cpp:406] BatchNorm1 <- Convolution1
I1128 13:09:50.088465  8649 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I1128 13:09:50.088798  8649 net.cpp:122] Setting up BatchNorm1
I1128 13:09:50.088809  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.088812  8649 net.cpp:137] Memory required for data: 1097720
I1128 13:09:50.088826  8649 layer_factory.hpp:77] Creating layer Scale1
I1128 13:09:50.088836  8649 net.cpp:84] Creating Layer Scale1
I1128 13:09:50.088841  8649 net.cpp:406] Scale1 <- Convolution1
I1128 13:09:50.088847  8649 net.cpp:367] Scale1 -> Convolution1 (in-place)
I1128 13:09:50.088907  8649 layer_factory.hpp:77] Creating layer Scale1
I1128 13:09:50.089087  8649 net.cpp:122] Setting up Scale1
I1128 13:09:50.089102  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.089107  8649 net.cpp:137] Memory required for data: 1599480
I1128 13:09:50.089115  8649 layer_factory.hpp:77] Creating layer ReLU1
I1128 13:09:50.089123  8649 net.cpp:84] Creating Layer ReLU1
I1128 13:09:50.089128  8649 net.cpp:406] ReLU1 <- Convolution1
I1128 13:09:50.089134  8649 net.cpp:367] ReLU1 -> Convolution1 (in-place)
I1128 13:09:50.089382  8649 net.cpp:122] Setting up ReLU1
I1128 13:09:50.089393  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.089398  8649 net.cpp:137] Memory required for data: 2101240
I1128 13:09:50.089403  8649 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I1128 13:09:50.089411  8649 net.cpp:84] Creating Layer Convolution1_ReLU1_0_split
I1128 13:09:50.089416  8649 net.cpp:406] Convolution1_ReLU1_0_split <- Convolution1
I1128 13:09:50.089424  8649 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I1128 13:09:50.089433  8649 net.cpp:380] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I1128 13:09:50.089484  8649 net.cpp:122] Setting up Convolution1_ReLU1_0_split
I1128 13:09:50.089493  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.089499  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.089504  8649 net.cpp:137] Memory required for data: 3104760
I1128 13:09:50.089509  8649 layer_factory.hpp:77] Creating layer Convolution2
I1128 13:09:50.089521  8649 net.cpp:84] Creating Layer Convolution2
I1128 13:09:50.089526  8649 net.cpp:406] Convolution2 <- Convolution1_ReLU1_0_split_0
I1128 13:09:50.089534  8649 net.cpp:380] Convolution2 -> Convolution2
I1128 13:09:50.091182  8649 net.cpp:122] Setting up Convolution2
I1128 13:09:50.091203  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.091208  8649 net.cpp:137] Memory required for data: 3606520
I1128 13:09:50.091230  8649 layer_factory.hpp:77] Creating layer BatchNorm2
I1128 13:09:50.091244  8649 net.cpp:84] Creating Layer BatchNorm2
I1128 13:09:50.091249  8649 net.cpp:406] BatchNorm2 <- Convolution2
I1128 13:09:50.091258  8649 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I1128 13:09:50.091550  8649 net.cpp:122] Setting up BatchNorm2
I1128 13:09:50.091559  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.091564  8649 net.cpp:137] Memory required for data: 4108280
I1128 13:09:50.091573  8649 layer_factory.hpp:77] Creating layer Scale2
I1128 13:09:50.091583  8649 net.cpp:84] Creating Layer Scale2
I1128 13:09:50.091588  8649 net.cpp:406] Scale2 <- Convolution2
I1128 13:09:50.091594  8649 net.cpp:367] Scale2 -> Convolution2 (in-place)
I1128 13:09:50.091652  8649 layer_factory.hpp:77] Creating layer Scale2
I1128 13:09:50.091816  8649 net.cpp:122] Setting up Scale2
I1128 13:09:50.091825  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.091830  8649 net.cpp:137] Memory required for data: 4610040
I1128 13:09:50.091837  8649 layer_factory.hpp:77] Creating layer ReLU2
I1128 13:09:50.091846  8649 net.cpp:84] Creating Layer ReLU2
I1128 13:09:50.091851  8649 net.cpp:406] ReLU2 <- Convolution2
I1128 13:09:50.091857  8649 net.cpp:367] ReLU2 -> Convolution2 (in-place)
I1128 13:09:50.092316  8649 net.cpp:122] Setting up ReLU2
I1128 13:09:50.092330  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.092341  8649 net.cpp:137] Memory required for data: 5111800
I1128 13:09:50.092348  8649 layer_factory.hpp:77] Creating layer Convolution3
I1128 13:09:50.092362  8649 net.cpp:84] Creating Layer Convolution3
I1128 13:09:50.092371  8649 net.cpp:406] Convolution3 <- Convolution2
I1128 13:09:50.092381  8649 net.cpp:380] Convolution3 -> Convolution3
I1128 13:09:50.093961  8649 net.cpp:122] Setting up Convolution3
I1128 13:09:50.093979  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.093984  8649 net.cpp:137] Memory required for data: 5613560
I1128 13:09:50.093994  8649 layer_factory.hpp:77] Creating layer BatchNorm3
I1128 13:09:50.094005  8649 net.cpp:84] Creating Layer BatchNorm3
I1128 13:09:50.094013  8649 net.cpp:406] BatchNorm3 <- Convolution3
I1128 13:09:50.094022  8649 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I1128 13:09:50.094293  8649 net.cpp:122] Setting up BatchNorm3
I1128 13:09:50.094303  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.094310  8649 net.cpp:137] Memory required for data: 6115320
I1128 13:09:50.094323  8649 layer_factory.hpp:77] Creating layer Scale3
I1128 13:09:50.094332  8649 net.cpp:84] Creating Layer Scale3
I1128 13:09:50.094338  8649 net.cpp:406] Scale3 <- Convolution3
I1128 13:09:50.094347  8649 net.cpp:367] Scale3 -> Convolution3 (in-place)
I1128 13:09:50.094403  8649 layer_factory.hpp:77] Creating layer Scale3
I1128 13:09:50.094558  8649 net.cpp:122] Setting up Scale3
I1128 13:09:50.094568  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.094574  8649 net.cpp:137] Memory required for data: 6617080
I1128 13:09:50.094583  8649 layer_factory.hpp:77] Creating layer Eltwise1
I1128 13:09:50.094594  8649 net.cpp:84] Creating Layer Eltwise1
I1128 13:09:50.094599  8649 net.cpp:406] Eltwise1 <- Convolution3
I1128 13:09:50.094606  8649 net.cpp:406] Eltwise1 <- Convolution1_ReLU1_0_split_1
I1128 13:09:50.094616  8649 net.cpp:380] Eltwise1 -> Eltwise1
I1128 13:09:50.094647  8649 net.cpp:122] Setting up Eltwise1
I1128 13:09:50.094656  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.094662  8649 net.cpp:137] Memory required for data: 7118840
I1128 13:09:50.094668  8649 layer_factory.hpp:77] Creating layer ReLU3
I1128 13:09:50.094677  8649 net.cpp:84] Creating Layer ReLU3
I1128 13:09:50.094683  8649 net.cpp:406] ReLU3 <- Eltwise1
I1128 13:09:50.094691  8649 net.cpp:367] ReLU3 -> Eltwise1 (in-place)
I1128 13:09:50.095093  8649 net.cpp:122] Setting up ReLU3
I1128 13:09:50.095106  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.095113  8649 net.cpp:137] Memory required for data: 7620600
I1128 13:09:50.095119  8649 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I1128 13:09:50.095131  8649 net.cpp:84] Creating Layer Eltwise1_ReLU3_0_split
I1128 13:09:50.095137  8649 net.cpp:406] Eltwise1_ReLU3_0_split <- Eltwise1
I1128 13:09:50.095160  8649 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I1128 13:09:50.095170  8649 net.cpp:380] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I1128 13:09:50.095223  8649 net.cpp:122] Setting up Eltwise1_ReLU3_0_split
I1128 13:09:50.095232  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.095240  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.095245  8649 net.cpp:137] Memory required for data: 8624120
I1128 13:09:50.095252  8649 layer_factory.hpp:77] Creating layer Convolution4
I1128 13:09:50.095268  8649 net.cpp:84] Creating Layer Convolution4
I1128 13:09:50.095274  8649 net.cpp:406] Convolution4 <- Eltwise1_ReLU3_0_split_0
I1128 13:09:50.095283  8649 net.cpp:380] Convolution4 -> Convolution4
I1128 13:09:50.096819  8649 net.cpp:122] Setting up Convolution4
I1128 13:09:50.096837  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.096843  8649 net.cpp:137] Memory required for data: 9125880
I1128 13:09:50.096853  8649 layer_factory.hpp:77] Creating layer BatchNorm4
I1128 13:09:50.096863  8649 net.cpp:84] Creating Layer BatchNorm4
I1128 13:09:50.096868  8649 net.cpp:406] BatchNorm4 <- Convolution4
I1128 13:09:50.096879  8649 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I1128 13:09:50.097157  8649 net.cpp:122] Setting up BatchNorm4
I1128 13:09:50.097170  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.097177  8649 net.cpp:137] Memory required for data: 9627640
I1128 13:09:50.097187  8649 layer_factory.hpp:77] Creating layer Scale4
I1128 13:09:50.097196  8649 net.cpp:84] Creating Layer Scale4
I1128 13:09:50.097203  8649 net.cpp:406] Scale4 <- Convolution4
I1128 13:09:50.097211  8649 net.cpp:367] Scale4 -> Convolution4 (in-place)
I1128 13:09:50.097266  8649 layer_factory.hpp:77] Creating layer Scale4
I1128 13:09:50.097425  8649 net.cpp:122] Setting up Scale4
I1128 13:09:50.097435  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.097441  8649 net.cpp:137] Memory required for data: 10129400
I1128 13:09:50.097450  8649 layer_factory.hpp:77] Creating layer ReLU4
I1128 13:09:50.097458  8649 net.cpp:84] Creating Layer ReLU4
I1128 13:09:50.097465  8649 net.cpp:406] ReLU4 <- Convolution4
I1128 13:09:50.097473  8649 net.cpp:367] ReLU4 -> Convolution4 (in-place)
I1128 13:09:50.097703  8649 net.cpp:122] Setting up ReLU4
I1128 13:09:50.097715  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.097721  8649 net.cpp:137] Memory required for data: 10631160
I1128 13:09:50.097728  8649 layer_factory.hpp:77] Creating layer Convolution5
I1128 13:09:50.097743  8649 net.cpp:84] Creating Layer Convolution5
I1128 13:09:50.097748  8649 net.cpp:406] Convolution5 <- Convolution4
I1128 13:09:50.097759  8649 net.cpp:380] Convolution5 -> Convolution5
I1128 13:09:50.099351  8649 net.cpp:122] Setting up Convolution5
I1128 13:09:50.099372  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.099376  8649 net.cpp:137] Memory required for data: 11132920
I1128 13:09:50.099385  8649 layer_factory.hpp:77] Creating layer BatchNorm5
I1128 13:09:50.099393  8649 net.cpp:84] Creating Layer BatchNorm5
I1128 13:09:50.099400  8649 net.cpp:406] BatchNorm5 <- Convolution5
I1128 13:09:50.099406  8649 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I1128 13:09:50.099614  8649 net.cpp:122] Setting up BatchNorm5
I1128 13:09:50.099622  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.099625  8649 net.cpp:137] Memory required for data: 11634680
I1128 13:09:50.099637  8649 layer_factory.hpp:77] Creating layer Scale5
I1128 13:09:50.099643  8649 net.cpp:84] Creating Layer Scale5
I1128 13:09:50.099647  8649 net.cpp:406] Scale5 <- Convolution5
I1128 13:09:50.099653  8649 net.cpp:367] Scale5 -> Convolution5 (in-place)
I1128 13:09:50.099694  8649 layer_factory.hpp:77] Creating layer Scale5
I1128 13:09:50.099805  8649 net.cpp:122] Setting up Scale5
I1128 13:09:50.099813  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.099817  8649 net.cpp:137] Memory required for data: 12136440
I1128 13:09:50.099834  8649 layer_factory.hpp:77] Creating layer Eltwise2
I1128 13:09:50.099843  8649 net.cpp:84] Creating Layer Eltwise2
I1128 13:09:50.099848  8649 net.cpp:406] Eltwise2 <- Convolution5
I1128 13:09:50.099851  8649 net.cpp:406] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I1128 13:09:50.099858  8649 net.cpp:380] Eltwise2 -> Eltwise2
I1128 13:09:50.099884  8649 net.cpp:122] Setting up Eltwise2
I1128 13:09:50.099891  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.099895  8649 net.cpp:137] Memory required for data: 12638200
I1128 13:09:50.099898  8649 layer_factory.hpp:77] Creating layer ReLU5
I1128 13:09:50.099905  8649 net.cpp:84] Creating Layer ReLU5
I1128 13:09:50.099908  8649 net.cpp:406] ReLU5 <- Eltwise2
I1128 13:09:50.099912  8649 net.cpp:367] ReLU5 -> Eltwise2 (in-place)
I1128 13:09:50.100597  8649 net.cpp:122] Setting up ReLU5
I1128 13:09:50.100613  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.100616  8649 net.cpp:137] Memory required for data: 13139960
I1128 13:09:50.100620  8649 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I1128 13:09:50.100628  8649 net.cpp:84] Creating Layer Eltwise2_ReLU5_0_split
I1128 13:09:50.100634  8649 net.cpp:406] Eltwise2_ReLU5_0_split <- Eltwise2
I1128 13:09:50.100643  8649 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I1128 13:09:50.100654  8649 net.cpp:380] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I1128 13:09:50.100713  8649 net.cpp:122] Setting up Eltwise2_ReLU5_0_split
I1128 13:09:50.100721  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.100725  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.100728  8649 net.cpp:137] Memory required for data: 14143480
I1128 13:09:50.100733  8649 layer_factory.hpp:77] Creating layer Convolution6
I1128 13:09:50.100746  8649 net.cpp:84] Creating Layer Convolution6
I1128 13:09:50.100754  8649 net.cpp:406] Convolution6 <- Eltwise2_ReLU5_0_split_0
I1128 13:09:50.100764  8649 net.cpp:380] Convolution6 -> Convolution6
I1128 13:09:50.102219  8649 net.cpp:122] Setting up Convolution6
I1128 13:09:50.102242  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.102246  8649 net.cpp:137] Memory required for data: 14645240
I1128 13:09:50.102257  8649 layer_factory.hpp:77] Creating layer BatchNorm6
I1128 13:09:50.102270  8649 net.cpp:84] Creating Layer BatchNorm6
I1128 13:09:50.102277  8649 net.cpp:406] BatchNorm6 <- Convolution6
I1128 13:09:50.102288  8649 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I1128 13:09:50.102505  8649 net.cpp:122] Setting up BatchNorm6
I1128 13:09:50.102514  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.102519  8649 net.cpp:137] Memory required for data: 15147000
I1128 13:09:50.102527  8649 layer_factory.hpp:77] Creating layer Scale6
I1128 13:09:50.102537  8649 net.cpp:84] Creating Layer Scale6
I1128 13:09:50.102543  8649 net.cpp:406] Scale6 <- Convolution6
I1128 13:09:50.102551  8649 net.cpp:367] Scale6 -> Convolution6 (in-place)
I1128 13:09:50.102607  8649 layer_factory.hpp:77] Creating layer Scale6
I1128 13:09:50.102735  8649 net.cpp:122] Setting up Scale6
I1128 13:09:50.102743  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.102749  8649 net.cpp:137] Memory required for data: 15648760
I1128 13:09:50.102757  8649 layer_factory.hpp:77] Creating layer ReLU6
I1128 13:09:50.102766  8649 net.cpp:84] Creating Layer ReLU6
I1128 13:09:50.102772  8649 net.cpp:406] ReLU6 <- Convolution6
I1128 13:09:50.102780  8649 net.cpp:367] ReLU6 -> Convolution6 (in-place)
I1128 13:09:50.102964  8649 net.cpp:122] Setting up ReLU6
I1128 13:09:50.102973  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.102979  8649 net.cpp:137] Memory required for data: 16150520
I1128 13:09:50.102984  8649 layer_factory.hpp:77] Creating layer Convolution7
I1128 13:09:50.102999  8649 net.cpp:84] Creating Layer Convolution7
I1128 13:09:50.103008  8649 net.cpp:406] Convolution7 <- Convolution6
I1128 13:09:50.103018  8649 net.cpp:380] Convolution7 -> Convolution7
I1128 13:09:50.104420  8649 net.cpp:122] Setting up Convolution7
I1128 13:09:50.104451  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.104456  8649 net.cpp:137] Memory required for data: 16652280
I1128 13:09:50.104466  8649 layer_factory.hpp:77] Creating layer BatchNorm7
I1128 13:09:50.104485  8649 net.cpp:84] Creating Layer BatchNorm7
I1128 13:09:50.104492  8649 net.cpp:406] BatchNorm7 <- Convolution7
I1128 13:09:50.104502  8649 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I1128 13:09:50.104714  8649 net.cpp:122] Setting up BatchNorm7
I1128 13:09:50.104723  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.104727  8649 net.cpp:137] Memory required for data: 17154040
I1128 13:09:50.104739  8649 layer_factory.hpp:77] Creating layer Scale7
I1128 13:09:50.104748  8649 net.cpp:84] Creating Layer Scale7
I1128 13:09:50.104755  8649 net.cpp:406] Scale7 <- Convolution7
I1128 13:09:50.104763  8649 net.cpp:367] Scale7 -> Convolution7 (in-place)
I1128 13:09:50.104816  8649 layer_factory.hpp:77] Creating layer Scale7
I1128 13:09:50.104946  8649 net.cpp:122] Setting up Scale7
I1128 13:09:50.104955  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.104960  8649 net.cpp:137] Memory required for data: 17655800
I1128 13:09:50.104969  8649 layer_factory.hpp:77] Creating layer Eltwise3
I1128 13:09:50.104977  8649 net.cpp:84] Creating Layer Eltwise3
I1128 13:09:50.104984  8649 net.cpp:406] Eltwise3 <- Convolution7
I1128 13:09:50.104992  8649 net.cpp:406] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I1128 13:09:50.105001  8649 net.cpp:380] Eltwise3 -> Eltwise3
I1128 13:09:50.105036  8649 net.cpp:122] Setting up Eltwise3
I1128 13:09:50.105043  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.105049  8649 net.cpp:137] Memory required for data: 18157560
I1128 13:09:50.105053  8649 layer_factory.hpp:77] Creating layer ReLU7
I1128 13:09:50.105062  8649 net.cpp:84] Creating Layer ReLU7
I1128 13:09:50.105070  8649 net.cpp:406] ReLU7 <- Eltwise3
I1128 13:09:50.105078  8649 net.cpp:367] ReLU7 -> Eltwise3 (in-place)
I1128 13:09:50.105406  8649 net.cpp:122] Setting up ReLU7
I1128 13:09:50.105417  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.105420  8649 net.cpp:137] Memory required for data: 18659320
I1128 13:09:50.105425  8649 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I1128 13:09:50.105437  8649 net.cpp:84] Creating Layer Eltwise3_ReLU7_0_split
I1128 13:09:50.105444  8649 net.cpp:406] Eltwise3_ReLU7_0_split <- Eltwise3
I1128 13:09:50.105454  8649 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I1128 13:09:50.105464  8649 net.cpp:380] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I1128 13:09:50.105515  8649 net.cpp:122] Setting up Eltwise3_ReLU7_0_split
I1128 13:09:50.105523  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.105531  8649 net.cpp:129] Top shape: 10 16 28 28 (125440)
I1128 13:09:50.105537  8649 net.cpp:137] Memory required for data: 19662840
I1128 13:09:50.105542  8649 layer_factory.hpp:77] Creating layer Convolution8
I1128 13:09:50.105556  8649 net.cpp:84] Creating Layer Convolution8
I1128 13:09:50.105562  8649 net.cpp:406] Convolution8 <- Eltwise3_ReLU7_0_split_0
I1128 13:09:50.105571  8649 net.cpp:380] Convolution8 -> Convolution8
I1128 13:09:50.106825  8649 net.cpp:122] Setting up Convolution8
I1128 13:09:50.106844  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.106849  8649 net.cpp:137] Memory required for data: 19913720
I1128 13:09:50.106860  8649 layer_factory.hpp:77] Creating layer BatchNorm8
I1128 13:09:50.106871  8649 net.cpp:84] Creating Layer BatchNorm8
I1128 13:09:50.106878  8649 net.cpp:406] BatchNorm8 <- Convolution8
I1128 13:09:50.106890  8649 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I1128 13:09:50.107184  8649 net.cpp:122] Setting up BatchNorm8
I1128 13:09:50.107197  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.107203  8649 net.cpp:137] Memory required for data: 20164600
I1128 13:09:50.107214  8649 layer_factory.hpp:77] Creating layer Scale8
I1128 13:09:50.107224  8649 net.cpp:84] Creating Layer Scale8
I1128 13:09:50.107244  8649 net.cpp:406] Scale8 <- Convolution8
I1128 13:09:50.107252  8649 net.cpp:367] Scale8 -> Convolution8 (in-place)
I1128 13:09:50.107666  8649 layer_factory.hpp:77] Creating layer Scale8
I1128 13:09:50.107785  8649 net.cpp:122] Setting up Scale8
I1128 13:09:50.107795  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.107801  8649 net.cpp:137] Memory required for data: 20415480
I1128 13:09:50.107810  8649 layer_factory.hpp:77] Creating layer ReLU8
I1128 13:09:50.107820  8649 net.cpp:84] Creating Layer ReLU8
I1128 13:09:50.107826  8649 net.cpp:406] ReLU8 <- Convolution8
I1128 13:09:50.107836  8649 net.cpp:367] ReLU8 -> Convolution8 (in-place)
I1128 13:09:50.108199  8649 net.cpp:122] Setting up ReLU8
I1128 13:09:50.108209  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.108217  8649 net.cpp:137] Memory required for data: 20666360
I1128 13:09:50.108234  8649 layer_factory.hpp:77] Creating layer Convolution9
I1128 13:09:50.108252  8649 net.cpp:84] Creating Layer Convolution9
I1128 13:09:50.108258  8649 net.cpp:406] Convolution9 <- Convolution8
I1128 13:09:50.108266  8649 net.cpp:380] Convolution9 -> Convolution9
I1128 13:09:50.109455  8649 net.cpp:122] Setting up Convolution9
I1128 13:09:50.109469  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.109473  8649 net.cpp:137] Memory required for data: 20917240
I1128 13:09:50.109483  8649 layer_factory.hpp:77] Creating layer BatchNorm9
I1128 13:09:50.109493  8649 net.cpp:84] Creating Layer BatchNorm9
I1128 13:09:50.109499  8649 net.cpp:406] BatchNorm9 <- Convolution9
I1128 13:09:50.109508  8649 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I1128 13:09:50.109668  8649 net.cpp:122] Setting up BatchNorm9
I1128 13:09:50.109676  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.109680  8649 net.cpp:137] Memory required for data: 21168120
I1128 13:09:50.109689  8649 layer_factory.hpp:77] Creating layer Scale9
I1128 13:09:50.109700  8649 net.cpp:84] Creating Layer Scale9
I1128 13:09:50.109709  8649 net.cpp:406] Scale9 <- Convolution9
I1128 13:09:50.109720  8649 net.cpp:367] Scale9 -> Convolution9 (in-place)
I1128 13:09:50.109771  8649 layer_factory.hpp:77] Creating layer Scale9
I1128 13:09:50.109876  8649 net.cpp:122] Setting up Scale9
I1128 13:09:50.109885  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.109889  8649 net.cpp:137] Memory required for data: 21419000
I1128 13:09:50.109897  8649 layer_factory.hpp:77] Creating layer Pooling1
I1128 13:09:50.109907  8649 net.cpp:84] Creating Layer Pooling1
I1128 13:09:50.109915  8649 net.cpp:406] Pooling1 <- Eltwise3_ReLU7_0_split_1
I1128 13:09:50.109926  8649 net.cpp:380] Pooling1 -> Pooling1
I1128 13:09:50.109972  8649 net.cpp:122] Setting up Pooling1
I1128 13:09:50.109980  8649 net.cpp:129] Top shape: 10 16 14 14 (31360)
I1128 13:09:50.109983  8649 net.cpp:137] Memory required for data: 21544440
I1128 13:09:50.109988  8649 layer_factory.hpp:77] Creating layer Convolution10
I1128 13:09:50.110003  8649 net.cpp:84] Creating Layer Convolution10
I1128 13:09:50.110011  8649 net.cpp:406] Convolution10 <- Pooling1
I1128 13:09:50.110019  8649 net.cpp:380] Convolution10 -> Convolution10
I1128 13:09:50.111663  8649 net.cpp:122] Setting up Convolution10
I1128 13:09:50.111708  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.111711  8649 net.cpp:137] Memory required for data: 21795320
I1128 13:09:50.111742  8649 layer_factory.hpp:77] Creating layer BatchNorm10
I1128 13:09:50.111764  8649 net.cpp:84] Creating Layer BatchNorm10
I1128 13:09:50.111773  8649 net.cpp:406] BatchNorm10 <- Convolution10
I1128 13:09:50.111783  8649 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I1128 13:09:50.112011  8649 net.cpp:122] Setting up BatchNorm10
I1128 13:09:50.112022  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.112027  8649 net.cpp:137] Memory required for data: 22046200
I1128 13:09:50.112040  8649 layer_factory.hpp:77] Creating layer Scale10
I1128 13:09:50.112051  8649 net.cpp:84] Creating Layer Scale10
I1128 13:09:50.112074  8649 net.cpp:406] Scale10 <- Convolution10
I1128 13:09:50.112082  8649 net.cpp:367] Scale10 -> Convolution10 (in-place)
I1128 13:09:50.112148  8649 layer_factory.hpp:77] Creating layer Scale10
I1128 13:09:50.112304  8649 net.cpp:122] Setting up Scale10
I1128 13:09:50.112316  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.112320  8649 net.cpp:137] Memory required for data: 22297080
I1128 13:09:50.112329  8649 layer_factory.hpp:77] Creating layer Eltwise4
I1128 13:09:50.112340  8649 net.cpp:84] Creating Layer Eltwise4
I1128 13:09:50.112347  8649 net.cpp:406] Eltwise4 <- Convolution9
I1128 13:09:50.112354  8649 net.cpp:406] Eltwise4 <- Convolution10
I1128 13:09:50.112363  8649 net.cpp:380] Eltwise4 -> Eltwise4
I1128 13:09:50.112397  8649 net.cpp:122] Setting up Eltwise4
I1128 13:09:50.112406  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.112411  8649 net.cpp:137] Memory required for data: 22547960
I1128 13:09:50.112416  8649 layer_factory.hpp:77] Creating layer ReLU9
I1128 13:09:50.112426  8649 net.cpp:84] Creating Layer ReLU9
I1128 13:09:50.112432  8649 net.cpp:406] ReLU9 <- Eltwise4
I1128 13:09:50.112438  8649 net.cpp:367] ReLU9 -> Eltwise4 (in-place)
I1128 13:09:50.112787  8649 net.cpp:122] Setting up ReLU9
I1128 13:09:50.112810  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.112814  8649 net.cpp:137] Memory required for data: 22798840
I1128 13:09:50.112820  8649 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I1128 13:09:50.112831  8649 net.cpp:84] Creating Layer Eltwise4_ReLU9_0_split
I1128 13:09:50.112836  8649 net.cpp:406] Eltwise4_ReLU9_0_split <- Eltwise4
I1128 13:09:50.112848  8649 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I1128 13:09:50.112861  8649 net.cpp:380] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I1128 13:09:50.112920  8649 net.cpp:122] Setting up Eltwise4_ReLU9_0_split
I1128 13:09:50.112927  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.112932  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.112936  8649 net.cpp:137] Memory required for data: 23300600
I1128 13:09:50.112941  8649 layer_factory.hpp:77] Creating layer Convolution11
I1128 13:09:50.112958  8649 net.cpp:84] Creating Layer Convolution11
I1128 13:09:50.112965  8649 net.cpp:406] Convolution11 <- Eltwise4_ReLU9_0_split_0
I1128 13:09:50.112973  8649 net.cpp:380] Convolution11 -> Convolution11
I1128 13:09:50.114974  8649 net.cpp:122] Setting up Convolution11
I1128 13:09:50.115011  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.115017  8649 net.cpp:137] Memory required for data: 23551480
I1128 13:09:50.115031  8649 layer_factory.hpp:77] Creating layer BatchNorm11
I1128 13:09:50.115043  8649 net.cpp:84] Creating Layer BatchNorm11
I1128 13:09:50.115051  8649 net.cpp:406] BatchNorm11 <- Convolution11
I1128 13:09:50.115063  8649 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I1128 13:09:50.115300  8649 net.cpp:122] Setting up BatchNorm11
I1128 13:09:50.115316  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.115324  8649 net.cpp:137] Memory required for data: 23802360
I1128 13:09:50.115334  8649 layer_factory.hpp:77] Creating layer Scale11
I1128 13:09:50.115344  8649 net.cpp:84] Creating Layer Scale11
I1128 13:09:50.115350  8649 net.cpp:406] Scale11 <- Convolution11
I1128 13:09:50.115357  8649 net.cpp:367] Scale11 -> Convolution11 (in-place)
I1128 13:09:50.115409  8649 layer_factory.hpp:77] Creating layer Scale11
I1128 13:09:50.115522  8649 net.cpp:122] Setting up Scale11
I1128 13:09:50.115532  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.115535  8649 net.cpp:137] Memory required for data: 24053240
I1128 13:09:50.115545  8649 layer_factory.hpp:77] Creating layer ReLU10
I1128 13:09:50.115555  8649 net.cpp:84] Creating Layer ReLU10
I1128 13:09:50.115561  8649 net.cpp:406] ReLU10 <- Convolution11
I1128 13:09:50.115571  8649 net.cpp:367] ReLU10 -> Convolution11 (in-place)
I1128 13:09:50.116127  8649 net.cpp:122] Setting up ReLU10
I1128 13:09:50.116147  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.116166  8649 net.cpp:137] Memory required for data: 24304120
I1128 13:09:50.116174  8649 layer_factory.hpp:77] Creating layer Convolution12
I1128 13:09:50.116194  8649 net.cpp:84] Creating Layer Convolution12
I1128 13:09:50.116201  8649 net.cpp:406] Convolution12 <- Convolution11
I1128 13:09:50.116212  8649 net.cpp:380] Convolution12 -> Convolution12
I1128 13:09:50.117494  8649 net.cpp:122] Setting up Convolution12
I1128 13:09:50.117512  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.117517  8649 net.cpp:137] Memory required for data: 24555000
I1128 13:09:50.117527  8649 layer_factory.hpp:77] Creating layer BatchNorm12
I1128 13:09:50.117538  8649 net.cpp:84] Creating Layer BatchNorm12
I1128 13:09:50.117544  8649 net.cpp:406] BatchNorm12 <- Convolution12
I1128 13:09:50.117552  8649 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I1128 13:09:50.117760  8649 net.cpp:122] Setting up BatchNorm12
I1128 13:09:50.117774  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.117779  8649 net.cpp:137] Memory required for data: 24805880
I1128 13:09:50.117789  8649 layer_factory.hpp:77] Creating layer Scale12
I1128 13:09:50.117799  8649 net.cpp:84] Creating Layer Scale12
I1128 13:09:50.117805  8649 net.cpp:406] Scale12 <- Convolution12
I1128 13:09:50.117813  8649 net.cpp:367] Scale12 -> Convolution12 (in-place)
I1128 13:09:50.117871  8649 layer_factory.hpp:77] Creating layer Scale12
I1128 13:09:50.118000  8649 net.cpp:122] Setting up Scale12
I1128 13:09:50.118013  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.118017  8649 net.cpp:137] Memory required for data: 25056760
I1128 13:09:50.118026  8649 layer_factory.hpp:77] Creating layer Eltwise5
I1128 13:09:50.118036  8649 net.cpp:84] Creating Layer Eltwise5
I1128 13:09:50.118041  8649 net.cpp:406] Eltwise5 <- Convolution12
I1128 13:09:50.118047  8649 net.cpp:406] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I1128 13:09:50.118054  8649 net.cpp:380] Eltwise5 -> Eltwise5
I1128 13:09:50.118079  8649 net.cpp:122] Setting up Eltwise5
I1128 13:09:50.118085  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.118088  8649 net.cpp:137] Memory required for data: 25307640
I1128 13:09:50.118091  8649 layer_factory.hpp:77] Creating layer ReLU11
I1128 13:09:50.118098  8649 net.cpp:84] Creating Layer ReLU11
I1128 13:09:50.118101  8649 net.cpp:406] ReLU11 <- Eltwise5
I1128 13:09:50.118106  8649 net.cpp:367] ReLU11 -> Eltwise5 (in-place)
I1128 13:09:50.118336  8649 net.cpp:122] Setting up ReLU11
I1128 13:09:50.118346  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.118350  8649 net.cpp:137] Memory required for data: 25558520
I1128 13:09:50.118355  8649 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I1128 13:09:50.118362  8649 net.cpp:84] Creating Layer Eltwise5_ReLU11_0_split
I1128 13:09:50.118367  8649 net.cpp:406] Eltwise5_ReLU11_0_split <- Eltwise5
I1128 13:09:50.118379  8649 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I1128 13:09:50.118396  8649 net.cpp:380] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I1128 13:09:50.118448  8649 net.cpp:122] Setting up Eltwise5_ReLU11_0_split
I1128 13:09:50.118456  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.118461  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.118468  8649 net.cpp:137] Memory required for data: 26060280
I1128 13:09:50.118474  8649 layer_factory.hpp:77] Creating layer Convolution13
I1128 13:09:50.118490  8649 net.cpp:84] Creating Layer Convolution13
I1128 13:09:50.118499  8649 net.cpp:406] Convolution13 <- Eltwise5_ReLU11_0_split_0
I1128 13:09:50.118507  8649 net.cpp:380] Convolution13 -> Convolution13
I1128 13:09:50.119715  8649 net.cpp:122] Setting up Convolution13
I1128 13:09:50.119729  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.119735  8649 net.cpp:137] Memory required for data: 26311160
I1128 13:09:50.119743  8649 layer_factory.hpp:77] Creating layer BatchNorm13
I1128 13:09:50.119753  8649 net.cpp:84] Creating Layer BatchNorm13
I1128 13:09:50.119772  8649 net.cpp:406] BatchNorm13 <- Convolution13
I1128 13:09:50.119784  8649 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I1128 13:09:50.119989  8649 net.cpp:122] Setting up BatchNorm13
I1128 13:09:50.119998  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.120003  8649 net.cpp:137] Memory required for data: 26562040
I1128 13:09:50.120013  8649 layer_factory.hpp:77] Creating layer Scale13
I1128 13:09:50.120023  8649 net.cpp:84] Creating Layer Scale13
I1128 13:09:50.120031  8649 net.cpp:406] Scale13 <- Convolution13
I1128 13:09:50.120041  8649 net.cpp:367] Scale13 -> Convolution13 (in-place)
I1128 13:09:50.120090  8649 layer_factory.hpp:77] Creating layer Scale13
I1128 13:09:50.120201  8649 net.cpp:122] Setting up Scale13
I1128 13:09:50.120211  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.120216  8649 net.cpp:137] Memory required for data: 26812920
I1128 13:09:50.120343  8649 layer_factory.hpp:77] Creating layer ReLU12
I1128 13:09:50.120368  8649 net.cpp:84] Creating Layer ReLU12
I1128 13:09:50.120386  8649 net.cpp:406] ReLU12 <- Convolution13
I1128 13:09:50.120404  8649 net.cpp:367] ReLU12 -> Convolution13 (in-place)
I1128 13:09:50.120820  8649 net.cpp:122] Setting up ReLU12
I1128 13:09:50.120831  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.120836  8649 net.cpp:137] Memory required for data: 27063800
I1128 13:09:50.120842  8649 layer_factory.hpp:77] Creating layer Convolution14
I1128 13:09:50.120868  8649 net.cpp:84] Creating Layer Convolution14
I1128 13:09:50.120885  8649 net.cpp:406] Convolution14 <- Convolution13
I1128 13:09:50.120908  8649 net.cpp:380] Convolution14 -> Convolution14
I1128 13:09:50.122295  8649 net.cpp:122] Setting up Convolution14
I1128 13:09:50.122328  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.122344  8649 net.cpp:137] Memory required for data: 27314680
I1128 13:09:50.122375  8649 layer_factory.hpp:77] Creating layer BatchNorm14
I1128 13:09:50.122396  8649 net.cpp:84] Creating Layer BatchNorm14
I1128 13:09:50.122413  8649 net.cpp:406] BatchNorm14 <- Convolution14
I1128 13:09:50.122431  8649 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I1128 13:09:50.122694  8649 net.cpp:122] Setting up BatchNorm14
I1128 13:09:50.122721  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.122735  8649 net.cpp:137] Memory required for data: 27565560
I1128 13:09:50.122755  8649 layer_factory.hpp:77] Creating layer Scale14
I1128 13:09:50.122776  8649 net.cpp:84] Creating Layer Scale14
I1128 13:09:50.122792  8649 net.cpp:406] Scale14 <- Convolution14
I1128 13:09:50.122810  8649 net.cpp:367] Scale14 -> Convolution14 (in-place)
I1128 13:09:50.122869  8649 layer_factory.hpp:77] Creating layer Scale14
I1128 13:09:50.122977  8649 net.cpp:122] Setting up Scale14
I1128 13:09:50.122987  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.122992  8649 net.cpp:137] Memory required for data: 27816440
I1128 13:09:50.123000  8649 layer_factory.hpp:77] Creating layer Eltwise6
I1128 13:09:50.123023  8649 net.cpp:84] Creating Layer Eltwise6
I1128 13:09:50.123029  8649 net.cpp:406] Eltwise6 <- Convolution14
I1128 13:09:50.123037  8649 net.cpp:406] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I1128 13:09:50.123055  8649 net.cpp:380] Eltwise6 -> Eltwise6
I1128 13:09:50.123097  8649 net.cpp:122] Setting up Eltwise6
I1128 13:09:50.123107  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.123112  8649 net.cpp:137] Memory required for data: 28067320
I1128 13:09:50.123117  8649 layer_factory.hpp:77] Creating layer ReLU13
I1128 13:09:50.123136  8649 net.cpp:84] Creating Layer ReLU13
I1128 13:09:50.123150  8649 net.cpp:406] ReLU13 <- Eltwise6
I1128 13:09:50.123168  8649 net.cpp:367] ReLU13 -> Eltwise6 (in-place)
I1128 13:09:50.123628  8649 net.cpp:122] Setting up ReLU13
I1128 13:09:50.123641  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.123646  8649 net.cpp:137] Memory required for data: 28318200
I1128 13:09:50.123651  8649 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I1128 13:09:50.123682  8649 net.cpp:84] Creating Layer Eltwise6_ReLU13_0_split
I1128 13:09:50.123697  8649 net.cpp:406] Eltwise6_ReLU13_0_split <- Eltwise6
I1128 13:09:50.123716  8649 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I1128 13:09:50.123728  8649 net.cpp:380] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I1128 13:09:50.123780  8649 net.cpp:122] Setting up Eltwise6_ReLU13_0_split
I1128 13:09:50.123790  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.123796  8649 net.cpp:129] Top shape: 10 32 14 14 (62720)
I1128 13:09:50.123811  8649 net.cpp:137] Memory required for data: 28819960
I1128 13:09:50.123826  8649 layer_factory.hpp:77] Creating layer Convolution15
I1128 13:09:50.123848  8649 net.cpp:84] Creating Layer Convolution15
I1128 13:09:50.123857  8649 net.cpp:406] Convolution15 <- Eltwise6_ReLU13_0_split_0
I1128 13:09:50.123865  8649 net.cpp:380] Convolution15 -> Convolution15
I1128 13:09:50.125483  8649 net.cpp:122] Setting up Convolution15
I1128 13:09:50.125540  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.125555  8649 net.cpp:137] Memory required for data: 28945400
I1128 13:09:50.125578  8649 layer_factory.hpp:77] Creating layer BatchNorm15
I1128 13:09:50.125604  8649 net.cpp:84] Creating Layer BatchNorm15
I1128 13:09:50.125620  8649 net.cpp:406] BatchNorm15 <- Convolution15
I1128 13:09:50.125641  8649 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I1128 13:09:50.125860  8649 net.cpp:122] Setting up BatchNorm15
I1128 13:09:50.125872  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.125877  8649 net.cpp:137] Memory required for data: 29070840
I1128 13:09:50.125888  8649 layer_factory.hpp:77] Creating layer Scale15
I1128 13:09:50.125910  8649 net.cpp:84] Creating Layer Scale15
I1128 13:09:50.125926  8649 net.cpp:406] Scale15 <- Convolution15
I1128 13:09:50.125944  8649 net.cpp:367] Scale15 -> Convolution15 (in-place)
I1128 13:09:50.126000  8649 layer_factory.hpp:77] Creating layer Scale15
I1128 13:09:50.126104  8649 net.cpp:122] Setting up Scale15
I1128 13:09:50.126113  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.126117  8649 net.cpp:137] Memory required for data: 29196280
I1128 13:09:50.126127  8649 layer_factory.hpp:77] Creating layer ReLU14
I1128 13:09:50.126145  8649 net.cpp:84] Creating Layer ReLU14
I1128 13:09:50.126160  8649 net.cpp:406] ReLU14 <- Convolution15
I1128 13:09:50.126178  8649 net.cpp:367] ReLU14 -> Convolution15 (in-place)
I1128 13:09:50.126418  8649 net.cpp:122] Setting up ReLU14
I1128 13:09:50.126430  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.126435  8649 net.cpp:137] Memory required for data: 29321720
I1128 13:09:50.126441  8649 layer_factory.hpp:77] Creating layer Convolution16
I1128 13:09:50.126466  8649 net.cpp:84] Creating Layer Convolution16
I1128 13:09:50.126473  8649 net.cpp:406] Convolution16 <- Convolution15
I1128 13:09:50.126483  8649 net.cpp:380] Convolution16 -> Convolution16
I1128 13:09:50.129052  8649 net.cpp:122] Setting up Convolution16
I1128 13:09:50.129117  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.129132  8649 net.cpp:137] Memory required for data: 29447160
I1128 13:09:50.129154  8649 layer_factory.hpp:77] Creating layer BatchNorm16
I1128 13:09:50.129179  8649 net.cpp:84] Creating Layer BatchNorm16
I1128 13:09:50.129195  8649 net.cpp:406] BatchNorm16 <- Convolution16
I1128 13:09:50.129214  8649 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I1128 13:09:50.129519  8649 net.cpp:122] Setting up BatchNorm16
I1128 13:09:50.129547  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.129562  8649 net.cpp:137] Memory required for data: 29572600
I1128 13:09:50.129583  8649 layer_factory.hpp:77] Creating layer Scale16
I1128 13:09:50.129603  8649 net.cpp:84] Creating Layer Scale16
I1128 13:09:50.129618  8649 net.cpp:406] Scale16 <- Convolution16
I1128 13:09:50.129637  8649 net.cpp:367] Scale16 -> Convolution16 (in-place)
I1128 13:09:50.129721  8649 layer_factory.hpp:77] Creating layer Scale16
I1128 13:09:50.129909  8649 net.cpp:122] Setting up Scale16
I1128 13:09:50.129935  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.129951  8649 net.cpp:137] Memory required for data: 29698040
I1128 13:09:50.129969  8649 layer_factory.hpp:77] Creating layer Pooling2
I1128 13:09:50.129988  8649 net.cpp:84] Creating Layer Pooling2
I1128 13:09:50.130004  8649 net.cpp:406] Pooling2 <- Eltwise6_ReLU13_0_split_1
I1128 13:09:50.130020  8649 net.cpp:380] Pooling2 -> Pooling2
I1128 13:09:50.130077  8649 net.cpp:122] Setting up Pooling2
I1128 13:09:50.130095  8649 net.cpp:129] Top shape: 10 32 7 7 (15680)
I1128 13:09:50.130106  8649 net.cpp:137] Memory required for data: 29760760
I1128 13:09:50.130118  8649 layer_factory.hpp:77] Creating layer Convolution17
I1128 13:09:50.130138  8649 net.cpp:84] Creating Layer Convolution17
I1128 13:09:50.130151  8649 net.cpp:406] Convolution17 <- Pooling2
I1128 13:09:50.130167  8649 net.cpp:380] Convolution17 -> Convolution17
I1128 13:09:50.131765  8649 net.cpp:122] Setting up Convolution17
I1128 13:09:50.131788  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.131793  8649 net.cpp:137] Memory required for data: 29886200
I1128 13:09:50.131803  8649 layer_factory.hpp:77] Creating layer BatchNorm17
I1128 13:09:50.131815  8649 net.cpp:84] Creating Layer BatchNorm17
I1128 13:09:50.131822  8649 net.cpp:406] BatchNorm17 <- Convolution17
I1128 13:09:50.131832  8649 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I1128 13:09:50.132038  8649 net.cpp:122] Setting up BatchNorm17
I1128 13:09:50.132048  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.132053  8649 net.cpp:137] Memory required for data: 30011640
I1128 13:09:50.132064  8649 layer_factory.hpp:77] Creating layer Scale17
I1128 13:09:50.132073  8649 net.cpp:84] Creating Layer Scale17
I1128 13:09:50.132081  8649 net.cpp:406] Scale17 <- Convolution17
I1128 13:09:50.132087  8649 net.cpp:367] Scale17 -> Convolution17 (in-place)
I1128 13:09:50.132135  8649 layer_factory.hpp:77] Creating layer Scale17
I1128 13:09:50.132270  8649 net.cpp:122] Setting up Scale17
I1128 13:09:50.132297  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.132311  8649 net.cpp:137] Memory required for data: 30137080
I1128 13:09:50.132328  8649 layer_factory.hpp:77] Creating layer Eltwise7
I1128 13:09:50.132344  8649 net.cpp:84] Creating Layer Eltwise7
I1128 13:09:50.132357  8649 net.cpp:406] Eltwise7 <- Convolution16
I1128 13:09:50.132371  8649 net.cpp:406] Eltwise7 <- Convolution17
I1128 13:09:50.132388  8649 net.cpp:380] Eltwise7 -> Eltwise7
I1128 13:09:50.132426  8649 net.cpp:122] Setting up Eltwise7
I1128 13:09:50.132447  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.132458  8649 net.cpp:137] Memory required for data: 30262520
I1128 13:09:50.132472  8649 layer_factory.hpp:77] Creating layer ReLU15
I1128 13:09:50.132488  8649 net.cpp:84] Creating Layer ReLU15
I1128 13:09:50.132500  8649 net.cpp:406] ReLU15 <- Eltwise7
I1128 13:09:50.132514  8649 net.cpp:367] ReLU15 -> Eltwise7 (in-place)
I1128 13:09:50.132984  8649 net.cpp:122] Setting up ReLU15
I1128 13:09:50.133018  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.133033  8649 net.cpp:137] Memory required for data: 30387960
I1128 13:09:50.133046  8649 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I1128 13:09:50.133064  8649 net.cpp:84] Creating Layer Eltwise7_ReLU15_0_split
I1128 13:09:50.133078  8649 net.cpp:406] Eltwise7_ReLU15_0_split <- Eltwise7
I1128 13:09:50.133095  8649 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I1128 13:09:50.133114  8649 net.cpp:380] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I1128 13:09:50.133173  8649 net.cpp:122] Setting up Eltwise7_ReLU15_0_split
I1128 13:09:50.133191  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.133206  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.133219  8649 net.cpp:137] Memory required for data: 30638840
I1128 13:09:50.133231  8649 layer_factory.hpp:77] Creating layer Convolution18
I1128 13:09:50.133260  8649 net.cpp:84] Creating Layer Convolution18
I1128 13:09:50.133286  8649 net.cpp:406] Convolution18 <- Eltwise7_ReLU15_0_split_0
I1128 13:09:50.133303  8649 net.cpp:380] Convolution18 -> Convolution18
I1128 13:09:50.135303  8649 net.cpp:122] Setting up Convolution18
I1128 13:09:50.135365  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.135383  8649 net.cpp:137] Memory required for data: 30764280
I1128 13:09:50.135406  8649 layer_factory.hpp:77] Creating layer BatchNorm18
I1128 13:09:50.135426  8649 net.cpp:84] Creating Layer BatchNorm18
I1128 13:09:50.135444  8649 net.cpp:406] BatchNorm18 <- Convolution18
I1128 13:09:50.135465  8649 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I1128 13:09:50.135710  8649 net.cpp:122] Setting up BatchNorm18
I1128 13:09:50.135736  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.135752  8649 net.cpp:137] Memory required for data: 30889720
I1128 13:09:50.135771  8649 layer_factory.hpp:77] Creating layer Scale18
I1128 13:09:50.135789  8649 net.cpp:84] Creating Layer Scale18
I1128 13:09:50.135803  8649 net.cpp:406] Scale18 <- Convolution18
I1128 13:09:50.135820  8649 net.cpp:367] Scale18 -> Convolution18 (in-place)
I1128 13:09:50.135886  8649 layer_factory.hpp:77] Creating layer Scale18
I1128 13:09:50.136037  8649 net.cpp:122] Setting up Scale18
I1128 13:09:50.136062  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.136076  8649 net.cpp:137] Memory required for data: 31015160
I1128 13:09:50.136092  8649 layer_factory.hpp:77] Creating layer ReLU16
I1128 13:09:50.136109  8649 net.cpp:84] Creating Layer ReLU16
I1128 13:09:50.136126  8649 net.cpp:406] ReLU16 <- Convolution18
I1128 13:09:50.136142  8649 net.cpp:367] ReLU16 -> Convolution18 (in-place)
I1128 13:09:50.136469  8649 net.cpp:122] Setting up ReLU16
I1128 13:09:50.136500  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.136515  8649 net.cpp:137] Memory required for data: 31140600
I1128 13:09:50.136529  8649 layer_factory.hpp:77] Creating layer Convolution19
I1128 13:09:50.136554  8649 net.cpp:84] Creating Layer Convolution19
I1128 13:09:50.136569  8649 net.cpp:406] Convolution19 <- Convolution18
I1128 13:09:50.136589  8649 net.cpp:380] Convolution19 -> Convolution19
I1128 13:09:50.139034  8649 net.cpp:122] Setting up Convolution19
I1128 13:09:50.139060  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.139065  8649 net.cpp:137] Memory required for data: 31266040
I1128 13:09:50.139076  8649 layer_factory.hpp:77] Creating layer BatchNorm19
I1128 13:09:50.139089  8649 net.cpp:84] Creating Layer BatchNorm19
I1128 13:09:50.139112  8649 net.cpp:406] BatchNorm19 <- Convolution19
I1128 13:09:50.139132  8649 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I1128 13:09:50.139310  8649 net.cpp:122] Setting up BatchNorm19
I1128 13:09:50.139322  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.139327  8649 net.cpp:137] Memory required for data: 31391480
I1128 13:09:50.139353  8649 layer_factory.hpp:77] Creating layer Scale19
I1128 13:09:50.139377  8649 net.cpp:84] Creating Layer Scale19
I1128 13:09:50.139392  8649 net.cpp:406] Scale19 <- Convolution19
I1128 13:09:50.139410  8649 net.cpp:367] Scale19 -> Convolution19 (in-place)
I1128 13:09:50.139469  8649 layer_factory.hpp:77] Creating layer Scale19
I1128 13:09:50.139576  8649 net.cpp:122] Setting up Scale19
I1128 13:09:50.139585  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.139590  8649 net.cpp:137] Memory required for data: 31516920
I1128 13:09:50.139598  8649 layer_factory.hpp:77] Creating layer Eltwise8
I1128 13:09:50.139622  8649 net.cpp:84] Creating Layer Eltwise8
I1128 13:09:50.139631  8649 net.cpp:406] Eltwise8 <- Convolution19
I1128 13:09:50.139638  8649 net.cpp:406] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I1128 13:09:50.139657  8649 net.cpp:380] Eltwise8 -> Eltwise8
I1128 13:09:50.139696  8649 net.cpp:122] Setting up Eltwise8
I1128 13:09:50.139705  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.139710  8649 net.cpp:137] Memory required for data: 31642360
I1128 13:09:50.139715  8649 layer_factory.hpp:77] Creating layer ReLU17
I1128 13:09:50.139741  8649 net.cpp:84] Creating Layer ReLU17
I1128 13:09:50.139757  8649 net.cpp:406] ReLU17 <- Eltwise8
I1128 13:09:50.139776  8649 net.cpp:367] ReLU17 -> Eltwise8 (in-place)
I1128 13:09:50.140158  8649 net.cpp:122] Setting up ReLU17
I1128 13:09:50.140170  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.140175  8649 net.cpp:137] Memory required for data: 31767800
I1128 13:09:50.140182  8649 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I1128 13:09:50.140202  8649 net.cpp:84] Creating Layer Eltwise8_ReLU17_0_split
I1128 13:09:50.140216  8649 net.cpp:406] Eltwise8_ReLU17_0_split <- Eltwise8
I1128 13:09:50.140246  8649 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I1128 13:09:50.140266  8649 net.cpp:380] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I1128 13:09:50.140323  8649 net.cpp:122] Setting up Eltwise8_ReLU17_0_split
I1128 13:09:50.140336  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.140342  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.140357  8649 net.cpp:137] Memory required for data: 32018680
I1128 13:09:50.140372  8649 layer_factory.hpp:77] Creating layer Convolution20
I1128 13:09:50.140394  8649 net.cpp:84] Creating Layer Convolution20
I1128 13:09:50.140401  8649 net.cpp:406] Convolution20 <- Eltwise8_ReLU17_0_split_0
I1128 13:09:50.140413  8649 net.cpp:380] Convolution20 -> Convolution20
I1128 13:09:50.141693  8649 net.cpp:122] Setting up Convolution20
I1128 13:09:50.141707  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.141712  8649 net.cpp:137] Memory required for data: 32144120
I1128 13:09:50.141721  8649 layer_factory.hpp:77] Creating layer BatchNorm20
I1128 13:09:50.141746  8649 net.cpp:84] Creating Layer BatchNorm20
I1128 13:09:50.141762  8649 net.cpp:406] BatchNorm20 <- Convolution20
I1128 13:09:50.141779  8649 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I1128 13:09:50.141952  8649 net.cpp:122] Setting up BatchNorm20
I1128 13:09:50.141963  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.141968  8649 net.cpp:137] Memory required for data: 32269560
I1128 13:09:50.141976  8649 layer_factory.hpp:77] Creating layer Scale20
I1128 13:09:50.141996  8649 net.cpp:84] Creating Layer Scale20
I1128 13:09:50.142011  8649 net.cpp:406] Scale20 <- Convolution20
I1128 13:09:50.142027  8649 net.cpp:367] Scale20 -> Convolution20 (in-place)
I1128 13:09:50.142083  8649 layer_factory.hpp:77] Creating layer Scale20
I1128 13:09:50.142186  8649 net.cpp:122] Setting up Scale20
I1128 13:09:50.142195  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.142200  8649 net.cpp:137] Memory required for data: 32395000
I1128 13:09:50.142208  8649 layer_factory.hpp:77] Creating layer ReLU18
I1128 13:09:50.142230  8649 net.cpp:84] Creating Layer ReLU18
I1128 13:09:50.142236  8649 net.cpp:406] ReLU18 <- Convolution20
I1128 13:09:50.142243  8649 net.cpp:367] ReLU18 -> Convolution20 (in-place)
I1128 13:09:50.142585  8649 net.cpp:122] Setting up ReLU18
I1128 13:09:50.142598  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.142603  8649 net.cpp:137] Memory required for data: 32520440
I1128 13:09:50.142608  8649 layer_factory.hpp:77] Creating layer Convolution21
I1128 13:09:50.142633  8649 net.cpp:84] Creating Layer Convolution21
I1128 13:09:50.142642  8649 net.cpp:406] Convolution21 <- Convolution20
I1128 13:09:50.142650  8649 net.cpp:380] Convolution21 -> Convolution21
I1128 13:09:50.144356  8649 net.cpp:122] Setting up Convolution21
I1128 13:09:50.144388  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.144393  8649 net.cpp:137] Memory required for data: 32645880
I1128 13:09:50.144407  8649 layer_factory.hpp:77] Creating layer BatchNorm21
I1128 13:09:50.144420  8649 net.cpp:84] Creating Layer BatchNorm21
I1128 13:09:50.144445  8649 net.cpp:406] BatchNorm21 <- Convolution21
I1128 13:09:50.144469  8649 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I1128 13:09:50.144732  8649 net.cpp:122] Setting up BatchNorm21
I1128 13:09:50.144762  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.144768  8649 net.cpp:137] Memory required for data: 32771320
I1128 13:09:50.144779  8649 layer_factory.hpp:77] Creating layer Scale21
I1128 13:09:50.144803  8649 net.cpp:84] Creating Layer Scale21
I1128 13:09:50.144819  8649 net.cpp:406] Scale21 <- Convolution21
I1128 13:09:50.144836  8649 net.cpp:367] Scale21 -> Convolution21 (in-place)
I1128 13:09:50.144910  8649 layer_factory.hpp:77] Creating layer Scale21
I1128 13:09:50.145018  8649 net.cpp:122] Setting up Scale21
I1128 13:09:50.145032  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.145050  8649 net.cpp:137] Memory required for data: 32896760
I1128 13:09:50.145067  8649 layer_factory.hpp:77] Creating layer Eltwise9
I1128 13:09:50.145087  8649 net.cpp:84] Creating Layer Eltwise9
I1128 13:09:50.145102  8649 net.cpp:406] Eltwise9 <- Convolution21
I1128 13:09:50.145119  8649 net.cpp:406] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I1128 13:09:50.145131  8649 net.cpp:380] Eltwise9 -> Eltwise9
I1128 13:09:50.145170  8649 net.cpp:122] Setting up Eltwise9
I1128 13:09:50.145179  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.145184  8649 net.cpp:137] Memory required for data: 33022200
I1128 13:09:50.145200  8649 layer_factory.hpp:77] Creating layer ReLU19
I1128 13:09:50.145218  8649 net.cpp:84] Creating Layer ReLU19
I1128 13:09:50.145233  8649 net.cpp:406] ReLU19 <- Eltwise9
I1128 13:09:50.145249  8649 net.cpp:367] ReLU19 -> Eltwise9 (in-place)
I1128 13:09:50.145514  8649 net.cpp:122] Setting up ReLU19
I1128 13:09:50.145525  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.145530  8649 net.cpp:137] Memory required for data: 33147640
I1128 13:09:50.145535  8649 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
I1128 13:09:50.145555  8649 net.cpp:84] Creating Layer Eltwise9_ReLU19_0_split
I1128 13:09:50.145570  8649 net.cpp:406] Eltwise9_ReLU19_0_split <- Eltwise9
I1128 13:09:50.145589  8649 net.cpp:380] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
I1128 13:09:50.145609  8649 net.cpp:380] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
I1128 13:09:50.145660  8649 net.cpp:122] Setting up Eltwise9_ReLU19_0_split
I1128 13:09:50.145669  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.145676  8649 net.cpp:129] Top shape: 10 64 7 7 (31360)
I1128 13:09:50.145691  8649 net.cpp:137] Memory required for data: 33398520
I1128 13:09:50.145705  8649 layer_factory.hpp:77] Creating layer Convolution22
I1128 13:09:50.145730  8649 net.cpp:84] Creating Layer Convolution22
I1128 13:09:50.145736  8649 net.cpp:406] Convolution22 <- Eltwise9_ReLU19_0_split_0
I1128 13:09:50.145745  8649 net.cpp:380] Convolution22 -> Convolution22
I1128 13:09:50.147301  8649 net.cpp:122] Setting up Convolution22
I1128 13:09:50.147338  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.147356  8649 net.cpp:137] Memory required for data: 33439480
I1128 13:09:50.147375  8649 layer_factory.hpp:77] Creating layer BatchNorm22
I1128 13:09:50.147397  8649 net.cpp:84] Creating Layer BatchNorm22
I1128 13:09:50.147414  8649 net.cpp:406] BatchNorm22 <- Convolution22
I1128 13:09:50.147433  8649 net.cpp:367] BatchNorm22 -> Convolution22 (in-place)
I1128 13:09:50.147653  8649 net.cpp:122] Setting up BatchNorm22
I1128 13:09:50.147676  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.147692  8649 net.cpp:137] Memory required for data: 33480440
I1128 13:09:50.147712  8649 layer_factory.hpp:77] Creating layer Scale22
I1128 13:09:50.147733  8649 net.cpp:84] Creating Layer Scale22
I1128 13:09:50.147749  8649 net.cpp:406] Scale22 <- Convolution22
I1128 13:09:50.147768  8649 net.cpp:367] Scale22 -> Convolution22 (in-place)
I1128 13:09:50.147840  8649 layer_factory.hpp:77] Creating layer Scale22
I1128 13:09:50.147945  8649 net.cpp:122] Setting up Scale22
I1128 13:09:50.147955  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.147960  8649 net.cpp:137] Memory required for data: 33521400
I1128 13:09:50.147985  8649 layer_factory.hpp:77] Creating layer ReLU20
I1128 13:09:50.148006  8649 net.cpp:84] Creating Layer ReLU20
I1128 13:09:50.148012  8649 net.cpp:406] ReLU20 <- Convolution22
I1128 13:09:50.148030  8649 net.cpp:367] ReLU20 -> Convolution22 (in-place)
I1128 13:09:50.148440  8649 net.cpp:122] Setting up ReLU20
I1128 13:09:50.148453  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.148458  8649 net.cpp:137] Memory required for data: 33562360
I1128 13:09:50.148464  8649 layer_factory.hpp:77] Creating layer Convolution23
I1128 13:09:50.148490  8649 net.cpp:84] Creating Layer Convolution23
I1128 13:09:50.148507  8649 net.cpp:406] Convolution23 <- Convolution22
I1128 13:09:50.148526  8649 net.cpp:380] Convolution23 -> Convolution23
I1128 13:09:50.150524  8649 net.cpp:122] Setting up Convolution23
I1128 13:09:50.150568  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.150573  8649 net.cpp:137] Memory required for data: 33603320
I1128 13:09:50.150591  8649 layer_factory.hpp:77] Creating layer BatchNorm23
I1128 13:09:50.150610  8649 net.cpp:84] Creating Layer BatchNorm23
I1128 13:09:50.150617  8649 net.cpp:406] BatchNorm23 <- Convolution23
I1128 13:09:50.150635  8649 net.cpp:367] BatchNorm23 -> Convolution23 (in-place)
I1128 13:09:50.150904  8649 net.cpp:122] Setting up BatchNorm23
I1128 13:09:50.150918  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.150923  8649 net.cpp:137] Memory required for data: 33644280
I1128 13:09:50.150935  8649 layer_factory.hpp:77] Creating layer Scale23
I1128 13:09:50.150959  8649 net.cpp:84] Creating Layer Scale23
I1128 13:09:50.150974  8649 net.cpp:406] Scale23 <- Convolution23
I1128 13:09:50.150993  8649 net.cpp:367] Scale23 -> Convolution23 (in-place)
I1128 13:09:50.151074  8649 layer_factory.hpp:77] Creating layer Scale23
I1128 13:09:50.151211  8649 net.cpp:122] Setting up Scale23
I1128 13:09:50.151222  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.151227  8649 net.cpp:137] Memory required for data: 33685240
I1128 13:09:50.151238  8649 layer_factory.hpp:77] Creating layer Pooling3
I1128 13:09:50.151268  8649 net.cpp:84] Creating Layer Pooling3
I1128 13:09:50.151286  8649 net.cpp:406] Pooling3 <- Eltwise9_ReLU19_0_split_1
I1128 13:09:50.151309  8649 net.cpp:380] Pooling3 -> Pooling3
I1128 13:09:50.151379  8649 net.cpp:122] Setting up Pooling3
I1128 13:09:50.151391  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.151396  8649 net.cpp:137] Memory required for data: 33726200
I1128 13:09:50.151401  8649 layer_factory.hpp:77] Creating layer Convolution24
I1128 13:09:50.151430  8649 net.cpp:84] Creating Layer Convolution24
I1128 13:09:50.151448  8649 net.cpp:406] Convolution24 <- Pooling3
I1128 13:09:50.151465  8649 net.cpp:380] Convolution24 -> Convolution24
I1128 13:09:50.153554  8649 net.cpp:122] Setting up Convolution24
I1128 13:09:50.153589  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.153594  8649 net.cpp:137] Memory required for data: 33767160
I1128 13:09:50.153607  8649 layer_factory.hpp:77] Creating layer BatchNorm24
I1128 13:09:50.153621  8649 net.cpp:84] Creating Layer BatchNorm24
I1128 13:09:50.153648  8649 net.cpp:406] BatchNorm24 <- Convolution24
I1128 13:09:50.153671  8649 net.cpp:367] BatchNorm24 -> Convolution24 (in-place)
I1128 13:09:50.153888  8649 net.cpp:122] Setting up BatchNorm24
I1128 13:09:50.153903  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.153908  8649 net.cpp:137] Memory required for data: 33808120
I1128 13:09:50.153921  8649 layer_factory.hpp:77] Creating layer Scale24
I1128 13:09:50.153956  8649 net.cpp:84] Creating Layer Scale24
I1128 13:09:50.153975  8649 net.cpp:406] Scale24 <- Convolution24
I1128 13:09:50.153995  8649 net.cpp:367] Scale24 -> Convolution24 (in-place)
I1128 13:09:50.154100  8649 layer_factory.hpp:77] Creating layer Scale24
I1128 13:09:50.154242  8649 net.cpp:122] Setting up Scale24
I1128 13:09:50.154255  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.154260  8649 net.cpp:137] Memory required for data: 33849080
I1128 13:09:50.154271  8649 layer_factory.hpp:77] Creating layer Eltwise10
I1128 13:09:50.154327  8649 net.cpp:84] Creating Layer Eltwise10
I1128 13:09:50.154346  8649 net.cpp:406] Eltwise10 <- Convolution23
I1128 13:09:50.154364  8649 net.cpp:406] Eltwise10 <- Convolution24
I1128 13:09:50.154383  8649 net.cpp:380] Eltwise10 -> Eltwise10
I1128 13:09:50.154446  8649 net.cpp:122] Setting up Eltwise10
I1128 13:09:50.154456  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.154461  8649 net.cpp:137] Memory required for data: 33890040
I1128 13:09:50.154466  8649 layer_factory.hpp:77] Creating layer ReLU21
I1128 13:09:50.154489  8649 net.cpp:84] Creating Layer ReLU21
I1128 13:09:50.154505  8649 net.cpp:406] ReLU21 <- Eltwise10
I1128 13:09:50.154532  8649 net.cpp:367] ReLU21 -> Eltwise10 (in-place)
I1128 13:09:50.154945  8649 net.cpp:122] Setting up ReLU21
I1128 13:09:50.154984  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.154989  8649 net.cpp:137] Memory required for data: 33931000
I1128 13:09:50.154999  8649 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
I1128 13:09:50.155014  8649 net.cpp:84] Creating Layer Eltwise10_ReLU21_0_split
I1128 13:09:50.155021  8649 net.cpp:406] Eltwise10_ReLU21_0_split <- Eltwise10
I1128 13:09:50.155033  8649 net.cpp:380] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
I1128 13:09:50.155079  8649 net.cpp:380] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
I1128 13:09:50.155187  8649 net.cpp:122] Setting up Eltwise10_ReLU21_0_split
I1128 13:09:50.155200  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.155205  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.155210  8649 net.cpp:137] Memory required for data: 34012920
I1128 13:09:50.155227  8649 layer_factory.hpp:77] Creating layer Convolution25
I1128 13:09:50.155256  8649 net.cpp:84] Creating Layer Convolution25
I1128 13:09:50.155272  8649 net.cpp:406] Convolution25 <- Eltwise10_ReLU21_0_split_0
I1128 13:09:50.155295  8649 net.cpp:380] Convolution25 -> Convolution25
I1128 13:09:50.157313  8649 net.cpp:122] Setting up Convolution25
I1128 13:09:50.157338  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.157344  8649 net.cpp:137] Memory required for data: 34053880
I1128 13:09:50.157356  8649 layer_factory.hpp:77] Creating layer BatchNorm25
I1128 13:09:50.157369  8649 net.cpp:84] Creating Layer BatchNorm25
I1128 13:09:50.157393  8649 net.cpp:406] BatchNorm25 <- Convolution25
I1128 13:09:50.157413  8649 net.cpp:367] BatchNorm25 -> Convolution25 (in-place)
I1128 13:09:50.157606  8649 net.cpp:122] Setting up BatchNorm25
I1128 13:09:50.157616  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.157621  8649 net.cpp:137] Memory required for data: 34094840
I1128 13:09:50.157631  8649 layer_factory.hpp:77] Creating layer Scale25
I1128 13:09:50.157655  8649 net.cpp:84] Creating Layer Scale25
I1128 13:09:50.157670  8649 net.cpp:406] Scale25 <- Convolution25
I1128 13:09:50.157687  8649 net.cpp:367] Scale25 -> Convolution25 (in-place)
I1128 13:09:50.157748  8649 layer_factory.hpp:77] Creating layer Scale25
I1128 13:09:50.157860  8649 net.cpp:122] Setting up Scale25
I1128 13:09:50.157871  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.157874  8649 net.cpp:137] Memory required for data: 34135800
I1128 13:09:50.157883  8649 layer_factory.hpp:77] Creating layer ReLU22
I1128 13:09:50.157904  8649 net.cpp:84] Creating Layer ReLU22
I1128 13:09:50.157912  8649 net.cpp:406] ReLU22 <- Convolution25
I1128 13:09:50.157919  8649 net.cpp:367] ReLU22 -> Convolution25 (in-place)
I1128 13:09:50.158361  8649 net.cpp:122] Setting up ReLU22
I1128 13:09:50.158375  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.158380  8649 net.cpp:137] Memory required for data: 34176760
I1128 13:09:50.158385  8649 layer_factory.hpp:77] Creating layer Convolution26
I1128 13:09:50.158401  8649 net.cpp:84] Creating Layer Convolution26
I1128 13:09:50.158421  8649 net.cpp:406] Convolution26 <- Convolution25
I1128 13:09:50.158442  8649 net.cpp:380] Convolution26 -> Convolution26
I1128 13:09:50.160591  8649 net.cpp:122] Setting up Convolution26
I1128 13:09:50.160641  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.160646  8649 net.cpp:137] Memory required for data: 34217720
I1128 13:09:50.160660  8649 layer_factory.hpp:77] Creating layer BatchNorm26
I1128 13:09:50.160691  8649 net.cpp:84] Creating Layer BatchNorm26
I1128 13:09:50.160712  8649 net.cpp:406] BatchNorm26 <- Convolution26
I1128 13:09:50.160730  8649 net.cpp:367] BatchNorm26 -> Convolution26 (in-place)
I1128 13:09:50.160933  8649 net.cpp:122] Setting up BatchNorm26
I1128 13:09:50.160943  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.160948  8649 net.cpp:137] Memory required for data: 34258680
I1128 13:09:50.160959  8649 layer_factory.hpp:77] Creating layer Scale26
I1128 13:09:50.160979  8649 net.cpp:84] Creating Layer Scale26
I1128 13:09:50.160995  8649 net.cpp:406] Scale26 <- Convolution26
I1128 13:09:50.161011  8649 net.cpp:367] Scale26 -> Convolution26 (in-place)
I1128 13:09:50.161077  8649 layer_factory.hpp:77] Creating layer Scale26
I1128 13:09:50.161201  8649 net.cpp:122] Setting up Scale26
I1128 13:09:50.161212  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.161217  8649 net.cpp:137] Memory required for data: 34299640
I1128 13:09:50.161226  8649 layer_factory.hpp:77] Creating layer Eltwise11
I1128 13:09:50.161247  8649 net.cpp:84] Creating Layer Eltwise11
I1128 13:09:50.161264  8649 net.cpp:406] Eltwise11 <- Convolution26
I1128 13:09:50.161290  8649 net.cpp:406] Eltwise11 <- Eltwise10_ReLU21_0_split_1
I1128 13:09:50.161300  8649 net.cpp:380] Eltwise11 -> Eltwise11
I1128 13:09:50.161337  8649 net.cpp:122] Setting up Eltwise11
I1128 13:09:50.161346  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.161351  8649 net.cpp:137] Memory required for data: 34340600
I1128 13:09:50.161368  8649 layer_factory.hpp:77] Creating layer ReLU23
I1128 13:09:50.161387  8649 net.cpp:84] Creating Layer ReLU23
I1128 13:09:50.161401  8649 net.cpp:406] ReLU23 <- Eltwise11
I1128 13:09:50.161417  8649 net.cpp:367] ReLU23 -> Eltwise11 (in-place)
I1128 13:09:50.162037  8649 net.cpp:122] Setting up ReLU23
I1128 13:09:50.162055  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.162060  8649 net.cpp:137] Memory required for data: 34381560
I1128 13:09:50.162066  8649 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
I1128 13:09:50.162101  8649 net.cpp:84] Creating Layer Eltwise11_ReLU23_0_split
I1128 13:09:50.162123  8649 net.cpp:406] Eltwise11_ReLU23_0_split <- Eltwise11
I1128 13:09:50.162143  8649 net.cpp:380] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
I1128 13:09:50.162164  8649 net.cpp:380] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
I1128 13:09:50.162223  8649 net.cpp:122] Setting up Eltwise11_ReLU23_0_split
I1128 13:09:50.162232  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.162238  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.162243  8649 net.cpp:137] Memory required for data: 34463480
I1128 13:09:50.162259  8649 layer_factory.hpp:77] Creating layer Convolution27
I1128 13:09:50.162284  8649 net.cpp:84] Creating Layer Convolution27
I1128 13:09:50.162292  8649 net.cpp:406] Convolution27 <- Eltwise11_ReLU23_0_split_0
I1128 13:09:50.162302  8649 net.cpp:380] Convolution27 -> Convolution27
I1128 13:09:50.163794  8649 net.cpp:122] Setting up Convolution27
I1128 13:09:50.163820  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.163825  8649 net.cpp:137] Memory required for data: 34504440
I1128 13:09:50.163836  8649 layer_factory.hpp:77] Creating layer BatchNorm27
I1128 13:09:50.163849  8649 net.cpp:84] Creating Layer BatchNorm27
I1128 13:09:50.163856  8649 net.cpp:406] BatchNorm27 <- Convolution27
I1128 13:09:50.163884  8649 net.cpp:367] BatchNorm27 -> Convolution27 (in-place)
I1128 13:09:50.164079  8649 net.cpp:122] Setting up BatchNorm27
I1128 13:09:50.164089  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.164094  8649 net.cpp:137] Memory required for data: 34545400
I1128 13:09:50.164104  8649 layer_factory.hpp:77] Creating layer Scale27
I1128 13:09:50.164134  8649 net.cpp:84] Creating Layer Scale27
I1128 13:09:50.164149  8649 net.cpp:406] Scale27 <- Convolution27
I1128 13:09:50.164157  8649 net.cpp:367] Scale27 -> Convolution27 (in-place)
I1128 13:09:50.164221  8649 layer_factory.hpp:77] Creating layer Scale27
I1128 13:09:50.164356  8649 net.cpp:122] Setting up Scale27
I1128 13:09:50.164366  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.164371  8649 net.cpp:137] Memory required for data: 34586360
I1128 13:09:50.164378  8649 layer_factory.hpp:77] Creating layer ReLU24
I1128 13:09:50.164400  8649 net.cpp:84] Creating Layer ReLU24
I1128 13:09:50.164417  8649 net.cpp:406] ReLU24 <- Convolution27
I1128 13:09:50.164433  8649 net.cpp:367] ReLU24 -> Convolution27 (in-place)
I1128 13:09:50.164671  8649 net.cpp:122] Setting up ReLU24
I1128 13:09:50.164682  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.164687  8649 net.cpp:137] Memory required for data: 34627320
I1128 13:09:50.164692  8649 layer_factory.hpp:77] Creating layer Convolution28
I1128 13:09:50.164706  8649 net.cpp:84] Creating Layer Convolution28
I1128 13:09:50.164723  8649 net.cpp:406] Convolution28 <- Convolution27
I1128 13:09:50.164744  8649 net.cpp:380] Convolution28 -> Convolution28
I1128 13:09:50.166270  8649 net.cpp:122] Setting up Convolution28
I1128 13:09:50.166297  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.166302  8649 net.cpp:137] Memory required for data: 34668280
I1128 13:09:50.166313  8649 layer_factory.hpp:77] Creating layer BatchNorm28
I1128 13:09:50.166326  8649 net.cpp:84] Creating Layer BatchNorm28
I1128 13:09:50.166349  8649 net.cpp:406] BatchNorm28 <- Convolution28
I1128 13:09:50.166370  8649 net.cpp:367] BatchNorm28 -> Convolution28 (in-place)
I1128 13:09:50.166561  8649 net.cpp:122] Setting up BatchNorm28
I1128 13:09:50.166570  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.166575  8649 net.cpp:137] Memory required for data: 34709240
I1128 13:09:50.166585  8649 layer_factory.hpp:77] Creating layer Scale28
I1128 13:09:50.166605  8649 net.cpp:84] Creating Layer Scale28
I1128 13:09:50.166620  8649 net.cpp:406] Scale28 <- Convolution28
I1128 13:09:50.166636  8649 net.cpp:367] Scale28 -> Convolution28 (in-place)
I1128 13:09:50.166694  8649 layer_factory.hpp:77] Creating layer Scale28
I1128 13:09:50.166805  8649 net.cpp:122] Setting up Scale28
I1128 13:09:50.166815  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.166818  8649 net.cpp:137] Memory required for data: 34750200
I1128 13:09:50.166827  8649 layer_factory.hpp:77] Creating layer Eltwise12
I1128 13:09:50.166848  8649 net.cpp:84] Creating Layer Eltwise12
I1128 13:09:50.166863  8649 net.cpp:406] Eltwise12 <- Convolution28
I1128 13:09:50.166882  8649 net.cpp:406] Eltwise12 <- Eltwise11_ReLU23_0_split_1
I1128 13:09:50.166901  8649 net.cpp:380] Eltwise12 -> Eltwise12
I1128 13:09:50.166947  8649 net.cpp:122] Setting up Eltwise12
I1128 13:09:50.166959  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.166963  8649 net.cpp:137] Memory required for data: 34791160
I1128 13:09:50.166967  8649 layer_factory.hpp:77] Creating layer ReLU25
I1128 13:09:50.166987  8649 net.cpp:84] Creating Layer ReLU25
I1128 13:09:50.167002  8649 net.cpp:406] ReLU25 <- Eltwise12
I1128 13:09:50.167018  8649 net.cpp:367] ReLU25 -> Eltwise12 (in-place)
I1128 13:09:50.167497  8649 net.cpp:122] Setting up ReLU25
I1128 13:09:50.167511  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.167516  8649 net.cpp:137] Memory required for data: 34832120
I1128 13:09:50.167521  8649 layer_factory.hpp:77] Creating layer Eltwise12_ReLU25_0_split
I1128 13:09:50.167529  8649 net.cpp:84] Creating Layer Eltwise12_ReLU25_0_split
I1128 13:09:50.167551  8649 net.cpp:406] Eltwise12_ReLU25_0_split <- Eltwise12
I1128 13:09:50.167570  8649 net.cpp:380] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_0
I1128 13:09:50.167584  8649 net.cpp:380] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_1
I1128 13:09:50.167640  8649 net.cpp:122] Setting up Eltwise12_ReLU25_0_split
I1128 13:09:50.167650  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.167667  8649 net.cpp:129] Top shape: 10 64 4 4 (10240)
I1128 13:09:50.167682  8649 net.cpp:137] Memory required for data: 34914040
I1128 13:09:50.167696  8649 layer_factory.hpp:77] Creating layer Convolution29
I1128 13:09:50.167721  8649 net.cpp:84] Creating Layer Convolution29
I1128 13:09:50.167727  8649 net.cpp:406] Convolution29 <- Eltwise12_ReLU25_0_split_0
I1128 13:09:50.167738  8649 net.cpp:380] Convolution29 -> Convolution29
I1128 13:09:50.169656  8649 net.cpp:122] Setting up Convolution29
I1128 13:09:50.169721  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.169736  8649 net.cpp:137] Memory required for data: 34924280
I1128 13:09:50.169757  8649 layer_factory.hpp:77] Creating layer BatchNorm29
I1128 13:09:50.169778  8649 net.cpp:84] Creating Layer BatchNorm29
I1128 13:09:50.169795  8649 net.cpp:406] BatchNorm29 <- Convolution29
I1128 13:09:50.169812  8649 net.cpp:367] BatchNorm29 -> Convolution29 (in-place)
I1128 13:09:50.170048  8649 net.cpp:122] Setting up BatchNorm29
I1128 13:09:50.170070  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.170083  8649 net.cpp:137] Memory required for data: 34934520
I1128 13:09:50.170101  8649 layer_factory.hpp:77] Creating layer Scale29
I1128 13:09:50.170119  8649 net.cpp:84] Creating Layer Scale29
I1128 13:09:50.170132  8649 net.cpp:406] Scale29 <- Convolution29
I1128 13:09:50.170147  8649 net.cpp:367] Scale29 -> Convolution29 (in-place)
I1128 13:09:50.170212  8649 layer_factory.hpp:77] Creating layer Scale29
I1128 13:09:50.170358  8649 net.cpp:122] Setting up Scale29
I1128 13:09:50.170378  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.170392  8649 net.cpp:137] Memory required for data: 34944760
I1128 13:09:50.170408  8649 layer_factory.hpp:77] Creating layer ReLU26
I1128 13:09:50.170424  8649 net.cpp:84] Creating Layer ReLU26
I1128 13:09:50.170439  8649 net.cpp:406] ReLU26 <- Convolution29
I1128 13:09:50.170454  8649 net.cpp:367] ReLU26 -> Convolution29 (in-place)
I1128 13:09:50.170732  8649 net.cpp:122] Setting up ReLU26
I1128 13:09:50.170754  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.170768  8649 net.cpp:137] Memory required for data: 34955000
I1128 13:09:50.170780  8649 layer_factory.hpp:77] Creating layer Convolution30
I1128 13:09:50.170801  8649 net.cpp:84] Creating Layer Convolution30
I1128 13:09:50.170817  8649 net.cpp:406] Convolution30 <- Convolution29
I1128 13:09:50.170835  8649 net.cpp:380] Convolution30 -> Convolution30
I1128 13:09:50.172924  8649 net.cpp:122] Setting up Convolution30
I1128 13:09:50.172971  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.172986  8649 net.cpp:137] Memory required for data: 34965240
I1128 13:09:50.173005  8649 layer_factory.hpp:77] Creating layer BatchNorm30
I1128 13:09:50.173089  8649 net.cpp:84] Creating Layer BatchNorm30
I1128 13:09:50.173107  8649 net.cpp:406] BatchNorm30 <- Convolution30
I1128 13:09:50.173125  8649 net.cpp:367] BatchNorm30 -> Convolution30 (in-place)
I1128 13:09:50.173358  8649 net.cpp:122] Setting up BatchNorm30
I1128 13:09:50.173383  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.173398  8649 net.cpp:137] Memory required for data: 34975480
I1128 13:09:50.173418  8649 layer_factory.hpp:77] Creating layer Scale30
I1128 13:09:50.173439  8649 net.cpp:84] Creating Layer Scale30
I1128 13:09:50.173456  8649 net.cpp:406] Scale30 <- Convolution30
I1128 13:09:50.173472  8649 net.cpp:367] Scale30 -> Convolution30 (in-place)
I1128 13:09:50.173544  8649 layer_factory.hpp:77] Creating layer Scale30
I1128 13:09:50.173696  8649 net.cpp:122] Setting up Scale30
I1128 13:09:50.173717  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.173732  8649 net.cpp:137] Memory required for data: 34985720
I1128 13:09:50.173749  8649 layer_factory.hpp:77] Creating layer Pooling4
I1128 13:09:50.173769  8649 net.cpp:84] Creating Layer Pooling4
I1128 13:09:50.173784  8649 net.cpp:406] Pooling4 <- Eltwise12_ReLU25_0_split_1
I1128 13:09:50.173802  8649 net.cpp:380] Pooling4 -> Pooling4
I1128 13:09:50.173876  8649 net.cpp:122] Setting up Pooling4
I1128 13:09:50.173895  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.173900  8649 net.cpp:137] Memory required for data: 34995960
I1128 13:09:50.173907  8649 layer_factory.hpp:77] Creating layer Convolution31
I1128 13:09:50.173928  8649 net.cpp:84] Creating Layer Convolution31
I1128 13:09:50.173934  8649 net.cpp:406] Convolution31 <- Pooling4
I1128 13:09:50.173943  8649 net.cpp:380] Convolution31 -> Convolution31
I1128 13:09:50.175384  8649 net.cpp:122] Setting up Convolution31
I1128 13:09:50.175437  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.175456  8649 net.cpp:137] Memory required for data: 35006200
I1128 13:09:50.175477  8649 layer_factory.hpp:77] Creating layer BatchNorm31
I1128 13:09:50.175504  8649 net.cpp:84] Creating Layer BatchNorm31
I1128 13:09:50.175514  8649 net.cpp:406] BatchNorm31 <- Convolution31
I1128 13:09:50.175525  8649 net.cpp:367] BatchNorm31 -> Convolution31 (in-place)
I1128 13:09:50.175774  8649 net.cpp:122] Setting up BatchNorm31
I1128 13:09:50.175797  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.175812  8649 net.cpp:137] Memory required for data: 35016440
I1128 13:09:50.175833  8649 layer_factory.hpp:77] Creating layer Scale31
I1128 13:09:50.175854  8649 net.cpp:84] Creating Layer Scale31
I1128 13:09:50.175871  8649 net.cpp:406] Scale31 <- Convolution31
I1128 13:09:50.175889  8649 net.cpp:367] Scale31 -> Convolution31 (in-place)
I1128 13:09:50.175961  8649 layer_factory.hpp:77] Creating layer Scale31
I1128 13:09:50.176107  8649 net.cpp:122] Setting up Scale31
I1128 13:09:50.176117  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.176128  8649 net.cpp:137] Memory required for data: 35026680
I1128 13:09:50.176148  8649 layer_factory.hpp:77] Creating layer Eltwise13
I1128 13:09:50.176174  8649 net.cpp:84] Creating Layer Eltwise13
I1128 13:09:50.176182  8649 net.cpp:406] Eltwise13 <- Convolution30
I1128 13:09:50.176189  8649 net.cpp:406] Eltwise13 <- Convolution31
I1128 13:09:50.176198  8649 net.cpp:380] Eltwise13 -> Eltwise13
I1128 13:09:50.176254  8649 net.cpp:122] Setting up Eltwise13
I1128 13:09:50.176276  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.176293  8649 net.cpp:137] Memory required for data: 35036920
I1128 13:09:50.176306  8649 layer_factory.hpp:77] Creating layer ReLU27
I1128 13:09:50.176321  8649 net.cpp:84] Creating Layer ReLU27
I1128 13:09:50.176331  8649 net.cpp:406] ReLU27 <- Eltwise13
I1128 13:09:50.176347  8649 net.cpp:367] ReLU27 -> Eltwise13 (in-place)
I1128 13:09:50.176940  8649 net.cpp:122] Setting up ReLU27
I1128 13:09:50.176975  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.176990  8649 net.cpp:137] Memory required for data: 35047160
I1128 13:09:50.177002  8649 layer_factory.hpp:77] Creating layer Eltwise13_ReLU27_0_split
I1128 13:09:50.177022  8649 net.cpp:84] Creating Layer Eltwise13_ReLU27_0_split
I1128 13:09:50.177037  8649 net.cpp:406] Eltwise13_ReLU27_0_split <- Eltwise13
I1128 13:09:50.177055  8649 net.cpp:380] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_0
I1128 13:09:50.177074  8649 net.cpp:380] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_1
I1128 13:09:50.177135  8649 net.cpp:122] Setting up Eltwise13_ReLU27_0_split
I1128 13:09:50.177156  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.177171  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.177183  8649 net.cpp:137] Memory required for data: 35067640
I1128 13:09:50.177196  8649 layer_factory.hpp:77] Creating layer Convolution32
I1128 13:09:50.177219  8649 net.cpp:84] Creating Layer Convolution32
I1128 13:09:50.177233  8649 net.cpp:406] Convolution32 <- Eltwise13_ReLU27_0_split_0
I1128 13:09:50.177253  8649 net.cpp:380] Convolution32 -> Convolution32
I1128 13:09:50.179216  8649 net.cpp:122] Setting up Convolution32
I1128 13:09:50.179255  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.179270  8649 net.cpp:137] Memory required for data: 35077880
I1128 13:09:50.179291  8649 layer_factory.hpp:77] Creating layer BatchNorm32
I1128 13:09:50.179327  8649 net.cpp:84] Creating Layer BatchNorm32
I1128 13:09:50.179345  8649 net.cpp:406] BatchNorm32 <- Convolution32
I1128 13:09:50.179365  8649 net.cpp:367] BatchNorm32 -> Convolution32 (in-place)
I1128 13:09:50.179608  8649 net.cpp:122] Setting up BatchNorm32
I1128 13:09:50.179631  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.179646  8649 net.cpp:137] Memory required for data: 35088120
I1128 13:09:50.179666  8649 layer_factory.hpp:77] Creating layer Scale32
I1128 13:09:50.179687  8649 net.cpp:84] Creating Layer Scale32
I1128 13:09:50.179703  8649 net.cpp:406] Scale32 <- Convolution32
I1128 13:09:50.179720  8649 net.cpp:367] Scale32 -> Convolution32 (in-place)
I1128 13:09:50.179786  8649 layer_factory.hpp:77] Creating layer Scale32
I1128 13:09:50.179937  8649 net.cpp:122] Setting up Scale32
I1128 13:09:50.179960  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.179975  8649 net.cpp:137] Memory required for data: 35098360
I1128 13:09:50.179992  8649 layer_factory.hpp:77] Creating layer ReLU28
I1128 13:09:50.180011  8649 net.cpp:84] Creating Layer ReLU28
I1128 13:09:50.180027  8649 net.cpp:406] ReLU28 <- Convolution32
I1128 13:09:50.180043  8649 net.cpp:367] ReLU28 -> Convolution32 (in-place)
I1128 13:09:50.180516  8649 net.cpp:122] Setting up ReLU28
I1128 13:09:50.180543  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.180559  8649 net.cpp:137] Memory required for data: 35108600
I1128 13:09:50.180573  8649 layer_factory.hpp:77] Creating layer Convolution33
I1128 13:09:50.180598  8649 net.cpp:84] Creating Layer Convolution33
I1128 13:09:50.180614  8649 net.cpp:406] Convolution33 <- Convolution32
I1128 13:09:50.180635  8649 net.cpp:380] Convolution33 -> Convolution33
I1128 13:09:50.183102  8649 net.cpp:122] Setting up Convolution33
I1128 13:09:50.183157  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.183172  8649 net.cpp:137] Memory required for data: 35118840
I1128 13:09:50.183193  8649 layer_factory.hpp:77] Creating layer BatchNorm33
I1128 13:09:50.183218  8649 net.cpp:84] Creating Layer BatchNorm33
I1128 13:09:50.183235  8649 net.cpp:406] BatchNorm33 <- Convolution33
I1128 13:09:50.183254  8649 net.cpp:367] BatchNorm33 -> Convolution33 (in-place)
I1128 13:09:50.183504  8649 net.cpp:122] Setting up BatchNorm33
I1128 13:09:50.183529  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.183545  8649 net.cpp:137] Memory required for data: 35129080
I1128 13:09:50.183565  8649 layer_factory.hpp:77] Creating layer Scale33
I1128 13:09:50.183583  8649 net.cpp:84] Creating Layer Scale33
I1128 13:09:50.183598  8649 net.cpp:406] Scale33 <- Convolution33
I1128 13:09:50.183614  8649 net.cpp:367] Scale33 -> Convolution33 (in-place)
I1128 13:09:50.183682  8649 layer_factory.hpp:77] Creating layer Scale33
I1128 13:09:50.183836  8649 net.cpp:122] Setting up Scale33
I1128 13:09:50.183858  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.183873  8649 net.cpp:137] Memory required for data: 35139320
I1128 13:09:50.183893  8649 layer_factory.hpp:77] Creating layer Eltwise14
I1128 13:09:50.183913  8649 net.cpp:84] Creating Layer Eltwise14
I1128 13:09:50.183929  8649 net.cpp:406] Eltwise14 <- Convolution33
I1128 13:09:50.183945  8649 net.cpp:406] Eltwise14 <- Eltwise13_ReLU27_0_split_1
I1128 13:09:50.183964  8649 net.cpp:380] Eltwise14 -> Eltwise14
I1128 13:09:50.184008  8649 net.cpp:122] Setting up Eltwise14
I1128 13:09:50.184028  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.184042  8649 net.cpp:137] Memory required for data: 35149560
I1128 13:09:50.184058  8649 layer_factory.hpp:77] Creating layer ReLU29
I1128 13:09:50.184075  8649 net.cpp:84] Creating Layer ReLU29
I1128 13:09:50.184089  8649 net.cpp:406] ReLU29 <- Eltwise14
I1128 13:09:50.184108  8649 net.cpp:367] ReLU29 -> Eltwise14 (in-place)
I1128 13:09:50.184411  8649 net.cpp:122] Setting up ReLU29
I1128 13:09:50.184440  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.184456  8649 net.cpp:137] Memory required for data: 35159800
I1128 13:09:50.184478  8649 layer_factory.hpp:77] Creating layer Eltwise14_ReLU29_0_split
I1128 13:09:50.184506  8649 net.cpp:84] Creating Layer Eltwise14_ReLU29_0_split
I1128 13:09:50.184521  8649 net.cpp:406] Eltwise14_ReLU29_0_split <- Eltwise14
I1128 13:09:50.184541  8649 net.cpp:380] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_0
I1128 13:09:50.184563  8649 net.cpp:380] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_1
I1128 13:09:50.184638  8649 net.cpp:122] Setting up Eltwise14_ReLU29_0_split
I1128 13:09:50.184659  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.184675  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.184689  8649 net.cpp:137] Memory required for data: 35180280
I1128 13:09:50.184705  8649 layer_factory.hpp:77] Creating layer Convolution34
I1128 13:09:50.184728  8649 net.cpp:84] Creating Layer Convolution34
I1128 13:09:50.184751  8649 net.cpp:406] Convolution34 <- Eltwise14_ReLU29_0_split_0
I1128 13:09:50.184772  8649 net.cpp:380] Convolution34 -> Convolution34
I1128 13:09:50.186767  8649 net.cpp:122] Setting up Convolution34
I1128 13:09:50.186797  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.186802  8649 net.cpp:137] Memory required for data: 35190520
I1128 13:09:50.186815  8649 layer_factory.hpp:77] Creating layer BatchNorm34
I1128 13:09:50.186830  8649 net.cpp:84] Creating Layer BatchNorm34
I1128 13:09:50.186859  8649 net.cpp:406] BatchNorm34 <- Convolution34
I1128 13:09:50.186882  8649 net.cpp:367] BatchNorm34 -> Convolution34 (in-place)
I1128 13:09:50.187075  8649 net.cpp:122] Setting up BatchNorm34
I1128 13:09:50.187084  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.187089  8649 net.cpp:137] Memory required for data: 35200760
I1128 13:09:50.187099  8649 layer_factory.hpp:77] Creating layer Scale34
I1128 13:09:50.187120  8649 net.cpp:84] Creating Layer Scale34
I1128 13:09:50.187135  8649 net.cpp:406] Scale34 <- Convolution34
I1128 13:09:50.187153  8649 net.cpp:367] Scale34 -> Convolution34 (in-place)
I1128 13:09:50.187218  8649 layer_factory.hpp:77] Creating layer Scale34
I1128 13:09:50.187340  8649 net.cpp:122] Setting up Scale34
I1128 13:09:50.187350  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.187355  8649 net.cpp:137] Memory required for data: 35211000
I1128 13:09:50.187363  8649 layer_factory.hpp:77] Creating layer ReLU30
I1128 13:09:50.187384  8649 net.cpp:84] Creating Layer ReLU30
I1128 13:09:50.187399  8649 net.cpp:406] ReLU30 <- Convolution34
I1128 13:09:50.187417  8649 net.cpp:367] ReLU30 -> Convolution34 (in-place)
I1128 13:09:50.187851  8649 net.cpp:122] Setting up ReLU30
I1128 13:09:50.187865  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.187870  8649 net.cpp:137] Memory required for data: 35221240
I1128 13:09:50.187875  8649 layer_factory.hpp:77] Creating layer Convolution35
I1128 13:09:50.187901  8649 net.cpp:84] Creating Layer Convolution35
I1128 13:09:50.187917  8649 net.cpp:406] Convolution35 <- Convolution34
I1128 13:09:50.187937  8649 net.cpp:380] Convolution35 -> Convolution35
I1128 13:09:50.189554  8649 net.cpp:122] Setting up Convolution35
I1128 13:09:50.189586  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.189591  8649 net.cpp:137] Memory required for data: 35231480
I1128 13:09:50.189604  8649 layer_factory.hpp:77] Creating layer BatchNorm35
I1128 13:09:50.189620  8649 net.cpp:84] Creating Layer BatchNorm35
I1128 13:09:50.189626  8649 net.cpp:406] BatchNorm35 <- Convolution35
I1128 13:09:50.189637  8649 net.cpp:367] BatchNorm35 -> Convolution35 (in-place)
I1128 13:09:50.189863  8649 net.cpp:122] Setting up BatchNorm35
I1128 13:09:50.189874  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.189879  8649 net.cpp:137] Memory required for data: 35241720
I1128 13:09:50.189890  8649 layer_factory.hpp:77] Creating layer Scale35
I1128 13:09:50.189916  8649 net.cpp:84] Creating Layer Scale35
I1128 13:09:50.189932  8649 net.cpp:406] Scale35 <- Convolution35
I1128 13:09:50.189952  8649 net.cpp:367] Scale35 -> Convolution35 (in-place)
I1128 13:09:50.190030  8649 layer_factory.hpp:77] Creating layer Scale35
I1128 13:09:50.190168  8649 net.cpp:122] Setting up Scale35
I1128 13:09:50.190181  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.190187  8649 net.cpp:137] Memory required for data: 35251960
I1128 13:09:50.190196  8649 layer_factory.hpp:77] Creating layer Eltwise15
I1128 13:09:50.190203  8649 net.cpp:84] Creating Layer Eltwise15
I1128 13:09:50.190208  8649 net.cpp:406] Eltwise15 <- Convolution35
I1128 13:09:50.190214  8649 net.cpp:406] Eltwise15 <- Eltwise14_ReLU29_0_split_1
I1128 13:09:50.190223  8649 net.cpp:380] Eltwise15 -> Eltwise15
I1128 13:09:50.190256  8649 net.cpp:122] Setting up Eltwise15
I1128 13:09:50.190281  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.190297  8649 net.cpp:137] Memory required for data: 35262200
I1128 13:09:50.190311  8649 layer_factory.hpp:77] Creating layer ReLU31
I1128 13:09:50.190327  8649 net.cpp:84] Creating Layer ReLU31
I1128 13:09:50.190341  8649 net.cpp:406] ReLU31 <- Eltwise15
I1128 13:09:50.190358  8649 net.cpp:367] ReLU31 -> Eltwise15 (in-place)
I1128 13:09:50.190623  8649 net.cpp:122] Setting up ReLU31
I1128 13:09:50.190634  8649 net.cpp:129] Top shape: 10 64 2 2 (2560)
I1128 13:09:50.190639  8649 net.cpp:137] Memory required for data: 35272440
I1128 13:09:50.190644  8649 layer_factory.hpp:77] Creating layer Pooling5
I1128 13:09:50.190657  8649 net.cpp:84] Creating Layer Pooling5
I1128 13:09:50.190678  8649 net.cpp:406] Pooling5 <- Eltwise15
I1128 13:09:50.190701  8649 net.cpp:380] Pooling5 -> Pooling5
I1128 13:09:50.191236  8649 net.cpp:122] Setting up Pooling5
I1128 13:09:50.191256  8649 net.cpp:129] Top shape: 10 64 1 1 (640)
I1128 13:09:50.191260  8649 net.cpp:137] Memory required for data: 35275000
I1128 13:09:50.191267  8649 layer_factory.hpp:77] Creating layer InnerProduct1
I1128 13:09:50.191278  8649 net.cpp:84] Creating Layer InnerProduct1
I1128 13:09:50.191284  8649 net.cpp:406] InnerProduct1 <- Pooling5
I1128 13:09:50.191295  8649 net.cpp:380] InnerProduct1 -> InnerProduct1
I1128 13:09:50.191465  8649 net.cpp:122] Setting up InnerProduct1
I1128 13:09:50.191475  8649 net.cpp:129] Top shape: 10 10 (100)
I1128 13:09:50.191480  8649 net.cpp:137] Memory required for data: 35275400
I1128 13:09:50.191489  8649 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I1128 13:09:50.191512  8649 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
I1128 13:09:50.191530  8649 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I1128 13:09:50.191550  8649 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I1128 13:09:50.191565  8649 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I1128 13:09:50.191625  8649 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
I1128 13:09:50.191635  8649 net.cpp:129] Top shape: 10 10 (100)
I1128 13:09:50.191642  8649 net.cpp:129] Top shape: 10 10 (100)
I1128 13:09:50.191645  8649 net.cpp:137] Memory required for data: 35276200
I1128 13:09:50.191663  8649 layer_factory.hpp:77] Creating layer Accuracy1
I1128 13:09:50.191682  8649 net.cpp:84] Creating Layer Accuracy1
I1128 13:09:50.191697  8649 net.cpp:406] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_0
I1128 13:09:50.191715  8649 net.cpp:406] Accuracy1 <- Data2_Data1_1_split_0
I1128 13:09:50.191731  8649 net.cpp:380] Accuracy1 -> Accuracy1
I1128 13:09:50.191753  8649 net.cpp:122] Setting up Accuracy1
I1128 13:09:50.191771  8649 net.cpp:129] Top shape: (1)
I1128 13:09:50.191782  8649 net.cpp:137] Memory required for data: 35276204
I1128 13:09:50.191795  8649 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1128 13:09:50.191813  8649 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1128 13:09:50.191828  8649 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_1
I1128 13:09:50.191843  8649 net.cpp:406] SoftmaxWithLoss1 <- Data2_Data1_1_split_1
I1128 13:09:50.191860  8649 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1128 13:09:50.191886  8649 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1128 13:09:50.192788  8649 net.cpp:122] Setting up SoftmaxWithLoss1
I1128 13:09:50.192836  8649 net.cpp:129] Top shape: (1)
I1128 13:09:50.192840  8649 net.cpp:132]     with loss weight 1
I1128 13:09:50.192852  8649 net.cpp:137] Memory required for data: 35276208
I1128 13:09:50.192860  8649 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1128 13:09:50.192868  8649 net.cpp:200] Accuracy1 does not need backward computation.
I1128 13:09:50.192895  8649 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
I1128 13:09:50.192912  8649 net.cpp:198] InnerProduct1 needs backward computation.
I1128 13:09:50.192925  8649 net.cpp:198] Pooling5 needs backward computation.
I1128 13:09:50.192939  8649 net.cpp:198] ReLU31 needs backward computation.
I1128 13:09:50.192952  8649 net.cpp:198] Eltwise15 needs backward computation.
I1128 13:09:50.192968  8649 net.cpp:198] Scale35 needs backward computation.
I1128 13:09:50.192984  8649 net.cpp:198] BatchNorm35 needs backward computation.
I1128 13:09:50.192998  8649 net.cpp:198] Convolution35 needs backward computation.
I1128 13:09:50.193015  8649 net.cpp:198] ReLU30 needs backward computation.
I1128 13:09:50.193028  8649 net.cpp:198] Scale34 needs backward computation.
I1128 13:09:50.193039  8649 net.cpp:198] BatchNorm34 needs backward computation.
I1128 13:09:50.193053  8649 net.cpp:198] Convolution34 needs backward computation.
I1128 13:09:50.193066  8649 net.cpp:198] Eltwise14_ReLU29_0_split needs backward computation.
I1128 13:09:50.193080  8649 net.cpp:198] ReLU29 needs backward computation.
I1128 13:09:50.193094  8649 net.cpp:198] Eltwise14 needs backward computation.
I1128 13:09:50.193110  8649 net.cpp:198] Scale33 needs backward computation.
I1128 13:09:50.193126  8649 net.cpp:198] BatchNorm33 needs backward computation.
I1128 13:09:50.193140  8649 net.cpp:198] Convolution33 needs backward computation.
I1128 13:09:50.193153  8649 net.cpp:198] ReLU28 needs backward computation.
I1128 13:09:50.193166  8649 net.cpp:198] Scale32 needs backward computation.
I1128 13:09:50.193181  8649 net.cpp:198] BatchNorm32 needs backward computation.
I1128 13:09:50.193197  8649 net.cpp:198] Convolution32 needs backward computation.
I1128 13:09:50.193213  8649 net.cpp:198] Eltwise13_ReLU27_0_split needs backward computation.
I1128 13:09:50.193229  8649 net.cpp:198] ReLU27 needs backward computation.
I1128 13:09:50.193245  8649 net.cpp:198] Eltwise13 needs backward computation.
I1128 13:09:50.193261  8649 net.cpp:198] Scale31 needs backward computation.
I1128 13:09:50.193276  8649 net.cpp:198] BatchNorm31 needs backward computation.
I1128 13:09:50.193290  8649 net.cpp:198] Convolution31 needs backward computation.
I1128 13:09:50.193307  8649 net.cpp:198] Pooling4 needs backward computation.
I1128 13:09:50.193321  8649 net.cpp:198] Scale30 needs backward computation.
I1128 13:09:50.193337  8649 net.cpp:198] BatchNorm30 needs backward computation.
I1128 13:09:50.193352  8649 net.cpp:198] Convolution30 needs backward computation.
I1128 13:09:50.193367  8649 net.cpp:198] ReLU26 needs backward computation.
I1128 13:09:50.193382  8649 net.cpp:198] Scale29 needs backward computation.
I1128 13:09:50.193397  8649 net.cpp:198] BatchNorm29 needs backward computation.
I1128 13:09:50.193413  8649 net.cpp:198] Convolution29 needs backward computation.
I1128 13:09:50.193436  8649 net.cpp:198] Eltwise12_ReLU25_0_split needs backward computation.
I1128 13:09:50.193449  8649 net.cpp:198] ReLU25 needs backward computation.
I1128 13:09:50.193454  8649 net.cpp:198] Eltwise12 needs backward computation.
I1128 13:09:50.193460  8649 net.cpp:198] Scale28 needs backward computation.
I1128 13:09:50.193466  8649 net.cpp:198] BatchNorm28 needs backward computation.
I1128 13:09:50.193485  8649 net.cpp:198] Convolution28 needs backward computation.
I1128 13:09:50.193500  8649 net.cpp:198] ReLU24 needs backward computation.
I1128 13:09:50.193516  8649 net.cpp:198] Scale27 needs backward computation.
I1128 13:09:50.193531  8649 net.cpp:198] BatchNorm27 needs backward computation.
I1128 13:09:50.193560  8649 net.cpp:198] Convolution27 needs backward computation.
I1128 13:09:50.193579  8649 net.cpp:198] Eltwise11_ReLU23_0_split needs backward computation.
I1128 13:09:50.193585  8649 net.cpp:198] ReLU23 needs backward computation.
I1128 13:09:50.193590  8649 net.cpp:198] Eltwise11 needs backward computation.
I1128 13:09:50.193598  8649 net.cpp:198] Scale26 needs backward computation.
I1128 13:09:50.193615  8649 net.cpp:198] BatchNorm26 needs backward computation.
I1128 13:09:50.193631  8649 net.cpp:198] Convolution26 needs backward computation.
I1128 13:09:50.193648  8649 net.cpp:198] ReLU22 needs backward computation.
I1128 13:09:50.193663  8649 net.cpp:198] Scale25 needs backward computation.
I1128 13:09:50.193678  8649 net.cpp:198] BatchNorm25 needs backward computation.
I1128 13:09:50.193692  8649 net.cpp:198] Convolution25 needs backward computation.
I1128 13:09:50.193708  8649 net.cpp:198] Eltwise10_ReLU21_0_split needs backward computation.
I1128 13:09:50.193723  8649 net.cpp:198] ReLU21 needs backward computation.
I1128 13:09:50.193738  8649 net.cpp:198] Eltwise10 needs backward computation.
I1128 13:09:50.193753  8649 net.cpp:198] Scale24 needs backward computation.
I1128 13:09:50.193769  8649 net.cpp:198] BatchNorm24 needs backward computation.
I1128 13:09:50.193784  8649 net.cpp:198] Convolution24 needs backward computation.
I1128 13:09:50.193800  8649 net.cpp:198] Pooling3 needs backward computation.
I1128 13:09:50.193815  8649 net.cpp:198] Scale23 needs backward computation.
I1128 13:09:50.193831  8649 net.cpp:198] BatchNorm23 needs backward computation.
I1128 13:09:50.193846  8649 net.cpp:198] Convolution23 needs backward computation.
I1128 13:09:50.193862  8649 net.cpp:198] ReLU20 needs backward computation.
I1128 13:09:50.193876  8649 net.cpp:198] Scale22 needs backward computation.
I1128 13:09:50.193892  8649 net.cpp:198] BatchNorm22 needs backward computation.
I1128 13:09:50.193907  8649 net.cpp:198] Convolution22 needs backward computation.
I1128 13:09:50.193922  8649 net.cpp:198] Eltwise9_ReLU19_0_split needs backward computation.
I1128 13:09:50.193938  8649 net.cpp:198] ReLU19 needs backward computation.
I1128 13:09:50.193953  8649 net.cpp:198] Eltwise9 needs backward computation.
I1128 13:09:50.193970  8649 net.cpp:198] Scale21 needs backward computation.
I1128 13:09:50.193984  8649 net.cpp:198] BatchNorm21 needs backward computation.
I1128 13:09:50.194000  8649 net.cpp:198] Convolution21 needs backward computation.
I1128 13:09:50.194016  8649 net.cpp:198] ReLU18 needs backward computation.
I1128 13:09:50.194032  8649 net.cpp:198] Scale20 needs backward computation.
I1128 13:09:50.194046  8649 net.cpp:198] BatchNorm20 needs backward computation.
I1128 13:09:50.194061  8649 net.cpp:198] Convolution20 needs backward computation.
I1128 13:09:50.194077  8649 net.cpp:198] Eltwise8_ReLU17_0_split needs backward computation.
I1128 13:09:50.194092  8649 net.cpp:198] ReLU17 needs backward computation.
I1128 13:09:50.194108  8649 net.cpp:198] Eltwise8 needs backward computation.
I1128 13:09:50.194133  8649 net.cpp:198] Scale19 needs backward computation.
I1128 13:09:50.194141  8649 net.cpp:198] BatchNorm19 needs backward computation.
I1128 13:09:50.194146  8649 net.cpp:198] Convolution19 needs backward computation.
I1128 13:09:50.194152  8649 net.cpp:198] ReLU16 needs backward computation.
I1128 13:09:50.194157  8649 net.cpp:198] Scale18 needs backward computation.
I1128 13:09:50.194161  8649 net.cpp:198] BatchNorm18 needs backward computation.
I1128 13:09:50.194166  8649 net.cpp:198] Convolution18 needs backward computation.
I1128 13:09:50.194172  8649 net.cpp:198] Eltwise7_ReLU15_0_split needs backward computation.
I1128 13:09:50.194190  8649 net.cpp:198] ReLU15 needs backward computation.
I1128 13:09:50.194205  8649 net.cpp:198] Eltwise7 needs backward computation.
I1128 13:09:50.194221  8649 net.cpp:198] Scale17 needs backward computation.
I1128 13:09:50.194237  8649 net.cpp:198] BatchNorm17 needs backward computation.
I1128 13:09:50.194253  8649 net.cpp:198] Convolution17 needs backward computation.
I1128 13:09:50.194283  8649 net.cpp:198] Pooling2 needs backward computation.
I1128 13:09:50.194299  8649 net.cpp:198] Scale16 needs backward computation.
I1128 13:09:50.194305  8649 net.cpp:198] BatchNorm16 needs backward computation.
I1128 13:09:50.194310  8649 net.cpp:198] Convolution16 needs backward computation.
I1128 13:09:50.194315  8649 net.cpp:198] ReLU14 needs backward computation.
I1128 13:09:50.194321  8649 net.cpp:198] Scale15 needs backward computation.
I1128 13:09:50.194339  8649 net.cpp:198] BatchNorm15 needs backward computation.
I1128 13:09:50.194353  8649 net.cpp:198] Convolution15 needs backward computation.
I1128 13:09:50.194370  8649 net.cpp:198] Eltwise6_ReLU13_0_split needs backward computation.
I1128 13:09:50.194386  8649 net.cpp:198] ReLU13 needs backward computation.
I1128 13:09:50.194399  8649 net.cpp:198] Eltwise6 needs backward computation.
I1128 13:09:50.194408  8649 net.cpp:198] Scale14 needs backward computation.
I1128 13:09:50.194414  8649 net.cpp:198] BatchNorm14 needs backward computation.
I1128 13:09:50.194418  8649 net.cpp:198] Convolution14 needs backward computation.
I1128 13:09:50.194422  8649 net.cpp:198] ReLU12 needs backward computation.
I1128 13:09:50.194427  8649 net.cpp:198] Scale13 needs backward computation.
I1128 13:09:50.194432  8649 net.cpp:198] BatchNorm13 needs backward computation.
I1128 13:09:50.194438  8649 net.cpp:198] Convolution13 needs backward computation.
I1128 13:09:50.194445  8649 net.cpp:198] Eltwise5_ReLU11_0_split needs backward computation.
I1128 13:09:50.194453  8649 net.cpp:198] ReLU11 needs backward computation.
I1128 13:09:50.194458  8649 net.cpp:198] Eltwise5 needs backward computation.
I1128 13:09:50.194463  8649 net.cpp:198] Scale12 needs backward computation.
I1128 13:09:50.194466  8649 net.cpp:198] BatchNorm12 needs backward computation.
I1128 13:09:50.194469  8649 net.cpp:198] Convolution12 needs backward computation.
I1128 13:09:50.194473  8649 net.cpp:198] ReLU10 needs backward computation.
I1128 13:09:50.194476  8649 net.cpp:198] Scale11 needs backward computation.
I1128 13:09:50.194480  8649 net.cpp:198] BatchNorm11 needs backward computation.
I1128 13:09:50.194484  8649 net.cpp:198] Convolution11 needs backward computation.
I1128 13:09:50.194490  8649 net.cpp:198] Eltwise4_ReLU9_0_split needs backward computation.
I1128 13:09:50.194497  8649 net.cpp:198] ReLU9 needs backward computation.
I1128 13:09:50.194504  8649 net.cpp:198] Eltwise4 needs backward computation.
I1128 13:09:50.194510  8649 net.cpp:198] Scale10 needs backward computation.
I1128 13:09:50.194517  8649 net.cpp:198] BatchNorm10 needs backward computation.
I1128 13:09:50.194524  8649 net.cpp:198] Convolution10 needs backward computation.
I1128 13:09:50.194532  8649 net.cpp:198] Pooling1 needs backward computation.
I1128 13:09:50.194540  8649 net.cpp:198] Scale9 needs backward computation.
I1128 13:09:50.194546  8649 net.cpp:198] BatchNorm9 needs backward computation.
I1128 13:09:50.194552  8649 net.cpp:198] Convolution9 needs backward computation.
I1128 13:09:50.194561  8649 net.cpp:198] ReLU8 needs backward computation.
I1128 13:09:50.194566  8649 net.cpp:198] Scale8 needs backward computation.
I1128 13:09:50.194571  8649 net.cpp:198] BatchNorm8 needs backward computation.
I1128 13:09:50.194576  8649 net.cpp:198] Convolution8 needs backward computation.
I1128 13:09:50.194592  8649 net.cpp:198] Eltwise3_ReLU7_0_split needs backward computation.
I1128 13:09:50.194599  8649 net.cpp:198] ReLU7 needs backward computation.
I1128 13:09:50.194604  8649 net.cpp:198] Eltwise3 needs backward computation.
I1128 13:09:50.194610  8649 net.cpp:198] Scale7 needs backward computation.
I1128 13:09:50.194617  8649 net.cpp:198] BatchNorm7 needs backward computation.
I1128 13:09:50.194623  8649 net.cpp:198] Convolution7 needs backward computation.
I1128 13:09:50.194630  8649 net.cpp:198] ReLU6 needs backward computation.
I1128 13:09:50.194638  8649 net.cpp:198] Scale6 needs backward computation.
I1128 13:09:50.194643  8649 net.cpp:198] BatchNorm6 needs backward computation.
I1128 13:09:50.194650  8649 net.cpp:198] Convolution6 needs backward computation.
I1128 13:09:50.194663  8649 net.cpp:198] Eltwise2_ReLU5_0_split needs backward computation.
I1128 13:09:50.194671  8649 net.cpp:198] ReLU5 needs backward computation.
I1128 13:09:50.194679  8649 net.cpp:198] Eltwise2 needs backward computation.
I1128 13:09:50.194687  8649 net.cpp:198] Scale5 needs backward computation.
I1128 13:09:50.194692  8649 net.cpp:198] BatchNorm5 needs backward computation.
I1128 13:09:50.194700  8649 net.cpp:198] Convolution5 needs backward computation.
I1128 13:09:50.194706  8649 net.cpp:198] ReLU4 needs backward computation.
I1128 13:09:50.194712  8649 net.cpp:198] Scale4 needs backward computation.
I1128 13:09:50.194720  8649 net.cpp:198] BatchNorm4 needs backward computation.
I1128 13:09:50.194726  8649 net.cpp:198] Convolution4 needs backward computation.
I1128 13:09:50.194733  8649 net.cpp:198] Eltwise1_ReLU3_0_split needs backward computation.
I1128 13:09:50.194739  8649 net.cpp:198] ReLU3 needs backward computation.
I1128 13:09:50.194746  8649 net.cpp:198] Eltwise1 needs backward computation.
I1128 13:09:50.194752  8649 net.cpp:198] Scale3 needs backward computation.
I1128 13:09:50.194759  8649 net.cpp:198] BatchNorm3 needs backward computation.
I1128 13:09:50.194766  8649 net.cpp:198] Convolution3 needs backward computation.
I1128 13:09:50.194772  8649 net.cpp:198] ReLU2 needs backward computation.
I1128 13:09:50.194779  8649 net.cpp:198] Scale2 needs backward computation.
I1128 13:09:50.194784  8649 net.cpp:198] BatchNorm2 needs backward computation.
I1128 13:09:50.194792  8649 net.cpp:198] Convolution2 needs backward computation.
I1128 13:09:50.194802  8649 net.cpp:198] Convolution1_ReLU1_0_split needs backward computation.
I1128 13:09:50.194810  8649 net.cpp:198] ReLU1 needs backward computation.
I1128 13:09:50.194816  8649 net.cpp:198] Scale1 needs backward computation.
I1128 13:09:50.194823  8649 net.cpp:198] BatchNorm1 needs backward computation.
I1128 13:09:50.194829  8649 net.cpp:198] Convolution1 needs backward computation.
I1128 13:09:50.194838  8649 net.cpp:200] Data2_Data1_1_split does not need backward computation.
I1128 13:09:50.194846  8649 net.cpp:200] Data1 does not need backward computation.
I1128 13:09:50.194852  8649 net.cpp:242] This network produces output Accuracy1
I1128 13:09:50.194860  8649 net.cpp:242] This network produces output SoftmaxWithLoss1
I1128 13:09:50.194970  8649 net.cpp:255] Network initialization done.
I1128 13:09:50.195283  8649 solver.cpp:56] Solver scaffolding done.
I1128 13:09:50.205783  8649 caffe.cpp:248] Starting Optimization
I1128 13:09:50.205845  8649 solver.cpp:272] Solving 
I1128 13:09:50.205860  8649 solver.cpp:273] Learning Rate Policy: multistep
I1128 13:09:50.211616  8649 solver.cpp:330] Iteration 0, Testing net (#0)
I1128 13:10:02.893589  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:10:02.946143  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 1
I1128 13:10:02.946174  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 87.3361 (* 1 = 87.3361 loss)
I1128 13:10:03.428901  8649 solver.cpp:218] Iteration 0 (9899.05 iter/s, 13.2241s/200 iters), loss = 2.69697
I1128 13:10:03.428944  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.32389 (* 1 = 2.32389 loss)
I1128 13:10:03.428972  8649 sgd_solver.cpp:105] Iteration 0, lr = 0.1
I1128 13:11:37.319823  8649 solver.cpp:218] Iteration 200 (2.12995 iter/s, 93.8988s/200 iters), loss = 1.63904
I1128 13:11:37.319922  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.67508 (* 1 = 1.67508 loss)
I1128 13:11:37.319933  8649 sgd_solver.cpp:105] Iteration 200, lr = 0.1
I1128 13:13:05.926133  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:13:11.213498  8649 solver.cpp:218] Iteration 400 (2.1299 iter/s, 93.9012s/200 iters), loss = 1.48383
I1128 13:13:11.213548  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.31237 (* 1 = 1.31237 loss)
I1128 13:13:11.213559  8649 sgd_solver.cpp:105] Iteration 400, lr = 0.1
I1128 13:14:45.040774  8649 solver.cpp:218] Iteration 600 (2.13141 iter/s, 93.8346s/200 iters), loss = 1.27585
I1128 13:14:45.040938  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.37163 (* 1 = 1.37163 loss)
I1128 13:14:45.040949  8649 sgd_solver.cpp:105] Iteration 600, lr = 0.1
I1128 13:16:09.154820  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:16:18.893532  8649 solver.cpp:218] Iteration 800 (2.13084 iter/s, 93.8597s/200 iters), loss = 0.853564
I1128 13:16:18.893574  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.641065 (* 1 = 0.641065 loss)
I1128 13:16:18.893582  8649 sgd_solver.cpp:105] Iteration 800, lr = 0.1
I1128 13:17:52.258847  8649 solver.cpp:330] Iteration 1000, Testing net (#0)
I1128 13:18:04.802510  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:18:04.855136  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.377999
I1128 13:18:04.855199  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.21895 (* 1 = 2.21895 loss)
I1128 13:18:05.319777  8649 solver.cpp:218] Iteration 1000 (1.8791 iter/s, 106.434s/200 iters), loss = 0.969861
I1128 13:18:05.319834  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 1.03915 (* 1 = 1.03915 loss)
I1128 13:18:05.319846  8649 sgd_solver.cpp:105] Iteration 1000, lr = 0.1
I1128 13:19:25.084002  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:19:39.174015  8649 solver.cpp:218] Iteration 1200 (2.13081 iter/s, 93.861s/200 iters), loss = 0.669296
I1128 13:19:39.174052  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.632792 (* 1 = 0.632792 loss)
I1128 13:19:39.174060  8649 sgd_solver.cpp:105] Iteration 1200, lr = 0.1
I1128 13:21:13.013082  8649 solver.cpp:218] Iteration 1400 (2.13116 iter/s, 93.8457s/200 iters), loss = 0.686483
I1128 13:21:13.013164  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.588332 (* 1 = 0.588332 loss)
I1128 13:21:13.013176  8649 sgd_solver.cpp:105] Iteration 1400, lr = 0.1
I1128 13:22:28.330014  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:22:46.877979  8649 solver.cpp:218] Iteration 1600 (2.13057 iter/s, 93.8714s/200 iters), loss = 0.842834
I1128 13:22:46.878026  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.887761 (* 1 = 0.887761 loss)
I1128 13:22:46.878034  8649 sgd_solver.cpp:105] Iteration 1600, lr = 0.1
I1128 13:24:20.738780  8649 solver.cpp:218] Iteration 1800 (2.13067 iter/s, 93.8672s/200 iters), loss = 0.71956
I1128 13:24:20.738865  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.851834 (* 1 = 0.851834 loss)
I1128 13:24:20.738880  8649 sgd_solver.cpp:105] Iteration 1800, lr = 0.1
I1128 13:25:31.716946  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:25:54.131528  8649 solver.cpp:330] Iteration 2000, Testing net (#0)
I1128 13:26:06.655310  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:26:06.707478  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7235
I1128 13:26:06.707504  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.783118 (* 1 = 0.783118 loss)
I1128 13:26:07.172284  8649 solver.cpp:218] Iteration 2000 (1.87898 iter/s, 106.441s/200 iters), loss = 0.701151
I1128 13:26:07.172322  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.509082 (* 1 = 0.509082 loss)
I1128 13:26:07.172330  8649 sgd_solver.cpp:105] Iteration 2000, lr = 0.1
I1128 13:27:41.020160  8649 solver.cpp:218] Iteration 2200 (2.13097 iter/s, 93.8542s/200 iters), loss = 0.553148
I1128 13:27:41.020272  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.968567 (* 1 = 0.968567 loss)
I1128 13:27:41.020283  8649 sgd_solver.cpp:105] Iteration 2200, lr = 0.1
I1128 13:28:47.527591  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:29:14.864428  8649 solver.cpp:218] Iteration 2400 (2.13105 iter/s, 93.8505s/200 iters), loss = 0.474697
I1128 13:29:14.864467  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.404772 (* 1 = 0.404772 loss)
I1128 13:29:14.864475  8649 sgd_solver.cpp:105] Iteration 2400, lr = 0.1
I1128 13:30:01.337821  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_2500.caffemodel
I1128 13:30:01.355348  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_2500.solverstate
I1128 13:30:48.746577  8649 solver.cpp:218] Iteration 2600 (2.13019 iter/s, 93.8884s/200 iters), loss = 0.668869
I1128 13:30:48.746670  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.725386 (* 1 = 0.725386 loss)
I1128 13:30:48.746682  8649 sgd_solver.cpp:105] Iteration 2600, lr = 0.1
I1128 13:31:50.922809  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:32:22.740052  8649 solver.cpp:218] Iteration 2800 (2.12767 iter/s, 93.9996s/200 iters), loss = 0.572144
I1128 13:32:22.740159  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.526018 (* 1 = 0.526018 loss)
I1128 13:32:22.740170  8649 sgd_solver.cpp:105] Iteration 2800, lr = 0.1
I1128 13:33:56.605049  8649 solver.cpp:330] Iteration 3000, Testing net (#0)
I1128 13:34:09.300216  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:34:09.354259  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7014
I1128 13:34:09.354315  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.868147 (* 1 = 0.868147 loss)
I1128 13:34:09.820933  8649 solver.cpp:218] Iteration 3000 (1.86763 iter/s, 107.088s/200 iters), loss = 0.591758
I1128 13:34:09.821059  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.65131 (* 1 = 0.65131 loss)
I1128 13:34:09.821074  8649 sgd_solver.cpp:105] Iteration 3000, lr = 0.1
I1128 13:35:07.830564  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:35:44.143007  8649 solver.cpp:218] Iteration 3200 (2.12026 iter/s, 94.3282s/200 iters), loss = 0.440875
I1128 13:35:44.143087  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.230094 (* 1 = 0.230094 loss)
I1128 13:35:44.143097  8649 sgd_solver.cpp:105] Iteration 3200, lr = 0.1
I1128 13:37:18.466131  8649 solver.cpp:218] Iteration 3400 (2.12023 iter/s, 94.3292s/200 iters), loss = 0.501419
I1128 13:37:18.466272  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.686715 (* 1 = 0.686715 loss)
I1128 13:37:18.466285  8649 sgd_solver.cpp:105] Iteration 3400, lr = 0.1
I1128 13:38:12.111771  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:38:52.781841  8649 solver.cpp:218] Iteration 3600 (2.1204 iter/s, 94.3217s/200 iters), loss = 0.528651
I1128 13:38:52.781977  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.534789 (* 1 = 0.534789 loss)
I1128 13:38:52.781992  8649 sgd_solver.cpp:105] Iteration 3600, lr = 0.1
I1128 13:40:27.084591  8649 solver.cpp:218] Iteration 3800 (2.1207 iter/s, 94.3086s/200 iters), loss = 0.543882
I1128 13:40:27.084688  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.549362 (* 1 = 0.549362 loss)
I1128 13:40:27.084702  8649 sgd_solver.cpp:105] Iteration 3800, lr = 0.1
I1128 13:41:16.249936  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:42:00.900657  8649 solver.cpp:330] Iteration 4000, Testing net (#0)
I1128 13:42:13.372602  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:42:13.424973  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7775
I1128 13:42:13.424999  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.658306 (* 1 = 0.658306 loss)
I1128 13:42:13.890081  8649 solver.cpp:218] Iteration 4000 (1.87244 iter/s, 106.812s/200 iters), loss = 0.348649
I1128 13:42:13.890131  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.298301 (* 1 = 0.298301 loss)
I1128 13:42:13.890141  8649 sgd_solver.cpp:105] Iteration 4000, lr = 0.1
I1128 13:43:47.694437  8649 solver.cpp:218] Iteration 4200 (2.13196 iter/s, 93.8103s/200 iters), loss = 0.455998
I1128 13:43:47.694613  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.440444 (* 1 = 0.440444 loss)
I1128 13:43:47.694629  8649 sgd_solver.cpp:105] Iteration 4200, lr = 0.1
I1128 13:44:32.260666  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:45:21.498605  8649 solver.cpp:218] Iteration 4400 (2.13197 iter/s, 93.81s/200 iters), loss = 0.465428
I1128 13:45:21.498741  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.642952 (* 1 = 0.642952 loss)
I1128 13:45:21.498755  8649 sgd_solver.cpp:105] Iteration 4400, lr = 0.1
I1128 13:46:55.300734  8649 solver.cpp:218] Iteration 4600 (2.13201 iter/s, 93.808s/200 iters), loss = 0.423922
I1128 13:46:55.300827  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.576296 (* 1 = 0.576296 loss)
I1128 13:46:55.300838  8649 sgd_solver.cpp:105] Iteration 4600, lr = 0.1
I1128 13:47:35.408825  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:48:29.110791  8649 solver.cpp:218] Iteration 4800 (2.13183 iter/s, 93.816s/200 iters), loss = 0.417641
I1128 13:48:29.110915  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.381658 (* 1 = 0.381658 loss)
I1128 13:48:29.110927  8649 sgd_solver.cpp:105] Iteration 4800, lr = 0.1
I1128 13:50:02.456694  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_5000.caffemodel
I1128 13:50:02.469466  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_5000.solverstate
I1128 13:50:02.473968  8649 solver.cpp:330] Iteration 5000, Testing net (#0)
I1128 13:50:15.021085  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:50:15.073623  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7819
I1128 13:50:15.073652  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.628341 (* 1 = 0.628341 loss)
I1128 13:50:15.537189  8649 solver.cpp:218] Iteration 5000 (1.87911 iter/s, 106.433s/200 iters), loss = 0.355209
I1128 13:50:15.537230  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.45369 (* 1 = 0.45369 loss)
I1128 13:50:15.537237  8649 sgd_solver.cpp:105] Iteration 5000, lr = 0.1
I1128 13:50:51.305018  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:51:49.347292  8649 solver.cpp:218] Iteration 5200 (2.13183 iter/s, 93.8161s/200 iters), loss = 0.469645
I1128 13:51:49.347381  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.688421 (* 1 = 0.688421 loss)
I1128 13:51:49.347391  8649 sgd_solver.cpp:105] Iteration 5200, lr = 0.1
I1128 13:53:23.217952  8649 solver.cpp:218] Iteration 5400 (2.13046 iter/s, 93.8766s/200 iters), loss = 0.322017
I1128 13:53:23.218091  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.188031 (* 1 = 0.188031 loss)
I1128 13:53:23.218101  8649 sgd_solver.cpp:105] Iteration 5400, lr = 0.1
I1128 13:53:54.555033  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:54:57.125917  8649 solver.cpp:218] Iteration 5600 (2.12961 iter/s, 93.9139s/200 iters), loss = 0.381209
I1128 13:54:57.126056  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.301169 (* 1 = 0.301169 loss)
I1128 13:54:57.126070  8649 sgd_solver.cpp:105] Iteration 5600, lr = 0.1
I1128 13:56:31.018757  8649 solver.cpp:218] Iteration 5800 (2.12995 iter/s, 93.8987s/200 iters), loss = 0.373188
I1128 13:56:31.018865  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.589822 (* 1 = 0.589822 loss)
I1128 13:56:31.018882  8649 sgd_solver.cpp:105] Iteration 5800, lr = 0.1
I1128 13:56:58.017093  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:58:04.443078  8649 solver.cpp:330] Iteration 6000, Testing net (#0)
I1128 13:58:16.954648  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 13:58:17.006801  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7533
I1128 13:58:17.006831  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.752736 (* 1 = 0.752736 loss)
I1128 13:58:17.471355  8649 solver.cpp:218] Iteration 6000 (1.87865 iter/s, 106.459s/200 iters), loss = 0.541334
I1128 13:58:17.471411  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.437983 (* 1 = 0.437983 loss)
I1128 13:58:17.471422  8649 sgd_solver.cpp:105] Iteration 6000, lr = 0.1
I1128 13:59:51.364650  8649 solver.cpp:218] Iteration 6200 (2.12994 iter/s, 93.8993s/200 iters), loss = 0.300079
I1128 13:59:51.364734  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210754 (* 1 = 0.210754 loss)
I1128 13:59:51.364744  8649 sgd_solver.cpp:105] Iteration 6200, lr = 0.1
I1128 14:00:13.912871  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:01:25.263595  8649 solver.cpp:218] Iteration 6400 (2.12981 iter/s, 93.9049s/200 iters), loss = 0.445638
I1128 14:01:25.263728  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.232628 (* 1 = 0.232628 loss)
I1128 14:01:25.263741  8649 sgd_solver.cpp:105] Iteration 6400, lr = 0.1
I1128 14:02:59.126497  8649 solver.cpp:218] Iteration 6600 (2.13063 iter/s, 93.8688s/200 iters), loss = 0.457511
I1128 14:02:59.126590  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.2809 (* 1 = 0.2809 loss)
I1128 14:02:59.126605  8649 sgd_solver.cpp:105] Iteration 6600, lr = 0.1
I1128 14:03:17.318100  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:04:32.982350  8649 solver.cpp:218] Iteration 6800 (2.13079 iter/s, 93.8618s/200 iters), loss = 0.394649
I1128 14:04:32.982511  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.498523 (* 1 = 0.498523 loss)
I1128 14:04:32.982520  8649 sgd_solver.cpp:105] Iteration 6800, lr = 0.1
I1128 14:06:06.377650  8649 solver.cpp:330] Iteration 7000, Testing net (#0)
I1128 14:06:18.880246  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:06:18.932494  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7991
I1128 14:06:18.932531  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.581172 (* 1 = 0.581172 loss)
I1128 14:06:19.397084  8649 solver.cpp:218] Iteration 7000 (1.87932 iter/s, 106.421s/200 iters), loss = 0.386253
I1128 14:06:19.397145  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.356121 (* 1 = 0.356121 loss)
I1128 14:06:19.397163  8649 sgd_solver.cpp:105] Iteration 7000, lr = 0.1
I1128 14:06:33.130060  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:07:53.263203  8649 solver.cpp:218] Iteration 7200 (2.13056 iter/s, 93.8721s/200 iters), loss = 0.377891
I1128 14:07:53.263296  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.460166 (* 1 = 0.460166 loss)
I1128 14:07:53.263308  8649 sgd_solver.cpp:105] Iteration 7200, lr = 0.1
I1128 14:09:27.123684  8649 solver.cpp:218] Iteration 7400 (2.13069 iter/s, 93.8664s/200 iters), loss = 0.311787
I1128 14:09:27.123780  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.227375 (* 1 = 0.227375 loss)
I1128 14:09:27.123790  8649 sgd_solver.cpp:105] Iteration 7400, lr = 0.1
I1128 14:09:36.517174  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:10:13.595113  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_7500.caffemodel
I1128 14:10:13.607978  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_7500.solverstate
I1128 14:11:01.014027  8649 solver.cpp:218] Iteration 7600 (2.13001 iter/s, 93.8963s/200 iters), loss = 0.366438
I1128 14:11:01.014189  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.463984 (* 1 = 0.463984 loss)
I1128 14:11:01.014202  8649 sgd_solver.cpp:105] Iteration 7600, lr = 0.1
I1128 14:12:34.892781  8649 solver.cpp:218] Iteration 7800 (2.13027 iter/s, 93.8847s/200 iters), loss = 0.283304
I1128 14:12:34.893002  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.253124 (* 1 = 0.253124 loss)
I1128 14:12:34.893015  8649 sgd_solver.cpp:105] Iteration 7800, lr = 0.1
I1128 14:12:39.830415  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:14:08.327908  8649 solver.cpp:330] Iteration 8000, Testing net (#0)
I1128 14:14:20.833837  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:14:20.886420  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.807201
I1128 14:14:20.886447  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.575319 (* 1 = 0.575319 loss)
I1128 14:14:21.351733  8649 solver.cpp:218] Iteration 8000 (1.87857 iter/s, 106.464s/200 iters), loss = 0.288041
I1128 14:14:21.351783  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.369187 (* 1 = 0.369187 loss)
I1128 14:14:21.351791  8649 sgd_solver.cpp:105] Iteration 8000, lr = 0.1
I1128 14:15:55.254729  8649 solver.cpp:218] Iteration 8200 (2.12976 iter/s, 93.9074s/200 iters), loss = 0.387256
I1128 14:15:55.254802  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.381375 (* 1 = 0.381375 loss)
I1128 14:15:55.254815  8649 sgd_solver.cpp:105] Iteration 8200, lr = 0.1
I1128 14:15:55.848373  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:17:29.167949  8649 solver.cpp:218] Iteration 8400 (2.12952 iter/s, 93.9178s/200 iters), loss = 0.439832
I1128 14:17:29.168112  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.31479 (* 1 = 0.31479 loss)
I1128 14:17:29.168125  8649 sgd_solver.cpp:105] Iteration 8400, lr = 0.1
I1128 14:18:59.198151  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:19:03.073508  8649 solver.cpp:218] Iteration 8600 (2.12969 iter/s, 93.9102s/200 iters), loss = 0.304621
I1128 14:19:03.073562  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.233722 (* 1 = 0.233722 loss)
I1128 14:19:03.073573  8649 sgd_solver.cpp:105] Iteration 8600, lr = 0.1
I1128 14:20:36.967275  8649 solver.cpp:218] Iteration 8800 (2.12995 iter/s, 93.8987s/200 iters), loss = 0.292149
I1128 14:20:36.967433  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.230517 (* 1 = 0.230517 loss)
I1128 14:20:36.967444  8649 sgd_solver.cpp:105] Iteration 8800, lr = 0.1
I1128 14:22:02.641618  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:22:10.383791  8649 solver.cpp:330] Iteration 9000, Testing net (#0)
I1128 14:22:22.886283  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:22:22.938891  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7748
I1128 14:22:22.938925  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.697173 (* 1 = 0.697173 loss)
I1128 14:22:23.402822  8649 solver.cpp:218] Iteration 9000 (1.87897 iter/s, 106.441s/200 iters), loss = 0.357026
I1128 14:22:23.402871  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.306638 (* 1 = 0.306638 loss)
I1128 14:22:23.402881  8649 sgd_solver.cpp:105] Iteration 9000, lr = 0.1
I1128 14:23:57.344780  8649 solver.cpp:218] Iteration 9200 (2.12886 iter/s, 93.9472s/200 iters), loss = 0.39669
I1128 14:23:57.344885  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.485507 (* 1 = 0.485507 loss)
I1128 14:23:57.344897  8649 sgd_solver.cpp:105] Iteration 9200, lr = 0.1
I1128 14:25:18.585728  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:25:31.268311  8649 solver.cpp:218] Iteration 9400 (2.12927 iter/s, 93.9288s/200 iters), loss = 0.360207
I1128 14:25:31.268355  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.392076 (* 1 = 0.392076 loss)
I1128 14:25:31.268364  8649 sgd_solver.cpp:105] Iteration 9400, lr = 0.1
I1128 14:27:05.185958  8649 solver.cpp:218] Iteration 9600 (2.1294 iter/s, 93.923s/200 iters), loss = 0.288201
I1128 14:27:05.186139  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.231909 (* 1 = 0.231909 loss)
I1128 14:27:05.186174  8649 sgd_solver.cpp:105] Iteration 9600, lr = 0.1
I1128 14:28:22.089967  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:28:39.107846  8649 solver.cpp:218] Iteration 9800 (2.12931 iter/s, 93.9272s/200 iters), loss = 0.370339
I1128 14:28:39.107890  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.59219 (* 1 = 0.59219 loss)
I1128 14:28:39.107898  8649 sgd_solver.cpp:105] Iteration 9800, lr = 0.1
I1128 14:30:12.578615  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_10000.caffemodel
I1128 14:30:12.591311  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_10000.solverstate
I1128 14:30:12.596395  8649 solver.cpp:330] Iteration 10000, Testing net (#0)
I1128 14:30:25.082864  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:30:25.135532  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.773301
I1128 14:30:25.135560  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.709068 (* 1 = 0.709068 loss)
I1128 14:30:25.602131  8649 solver.cpp:218] Iteration 10000 (1.87793 iter/s, 106.501s/200 iters), loss = 0.380943
I1128 14:30:25.602169  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.419588 (* 1 = 0.419588 loss)
I1128 14:30:25.602177  8649 sgd_solver.cpp:105] Iteration 10000, lr = 0.1
I1128 14:31:38.062362  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:31:59.545385  8649 solver.cpp:218] Iteration 10200 (2.12882 iter/s, 93.9488s/200 iters), loss = 0.327508
I1128 14:31:59.545423  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.453904 (* 1 = 0.453904 loss)
I1128 14:31:59.545430  8649 sgd_solver.cpp:105] Iteration 10200, lr = 0.1
I1128 14:33:33.339282  8649 solver.cpp:218] Iteration 10400 (2.13221 iter/s, 93.7994s/200 iters), loss = 0.314824
I1128 14:33:33.339354  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.311514 (* 1 = 0.311514 loss)
I1128 14:33:33.339362  8649 sgd_solver.cpp:105] Iteration 10400, lr = 0.1
I1128 14:34:41.350184  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:35:07.140022  8649 solver.cpp:218] Iteration 10600 (2.13205 iter/s, 93.8063s/200 iters), loss = 0.328301
I1128 14:35:07.140054  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.260493 (* 1 = 0.260493 loss)
I1128 14:35:07.140064  8649 sgd_solver.cpp:105] Iteration 10600, lr = 0.1
I1128 14:36:40.932962  8649 solver.cpp:218] Iteration 10800 (2.13223 iter/s, 93.7985s/200 iters), loss = 0.39032
I1128 14:36:40.933115  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.389774 (* 1 = 0.389774 loss)
I1128 14:36:40.933128  8649 sgd_solver.cpp:105] Iteration 10800, lr = 0.1
I1128 14:37:44.490301  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:38:14.269150  8649 solver.cpp:330] Iteration 11000, Testing net (#0)
I1128 14:38:26.811004  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:38:26.863234  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7851
I1128 14:38:26.863260  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.659018 (* 1 = 0.659018 loss)
I1128 14:38:27.327661  8649 solver.cpp:218] Iteration 11000 (1.87968 iter/s, 106.401s/200 iters), loss = 0.280975
I1128 14:38:27.327697  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.137264 (* 1 = 0.137264 loss)
I1128 14:38:27.327705  8649 sgd_solver.cpp:105] Iteration 11000, lr = 0.1
I1128 14:40:01.124249  8649 solver.cpp:218] Iteration 11200 (2.13215 iter/s, 93.8022s/200 iters), loss = 0.375926
I1128 14:40:01.124392  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.101191 (* 1 = 0.101191 loss)
I1128 14:40:01.124403  8649 sgd_solver.cpp:105] Iteration 11200, lr = 0.1
I1128 14:41:00.352494  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:41:34.936461  8649 solver.cpp:218] Iteration 11400 (2.13179 iter/s, 93.8177s/200 iters), loss = 0.347317
I1128 14:41:34.936606  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.251175 (* 1 = 0.251175 loss)
I1128 14:41:34.936619  8649 sgd_solver.cpp:105] Iteration 11400, lr = 0.1
I1128 14:43:08.738344  8649 solver.cpp:218] Iteration 11600 (2.13203 iter/s, 93.8074s/200 iters), loss = 0.285721
I1128 14:43:08.738423  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.123241 (* 1 = 0.123241 loss)
I1128 14:43:08.738435  8649 sgd_solver.cpp:105] Iteration 11600, lr = 0.1
I1128 14:44:03.501104  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:44:42.546576  8649 solver.cpp:218] Iteration 11800 (2.13188 iter/s, 93.8139s/200 iters), loss = 0.343424
I1128 14:44:42.546650  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.4823 (* 1 = 0.4823 loss)
I1128 14:44:42.546659  8649 sgd_solver.cpp:105] Iteration 11800, lr = 0.1
I1128 14:46:15.872485  8649 solver.cpp:330] Iteration 12000, Testing net (#0)
I1128 14:46:28.339716  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:46:28.391988  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7881
I1128 14:46:28.392053  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.674365 (* 1 = 0.674365 loss)
I1128 14:46:28.855542  8649 solver.cpp:218] Iteration 12000 (1.8812 iter/s, 106.315s/200 iters), loss = 0.251693
I1128 14:46:28.855587  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.316189 (* 1 = 0.316189 loss)
I1128 14:46:28.855597  8649 sgd_solver.cpp:105] Iteration 12000, lr = 0.1
I1128 14:47:19.286581  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:48:02.663661  8649 solver.cpp:218] Iteration 12200 (2.13193 iter/s, 93.8116s/200 iters), loss = 0.290061
I1128 14:48:02.663784  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.270447 (* 1 = 0.270447 loss)
I1128 14:48:02.663795  8649 sgd_solver.cpp:105] Iteration 12200, lr = 0.1
I1128 14:49:36.483896  8649 solver.cpp:218] Iteration 12400 (2.13168 iter/s, 93.8226s/200 iters), loss = 0.275957
I1128 14:49:36.484019  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.389797 (* 1 = 0.389797 loss)
I1128 14:49:36.484028  8649 sgd_solver.cpp:105] Iteration 12400, lr = 0.1
I1128 14:50:22.454437  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:50:22.923019  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_12500.caffemodel
I1128 14:50:22.935346  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_12500.solverstate
I1128 14:51:10.304826  8649 solver.cpp:218] Iteration 12600 (2.13166 iter/s, 93.8238s/200 iters), loss = 0.164838
I1128 14:51:10.304975  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.106768 (* 1 = 0.106768 loss)
I1128 14:51:10.304986  8649 sgd_solver.cpp:105] Iteration 12600, lr = 0.1
I1128 14:52:44.138530  8649 solver.cpp:218] Iteration 12800 (2.13136 iter/s, 93.8369s/200 iters), loss = 0.276412
I1128 14:52:44.138640  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.340628 (* 1 = 0.340628 loss)
I1128 14:52:44.138654  8649 sgd_solver.cpp:105] Iteration 12800, lr = 0.1
I1128 14:53:25.795280  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:54:17.541254  8649 solver.cpp:330] Iteration 13000, Testing net (#0)
I1128 14:54:30.077615  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:54:30.130141  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.822201
I1128 14:54:30.130169  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.550559 (* 1 = 0.550559 loss)
I1128 14:54:30.595643  8649 solver.cpp:218] Iteration 13000 (1.87862 iter/s, 106.461s/200 iters), loss = 0.24726
I1128 14:54:30.595697  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.401515 (* 1 = 0.401515 loss)
I1128 14:54:30.595710  8649 sgd_solver.cpp:105] Iteration 13000, lr = 0.1
I1128 14:56:04.473981  8649 solver.cpp:218] Iteration 13200 (2.13033 iter/s, 93.8823s/200 iters), loss = 0.332581
I1128 14:56:04.474145  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.244922 (* 1 = 0.244922 loss)
I1128 14:56:04.474164  8649 sgd_solver.cpp:105] Iteration 13200, lr = 0.1
I1128 14:56:41.674361  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 14:57:38.351181  8649 solver.cpp:218] Iteration 13400 (2.13035 iter/s, 93.8812s/200 iters), loss = 0.310097
I1128 14:57:38.351326  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.25812 (* 1 = 0.25812 loss)
I1128 14:57:38.351339  8649 sgd_solver.cpp:105] Iteration 13400, lr = 0.1
I1128 14:59:12.223821  8649 solver.cpp:218] Iteration 13600 (2.13045 iter/s, 93.8769s/200 iters), loss = 0.285046
I1128 14:59:12.223929  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.512448 (* 1 = 0.512448 loss)
I1128 14:59:12.223944  8649 sgd_solver.cpp:105] Iteration 13600, lr = 0.1
I1128 14:59:45.078275  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:00:46.080873  8649 solver.cpp:218] Iteration 13800 (2.1308 iter/s, 93.8615s/200 iters), loss = 0.384889
I1128 15:00:46.080946  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.531708 (* 1 = 0.531708 loss)
I1128 15:00:46.080956  8649 sgd_solver.cpp:105] Iteration 13800, lr = 0.1
I1128 15:02:19.476605  8649 solver.cpp:330] Iteration 14000, Testing net (#0)
I1128 15:02:31.958878  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:02:32.010947  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7401
I1128 15:02:32.010984  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.917827 (* 1 = 0.917827 loss)
I1128 15:02:32.475646  8649 solver.cpp:218] Iteration 14000 (1.8797 iter/s, 106.4s/200 iters), loss = 0.26089
I1128 15:02:32.475693  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.205635 (* 1 = 0.205635 loss)
I1128 15:02:32.475703  8649 sgd_solver.cpp:105] Iteration 14000, lr = 0.1
I1128 15:03:00.867925  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:04:06.302642  8649 solver.cpp:218] Iteration 14200 (2.13147 iter/s, 93.8317s/200 iters), loss = 0.303356
I1128 15:04:06.302793  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.23694 (* 1 = 0.23694 loss)
I1128 15:04:06.302804  8649 sgd_solver.cpp:105] Iteration 14200, lr = 0.1
I1128 15:05:40.114555  8649 solver.cpp:218] Iteration 14400 (2.13182 iter/s, 93.8167s/200 iters), loss = 0.259684
I1128 15:05:40.114648  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.240846 (* 1 = 0.240846 loss)
I1128 15:05:40.114658  8649 sgd_solver.cpp:105] Iteration 14400, lr = 0.1
I1128 15:06:04.170209  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:07:13.930217  8649 solver.cpp:218] Iteration 14600 (2.13173 iter/s, 93.8205s/200 iters), loss = 0.240647
I1128 15:07:13.930342  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0763738 (* 1 = 0.0763738 loss)
I1128 15:07:13.930353  8649 sgd_solver.cpp:105] Iteration 14600, lr = 0.1
I1128 15:08:47.754771  8649 solver.cpp:218] Iteration 14800 (2.13153 iter/s, 93.8294s/200 iters), loss = 0.269031
I1128 15:08:47.754869  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.243194 (* 1 = 0.243194 loss)
I1128 15:08:47.754882  8649 sgd_solver.cpp:105] Iteration 14800, lr = 0.1
I1128 15:09:07.346686  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:10:21.106889  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_15000.caffemodel
I1128 15:10:21.119530  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_15000.solverstate
I1128 15:10:21.123857  8649 solver.cpp:330] Iteration 15000, Testing net (#0)
I1128 15:10:33.590762  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:10:33.643282  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8289
I1128 15:10:33.643311  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.539704 (* 1 = 0.539704 loss)
I1128 15:10:34.108191  8649 solver.cpp:218] Iteration 15000 (1.88042 iter/s, 106.359s/200 iters), loss = 0.230827
I1128 15:10:34.108245  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.381791 (* 1 = 0.381791 loss)
I1128 15:10:34.108255  8649 sgd_solver.cpp:105] Iteration 15000, lr = 0.1
I1128 15:12:07.930969  8649 solver.cpp:218] Iteration 15200 (2.13156 iter/s, 93.8278s/200 iters), loss = 0.277179
I1128 15:12:07.931133  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.15318 (* 1 = 0.15318 loss)
I1128 15:12:07.931145  8649 sgd_solver.cpp:105] Iteration 15200, lr = 0.1
I1128 15:12:23.186364  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:13:41.743688  8649 solver.cpp:218] Iteration 15400 (2.13179 iter/s, 93.8177s/200 iters), loss = 0.277018
I1128 15:13:41.743777  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.214355 (* 1 = 0.214355 loss)
I1128 15:13:41.743787  8649 sgd_solver.cpp:105] Iteration 15400, lr = 0.1
I1128 15:15:15.553134  8649 solver.cpp:218] Iteration 15600 (2.13187 iter/s, 93.8145s/200 iters), loss = 0.30623
I1128 15:15:15.553227  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.340816 (* 1 = 0.340816 loss)
I1128 15:15:15.553236  8649 sgd_solver.cpp:105] Iteration 15600, lr = 0.1
I1128 15:15:26.348461  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:16:49.349691  8649 solver.cpp:218] Iteration 15800 (2.13216 iter/s, 93.8017s/200 iters), loss = 0.322478
I1128 15:16:49.349774  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.370771 (* 1 = 0.370771 loss)
I1128 15:16:49.349783  8649 sgd_solver.cpp:105] Iteration 15800, lr = 0.1
I1128 15:18:22.694988  8649 solver.cpp:330] Iteration 16000, Testing net (#0)
I1128 15:18:35.193898  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:18:35.246090  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.814301
I1128 15:18:35.246117  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.581793 (* 1 = 0.581793 loss)
I1128 15:18:35.711110  8649 solver.cpp:218] Iteration 16000 (1.88028 iter/s, 106.367s/200 iters), loss = 0.286487
I1128 15:18:35.711153  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.12761 (* 1 = 0.12761 loss)
I1128 15:18:35.711163  8649 sgd_solver.cpp:105] Iteration 16000, lr = 0.1
I1128 15:18:42.164021  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:20:09.512818  8649 solver.cpp:218] Iteration 16200 (2.13204 iter/s, 93.8069s/200 iters), loss = 0.193141
I1128 15:20:09.512969  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.190106 (* 1 = 0.190106 loss)
I1128 15:20:09.512980  8649 sgd_solver.cpp:105] Iteration 16200, lr = 0.1
I1128 15:21:43.308140  8649 solver.cpp:218] Iteration 16400 (2.13216 iter/s, 93.8015s/200 iters), loss = 0.371038
I1128 15:21:43.308214  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.407986 (* 1 = 0.407986 loss)
I1128 15:21:43.308243  8649 sgd_solver.cpp:105] Iteration 16400, lr = 0.1
I1128 15:21:45.306413  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:23:17.118441  8649 solver.cpp:218] Iteration 16600 (2.13177 iter/s, 93.8189s/200 iters), loss = 0.272561
I1128 15:23:17.118592  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.332306 (* 1 = 0.332306 loss)
I1128 15:23:17.118603  8649 sgd_solver.cpp:105] Iteration 16600, lr = 0.1
I1128 15:24:48.574682  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:24:50.916710  8649 solver.cpp:218] Iteration 16800 (2.13205 iter/s, 93.8063s/200 iters), loss = 0.195392
I1128 15:24:50.916749  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.233709 (* 1 = 0.233709 loss)
I1128 15:24:50.916757  8649 sgd_solver.cpp:105] Iteration 16800, lr = 0.1
I1128 15:26:24.269374  8649 solver.cpp:330] Iteration 17000, Testing net (#0)
I1128 15:26:36.762754  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:26:36.814965  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8174
I1128 15:26:36.814992  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.589193 (* 1 = 0.589193 loss)
I1128 15:26:37.279074  8649 solver.cpp:218] Iteration 17000 (1.88021 iter/s, 106.371s/200 iters), loss = 0.155242
I1128 15:26:37.279109  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.15379 (* 1 = 0.15379 loss)
I1128 15:26:37.279117  8649 sgd_solver.cpp:105] Iteration 17000, lr = 0.1
I1128 15:28:04.296993  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:28:11.097743  8649 solver.cpp:218] Iteration 17200 (2.1316 iter/s, 93.8261s/200 iters), loss = 0.302697
I1128 15:28:11.097790  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.239435 (* 1 = 0.239435 loss)
I1128 15:28:11.097802  8649 sgd_solver.cpp:105] Iteration 17200, lr = 0.1
I1128 15:29:44.918932  8649 solver.cpp:218] Iteration 17400 (2.13155 iter/s, 93.8283s/200 iters), loss = 0.189522
I1128 15:29:44.919031  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0606675 (* 1 = 0.0606675 loss)
I1128 15:29:44.919042  8649 sgd_solver.cpp:105] Iteration 17400, lr = 0.1
I1128 15:30:31.357851  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_17500.caffemodel
I1128 15:30:31.370501  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_17500.solverstate
I1128 15:31:07.604737  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:31:18.745071  8649 solver.cpp:218] Iteration 17600 (2.13145 iter/s, 93.833s/200 iters), loss = 0.201893
I1128 15:31:18.745118  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.186488 (* 1 = 0.186488 loss)
I1128 15:31:18.745128  8649 sgd_solver.cpp:105] Iteration 17600, lr = 0.1
I1128 15:32:52.841852  8649 solver.cpp:218] Iteration 17800 (2.12532 iter/s, 94.1035s/200 iters), loss = 0.269295
I1128 15:32:52.842010  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.284743 (* 1 = 0.284743 loss)
I1128 15:32:52.842020  8649 sgd_solver.cpp:105] Iteration 17800, lr = 0.1
I1128 15:34:11.444037  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:34:26.653831  8649 solver.cpp:330] Iteration 18000, Testing net (#0)
I1128 15:34:39.296331  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:34:39.349196  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.810501
I1128 15:34:39.349231  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.645276 (* 1 = 0.645276 loss)
I1128 15:34:39.815250  8649 solver.cpp:218] Iteration 18000 (1.86949 iter/s, 106.981s/200 iters), loss = 0.217435
I1128 15:34:39.815301  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0984896 (* 1 = 0.0984896 loss)
I1128 15:34:39.815310  8649 sgd_solver.cpp:105] Iteration 18000, lr = 0.1
I1128 15:36:14.102604  8649 solver.cpp:218] Iteration 18200 (2.12103 iter/s, 94.2938s/200 iters), loss = 0.249461
I1128 15:36:14.102707  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.113158 (* 1 = 0.113158 loss)
I1128 15:36:14.102717  8649 sgd_solver.cpp:105] Iteration 18200, lr = 0.1
I1128 15:37:28.365887  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:37:48.403697  8649 solver.cpp:218] Iteration 18400 (2.12072 iter/s, 94.3074s/200 iters), loss = 0.246836
I1128 15:37:48.403774  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.209879 (* 1 = 0.209879 loss)
I1128 15:37:48.403786  8649 sgd_solver.cpp:105] Iteration 18400, lr = 0.1
I1128 15:39:22.677286  8649 solver.cpp:218] Iteration 18600 (2.12134 iter/s, 94.2799s/200 iters), loss = 0.301649
I1128 15:39:22.677491  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.164057 (* 1 = 0.164057 loss)
I1128 15:39:22.677501  8649 sgd_solver.cpp:105] Iteration 18600, lr = 0.1
I1128 15:40:32.452361  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:40:56.959270  8649 solver.cpp:218] Iteration 18800 (2.12116 iter/s, 94.2881s/200 iters), loss = 0.232186
I1128 15:40:56.959319  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.27542 (* 1 = 0.27542 loss)
I1128 15:40:56.959328  8649 sgd_solver.cpp:105] Iteration 18800, lr = 0.1
I1128 15:42:30.619843  8649 solver.cpp:330] Iteration 19000, Testing net (#0)
I1128 15:42:43.140033  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:42:43.193979  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8297
I1128 15:42:43.194006  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.538623 (* 1 = 0.538623 loss)
I1128 15:42:43.658901  8649 solver.cpp:218] Iteration 19000 (1.8743 iter/s, 106.707s/200 iters), loss = 0.29346
I1128 15:42:43.658949  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0731733 (* 1 = 0.0731733 loss)
I1128 15:42:43.658959  8649 sgd_solver.cpp:105] Iteration 19000, lr = 0.1
I1128 15:43:48.853767  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:44:17.617794  8649 solver.cpp:218] Iteration 19200 (2.12845 iter/s, 93.965s/200 iters), loss = 0.388097
I1128 15:44:17.617836  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.329618 (* 1 = 0.329618 loss)
I1128 15:44:17.617846  8649 sgd_solver.cpp:105] Iteration 19200, lr = 0.1
I1128 15:45:51.562299  8649 solver.cpp:218] Iteration 19400 (2.12878 iter/s, 93.9506s/200 iters), loss = 0.259994
I1128 15:45:51.562400  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.250972 (* 1 = 0.250972 loss)
I1128 15:45:51.562410  8649 sgd_solver.cpp:105] Iteration 19400, lr = 0.1
I1128 15:46:52.265782  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:47:25.496656  8649 solver.cpp:218] Iteration 19600 (2.12901 iter/s, 93.9403s/200 iters), loss = 0.176366
I1128 15:47:25.496726  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.214128 (* 1 = 0.214128 loss)
I1128 15:47:25.496736  8649 sgd_solver.cpp:105] Iteration 19600, lr = 0.1
I1128 15:48:59.416631  8649 solver.cpp:218] Iteration 19800 (2.12934 iter/s, 93.926s/200 iters), loss = 0.254759
I1128 15:48:59.416790  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210175 (* 1 = 0.210175 loss)
I1128 15:48:59.416805  8649 sgd_solver.cpp:105] Iteration 19800, lr = 0.1
I1128 15:49:55.795828  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:50:32.879590  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_20000.caffemodel
I1128 15:50:32.892066  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_20000.solverstate
I1128 15:50:32.896965  8649 solver.cpp:330] Iteration 20000, Testing net (#0)
I1128 15:50:45.417393  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:50:45.470202  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7582
I1128 15:50:45.470234  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.858497 (* 1 = 0.858497 loss)
I1128 15:50:45.935452  8649 solver.cpp:218] Iteration 20000 (1.87748 iter/s, 106.525s/200 iters), loss = 0.281967
I1128 15:50:45.935503  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.104856 (* 1 = 0.104856 loss)
I1128 15:50:45.935513  8649 sgd_solver.cpp:105] Iteration 20000, lr = 0.1
I1128 15:52:19.884732  8649 solver.cpp:218] Iteration 20200 (2.12867 iter/s, 93.9552s/200 iters), loss = 0.263413
I1128 15:52:19.884963  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.396244 (* 1 = 0.396244 loss)
I1128 15:52:19.884980  8649 sgd_solver.cpp:105] Iteration 20200, lr = 0.1
I1128 15:53:11.793527  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:53:53.848753  8649 solver.cpp:218] Iteration 20400 (2.12834 iter/s, 93.9698s/200 iters), loss = 0.228507
I1128 15:53:53.848860  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.334828 (* 1 = 0.334828 loss)
I1128 15:53:53.848870  8649 sgd_solver.cpp:105] Iteration 20400, lr = 0.1
I1128 15:55:27.793679  8649 solver.cpp:218] Iteration 20600 (2.12878 iter/s, 93.9507s/200 iters), loss = 0.305279
I1128 15:55:27.793752  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.237326 (* 1 = 0.237326 loss)
I1128 15:55:27.793762  8649 sgd_solver.cpp:105] Iteration 20600, lr = 0.1
I1128 15:56:15.348462  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:57:01.719884  8649 solver.cpp:218] Iteration 20800 (2.12924 iter/s, 93.9301s/200 iters), loss = 0.136676
I1128 15:57:01.719949  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.156594 (* 1 = 0.156594 loss)
I1128 15:57:01.719959  8649 sgd_solver.cpp:105] Iteration 20800, lr = 0.1
I1128 15:58:35.187801  8649 solver.cpp:330] Iteration 21000, Testing net (#0)
I1128 15:58:47.771751  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 15:58:47.823807  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.826301
I1128 15:58:47.823832  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.560646 (* 1 = 0.560646 loss)
I1128 15:58:48.289137  8649 solver.cpp:218] Iteration 21000 (1.87663 iter/s, 106.574s/200 iters), loss = 0.316941
I1128 15:58:48.289242  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.243566 (* 1 = 0.243566 loss)
I1128 15:58:48.289261  8649 sgd_solver.cpp:105] Iteration 21000, lr = 0.1
I1128 15:59:31.411790  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:00:22.247062  8649 solver.cpp:218] Iteration 21200 (2.12851 iter/s, 93.9624s/200 iters), loss = 0.261113
I1128 16:00:22.247196  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.210189 (* 1 = 0.210189 loss)
I1128 16:00:22.247212  8649 sgd_solver.cpp:105] Iteration 21200, lr = 0.1
I1128 16:01:56.199012  8649 solver.cpp:218] Iteration 21400 (2.12864 iter/s, 93.9566s/200 iters), loss = 0.269968
I1128 16:01:56.199131  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.173642 (* 1 = 0.173642 loss)
I1128 16:01:56.199147  8649 sgd_solver.cpp:105] Iteration 21400, lr = 0.1
I1128 16:02:34.935390  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:03:30.154481  8649 solver.cpp:218] Iteration 21600 (2.12856 iter/s, 93.9602s/200 iters), loss = 0.361003
I1128 16:03:30.154610  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.451089 (* 1 = 0.451089 loss)
I1128 16:03:30.154620  8649 sgd_solver.cpp:105] Iteration 21600, lr = 0.1
I1128 16:05:04.085253  8649 solver.cpp:218] Iteration 21800 (2.12912 iter/s, 93.9356s/200 iters), loss = 0.189176
I1128 16:05:04.085494  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.192072 (* 1 = 0.192072 loss)
I1128 16:05:04.085518  8649 sgd_solver.cpp:105] Iteration 21800, lr = 0.1
I1128 16:05:38.391072  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:06:37.550938  8649 solver.cpp:330] Iteration 22000, Testing net (#0)
I1128 16:06:50.078330  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:06:50.131520  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.7828
I1128 16:06:50.131546  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.713465 (* 1 = 0.713465 loss)
I1128 16:06:50.596076  8649 solver.cpp:218] Iteration 22000 (1.87764 iter/s, 106.516s/200 iters), loss = 0.29907
I1128 16:06:50.596127  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.277685 (* 1 = 0.277685 loss)
I1128 16:06:50.596138  8649 sgd_solver.cpp:105] Iteration 22000, lr = 0.1
I1128 16:08:24.551105  8649 solver.cpp:218] Iteration 22200 (2.12856 iter/s, 93.9602s/200 iters), loss = 0.255407
I1128 16:08:24.551287  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.367269 (* 1 = 0.367269 loss)
I1128 16:08:24.551306  8649 sgd_solver.cpp:105] Iteration 22200, lr = 0.1
I1128 16:08:54.493273  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:09:58.507048  8649 solver.cpp:218] Iteration 22400 (2.12854 iter/s, 93.9611s/200 iters), loss = 0.301596
I1128 16:09:58.507141  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.34256 (* 1 = 0.34256 loss)
I1128 16:09:58.507151  8649 sgd_solver.cpp:105] Iteration 22400, lr = 0.1
I1128 16:10:44.986807  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_22500.caffemodel
I1128 16:10:44.999032  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_22500.solverstate
I1128 16:11:32.448276  8649 solver.cpp:218] Iteration 22600 (2.12887 iter/s, 93.9465s/200 iters), loss = 0.177711
I1128 16:11:32.448375  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.125669 (* 1 = 0.125669 loss)
I1128 16:11:32.448385  8649 sgd_solver.cpp:105] Iteration 22600, lr = 0.1
I1128 16:11:57.939882  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:13:06.354995  8649 solver.cpp:218] Iteration 22800 (2.12965 iter/s, 93.912s/200 iters), loss = 0.337738
I1128 16:13:06.355134  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.235962 (* 1 = 0.235962 loss)
I1128 16:13:06.355146  8649 sgd_solver.cpp:105] Iteration 22800, lr = 0.1
I1128 16:14:39.779275  8649 solver.cpp:330] Iteration 23000, Testing net (#0)
I1128 16:14:52.280050  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:14:52.334257  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.826801
I1128 16:14:52.334321  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.555503 (* 1 = 0.555503 loss)
I1128 16:14:52.798928  8649 solver.cpp:218] Iteration 23000 (1.87882 iter/s, 106.45s/200 iters), loss = 0.190779
I1128 16:14:52.798984  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.388358 (* 1 = 0.388358 loss)
I1128 16:14:52.798995  8649 sgd_solver.cpp:105] Iteration 23000, lr = 0.1
I1128 16:15:13.930838  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:16:26.679227  8649 solver.cpp:218] Iteration 23200 (2.13025 iter/s, 93.8857s/200 iters), loss = 0.255669
I1128 16:16:26.679332  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.131683 (* 1 = 0.131683 loss)
I1128 16:16:26.679342  8649 sgd_solver.cpp:105] Iteration 23200, lr = 0.1
I1128 16:18:00.570637  8649 solver.cpp:218] Iteration 23400 (2.13 iter/s, 93.8968s/200 iters), loss = 0.191527
I1128 16:18:00.570716  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.402229 (* 1 = 0.402229 loss)
I1128 16:18:00.570725  8649 sgd_solver.cpp:105] Iteration 23400, lr = 0.1
I1128 16:18:17.241998  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:19:34.462505  8649 solver.cpp:218] Iteration 23600 (2.12999 iter/s, 93.8973s/200 iters), loss = 0.163546
I1128 16:19:34.462647  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.110394 (* 1 = 0.110394 loss)
I1128 16:19:34.462659  8649 sgd_solver.cpp:105] Iteration 23600, lr = 0.1
I1128 16:21:08.347209  8649 solver.cpp:218] Iteration 23800 (2.13015 iter/s, 93.8901s/200 iters), loss = 0.375718
I1128 16:21:08.347302  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.495826 (* 1 = 0.495826 loss)
I1128 16:21:08.347313  8649 sgd_solver.cpp:105] Iteration 23800, lr = 0.1
I1128 16:21:20.672938  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:22:41.932968  8649 solver.cpp:330] Iteration 24000, Testing net (#0)
I1128 16:22:54.587234  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:22:54.640403  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.829701
I1128 16:22:54.640435  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.540045 (* 1 = 0.540045 loss)
I1128 16:22:55.106405  8649 solver.cpp:218] Iteration 24000 (1.87327 iter/s, 106.765s/200 iters), loss = 0.316487
I1128 16:22:55.106528  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0820089 (* 1 = 0.0820089 loss)
I1128 16:22:55.106554  8649 sgd_solver.cpp:105] Iteration 24000, lr = 0.1
I1128 16:24:29.334146  8649 solver.cpp:218] Iteration 24200 (2.12239 iter/s, 94.2333s/200 iters), loss = 0.250762
I1128 16:24:29.334255  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.39976 (* 1 = 0.39976 loss)
I1128 16:24:29.334270  8649 sgd_solver.cpp:105] Iteration 24200, lr = 0.1
I1128 16:24:37.230587  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:26:03.554669  8649 solver.cpp:218] Iteration 24400 (2.12256 iter/s, 94.2261s/200 iters), loss = 0.201328
I1128 16:26:03.554833  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.199585 (* 1 = 0.199585 loss)
I1128 16:26:03.554843  8649 sgd_solver.cpp:105] Iteration 24400, lr = 0.1
I1128 16:27:37.740718  8649 solver.cpp:218] Iteration 24600 (2.12333 iter/s, 94.1915s/200 iters), loss = 0.124636
I1128 16:27:37.740816  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0497777 (* 1 = 0.0497777 loss)
I1128 16:27:37.740826  8649 sgd_solver.cpp:105] Iteration 24600, lr = 0.1
I1128 16:27:41.277128  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:29:11.954727  8649 solver.cpp:218] Iteration 24800 (2.1227 iter/s, 94.2196s/200 iters), loss = 0.154364
I1128 16:29:11.954864  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.119328 (* 1 = 0.119328 loss)
I1128 16:29:11.954879  8649 sgd_solver.cpp:105] Iteration 24800, lr = 0.1
I1128 16:30:45.194761  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:30:45.667340  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_25000.caffemodel
I1128 16:30:45.681141  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_25000.solverstate
I1128 16:30:45.686224  8649 solver.cpp:330] Iteration 25000, Testing net (#0)
I1128 16:30:58.296955  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:30:58.350136  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.823801
I1128 16:30:58.350196  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.571639 (* 1 = 0.571639 loss)
I1128 16:30:58.816479  8649 solver.cpp:218] Iteration 25000 (1.87151 iter/s, 106.866s/200 iters), loss = 0.170586
I1128 16:30:58.816529  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.188183 (* 1 = 0.188183 loss)
I1128 16:30:58.816539  8649 sgd_solver.cpp:105] Iteration 25000, lr = 0.1
I1128 16:32:32.883512  8649 solver.cpp:218] Iteration 25200 (2.12606 iter/s, 94.0705s/200 iters), loss = 0.228168
I1128 16:32:32.883591  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.227587 (* 1 = 0.227587 loss)
I1128 16:32:32.883601  8649 sgd_solver.cpp:105] Iteration 25200, lr = 0.1
I1128 16:34:01.421105  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:34:06.697829  8649 solver.cpp:218] Iteration 25400 (2.13179 iter/s, 93.8181s/200 iters), loss = 0.224985
I1128 16:34:06.697868  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.105483 (* 1 = 0.105483 loss)
I1128 16:34:06.697877  8649 sgd_solver.cpp:105] Iteration 25400, lr = 0.1
I1128 16:35:40.502578  8649 solver.cpp:218] Iteration 25600 (2.132 iter/s, 93.8088s/200 iters), loss = 0.331306
I1128 16:35:40.502707  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.222192 (* 1 = 0.222192 loss)
I1128 16:35:40.502722  8649 sgd_solver.cpp:105] Iteration 25600, lr = 0.1
I1128 16:37:04.581058  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:37:14.311882  8649 solver.cpp:218] Iteration 25800 (2.13189 iter/s, 93.8134s/200 iters), loss = 0.180416
I1128 16:37:14.311919  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.413142 (* 1 = 0.413142 loss)
I1128 16:37:14.311928  8649 sgd_solver.cpp:105] Iteration 25800, lr = 0.1
I1128 16:38:47.655524  8649 solver.cpp:330] Iteration 26000, Testing net (#0)
I1128 16:39:00.133744  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:39:00.185371  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.673001
I1128 16:39:00.185401  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 1.44625 (* 1 = 1.44625 loss)
I1128 16:39:00.649847  8649 solver.cpp:218] Iteration 26000 (1.88071 iter/s, 106.343s/200 iters), loss = 0.204536
I1128 16:39:00.649906  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.150835 (* 1 = 0.150835 loss)
I1128 16:39:00.649921  8649 sgd_solver.cpp:105] Iteration 26000, lr = 0.1
I1128 16:40:20.384438  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:40:34.455503  8649 solver.cpp:218] Iteration 26200 (2.13196 iter/s, 93.8102s/200 iters), loss = 0.264921
I1128 16:40:34.455559  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.19765 (* 1 = 0.19765 loss)
I1128 16:40:34.455574  8649 sgd_solver.cpp:105] Iteration 26200, lr = 0.1
I1128 16:42:08.277755  8649 solver.cpp:218] Iteration 26400 (2.13159 iter/s, 93.8269s/200 iters), loss = 0.214436
I1128 16:42:08.277844  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.262689 (* 1 = 0.262689 loss)
I1128 16:42:08.277853  8649 sgd_solver.cpp:105] Iteration 26400, lr = 0.1
I1128 16:43:23.646944  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:43:42.191808  8649 solver.cpp:218] Iteration 26600 (2.1295 iter/s, 93.9187s/200 iters), loss = 0.124706
I1128 16:43:42.191859  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0918079 (* 1 = 0.0918079 loss)
I1128 16:43:42.191869  8649 sgd_solver.cpp:105] Iteration 26600, lr = 0.1
I1128 16:45:16.081202  8649 solver.cpp:218] Iteration 26800 (2.13006 iter/s, 93.8942s/200 iters), loss = 0.208607
I1128 16:45:16.081296  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.394986 (* 1 = 0.394986 loss)
I1128 16:45:16.081307  8649 sgd_solver.cpp:105] Iteration 26800, lr = 0.1
I1128 16:46:27.087944  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:46:49.509465  8649 solver.cpp:330] Iteration 27000, Testing net (#0)
I1128 16:47:02.031015  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:47:02.083461  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.842201
I1128 16:47:02.083497  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.497126 (* 1 = 0.497126 loss)
I1128 16:47:02.548419  8649 solver.cpp:218] Iteration 27000 (1.87842 iter/s, 106.473s/200 iters), loss = 0.319203
I1128 16:47:02.548471  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.249243 (* 1 = 0.249243 loss)
I1128 16:47:02.548482  8649 sgd_solver.cpp:105] Iteration 27000, lr = 0.1
I1128 16:48:36.448869  8649 solver.cpp:218] Iteration 27200 (2.1298 iter/s, 93.9054s/200 iters), loss = 0.248195
I1128 16:48:36.448941  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.321895 (* 1 = 0.321895 loss)
I1128 16:48:36.448951  8649 sgd_solver.cpp:105] Iteration 27200, lr = 0.1
I1128 16:49:43.022163  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:50:10.375597  8649 solver.cpp:218] Iteration 27400 (2.12921 iter/s, 93.9317s/200 iters), loss = 0.198508
I1128 16:50:10.375659  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0872629 (* 1 = 0.0872629 loss)
I1128 16:50:10.375669  8649 sgd_solver.cpp:105] Iteration 27400, lr = 0.1
I1128 16:50:56.868517  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_27500.caffemodel
I1128 16:50:56.881142  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_27500.solverstate
I1128 16:51:44.312623  8649 solver.cpp:218] Iteration 27600 (2.12897 iter/s, 93.942s/200 iters), loss = 0.264203
I1128 16:51:44.312754  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.216286 (* 1 = 0.216286 loss)
I1128 16:51:44.312764  8649 sgd_solver.cpp:105] Iteration 27600, lr = 0.1
I1128 16:52:46.620852  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:53:18.385182  8649 solver.cpp:218] Iteration 27800 (2.12591 iter/s, 94.0775s/200 iters), loss = 0.177504
I1128 16:53:18.385316  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0687554 (* 1 = 0.0687554 loss)
I1128 16:53:18.385329  8649 sgd_solver.cpp:105] Iteration 27800, lr = 0.1
I1128 16:54:52.067049  8649 solver.cpp:330] Iteration 28000, Testing net (#0)
I1128 16:55:04.656872  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:55:04.711148  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8247
I1128 16:55:04.711200  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.578685 (* 1 = 0.578685 loss)
I1128 16:55:05.179649  8649 solver.cpp:218] Iteration 28000 (1.87266 iter/s, 106.8s/200 iters), loss = 0.234499
I1128 16:55:05.179699  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.246036 (* 1 = 0.246036 loss)
I1128 16:55:05.179710  8649 sgd_solver.cpp:105] Iteration 28000, lr = 0.1
I1128 16:56:03.057961  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:56:39.280182  8649 solver.cpp:218] Iteration 28200 (2.12527 iter/s, 94.1056s/200 iters), loss = 0.123662
I1128 16:56:39.280351  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0499883 (* 1 = 0.0499883 loss)
I1128 16:56:39.280381  8649 sgd_solver.cpp:105] Iteration 28200, lr = 0.1
I1128 16:58:13.383642  8649 solver.cpp:218] Iteration 28400 (2.12521 iter/s, 94.1085s/200 iters), loss = 0.188579
I1128 16:58:13.383764  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.263061 (* 1 = 0.263061 loss)
I1128 16:58:13.383774  8649 sgd_solver.cpp:105] Iteration 28400, lr = 0.1
I1128 16:59:06.907995  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 16:59:47.498167  8649 solver.cpp:218] Iteration 28600 (2.12496 iter/s, 94.1196s/200 iters), loss = 0.147069
I1128 16:59:47.498342  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0849194 (* 1 = 0.0849194 loss)
I1128 16:59:47.498391  8649 sgd_solver.cpp:105] Iteration 28600, lr = 0.1
I1128 17:01:21.611874  8649 solver.cpp:218] Iteration 28800 (2.12497 iter/s, 94.1188s/200 iters), loss = 0.359925
I1128 17:01:21.611971  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.352806 (* 1 = 0.352806 loss)
I1128 17:01:21.611987  8649 sgd_solver.cpp:105] Iteration 28800, lr = 0.1
I1128 17:02:10.641881  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:02:55.127007  8649 solver.cpp:330] Iteration 29000, Testing net (#0)
I1128 17:03:07.643787  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:03:07.696440  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8032
I1128 17:03:07.696473  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.652312 (* 1 = 0.652312 loss)
I1128 17:03:08.161406  8649 solver.cpp:218] Iteration 29000 (1.87696 iter/s, 106.555s/200 iters), loss = 0.242691
I1128 17:03:08.161463  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.107969 (* 1 = 0.107969 loss)
I1128 17:03:08.161473  8649 sgd_solver.cpp:105] Iteration 29000, lr = 0.1
I1128 17:04:42.068339  8649 solver.cpp:218] Iteration 29200 (2.12964 iter/s, 93.9126s/200 iters), loss = 0.230187
I1128 17:04:42.068436  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.145489 (* 1 = 0.145489 loss)
I1128 17:04:42.068449  8649 sgd_solver.cpp:105] Iteration 29200, lr = 0.1
I1128 17:05:26.684617  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:06:16.092191  8649 solver.cpp:218] Iteration 29400 (2.12699 iter/s, 94.0296s/200 iters), loss = 0.22785
I1128 17:06:16.092444  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.195749 (* 1 = 0.195749 loss)
I1128 17:06:16.092465  8649 sgd_solver.cpp:105] Iteration 29400, lr = 0.1
I1128 17:07:50.115185  8649 solver.cpp:218] Iteration 29600 (2.12701 iter/s, 94.0286s/200 iters), loss = 0.223476
I1128 17:07:50.115270  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.166136 (* 1 = 0.166136 loss)
I1128 17:07:50.115284  8649 sgd_solver.cpp:105] Iteration 29600, lr = 0.1
I1128 17:08:30.280532  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:09:24.052103  8649 solver.cpp:218] Iteration 29800 (2.12896 iter/s, 93.9425s/200 iters), loss = 0.163517
I1128 17:09:24.052217  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.138928 (* 1 = 0.138928 loss)
I1128 17:09:24.052249  8649 sgd_solver.cpp:105] Iteration 29800, lr = 0.1
I1128 17:10:57.506341  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_30000.caffemodel
I1128 17:10:57.518784  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_30000.solverstate
I1128 17:10:57.523252  8649 solver.cpp:330] Iteration 30000, Testing net (#0)
I1128 17:11:10.052820  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:11:10.105731  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.8317
I1128 17:11:10.105760  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.554333 (* 1 = 0.554333 loss)
I1128 17:11:10.570682  8649 solver.cpp:218] Iteration 30000 (1.8775 iter/s, 106.525s/200 iters), loss = 0.165538
I1128 17:11:10.570732  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0750534 (* 1 = 0.0750534 loss)
I1128 17:11:10.570744  8649 sgd_solver.cpp:105] Iteration 30000, lr = 0.1
I1128 17:11:46.382562  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:12:44.594446  8649 solver.cpp:218] Iteration 30200 (2.127 iter/s, 94.0293s/200 iters), loss = 0.180356
I1128 17:12:44.594547  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.36922 (* 1 = 0.36922 loss)
I1128 17:12:44.594564  8649 sgd_solver.cpp:105] Iteration 30200, lr = 0.1
I1128 17:14:18.699517  8649 solver.cpp:218] Iteration 30400 (2.12516 iter/s, 94.1105s/200 iters), loss = 0.162826
I1128 17:14:18.699712  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.177715 (* 1 = 0.177715 loss)
I1128 17:14:18.699734  8649 sgd_solver.cpp:105] Iteration 30400, lr = 0.1
I1128 17:14:50.123299  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:15:52.840900  8649 solver.cpp:218] Iteration 30600 (2.12434 iter/s, 94.1467s/200 iters), loss = 0.209925
I1128 17:15:52.841006  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.186916 (* 1 = 0.186916 loss)
I1128 17:15:52.841020  8649 sgd_solver.cpp:105] Iteration 30600, lr = 0.1
I1128 17:17:26.908512  8649 solver.cpp:218] Iteration 30800 (2.12601 iter/s, 94.073s/200 iters), loss = 0.155374
I1128 17:17:26.908605  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.267624 (* 1 = 0.267624 loss)
I1128 17:17:26.908622  8649 sgd_solver.cpp:105] Iteration 30800, lr = 0.1
I1128 17:17:53.959753  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:19:00.519708  8649 solver.cpp:330] Iteration 31000, Testing net (#0)
I1128 17:19:13.108639  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:19:13.160853  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.833001
I1128 17:19:13.160886  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.534202 (* 1 = 0.534202 loss)
I1128 17:19:13.626422  8649 solver.cpp:218] Iteration 31000 (1.87399 iter/s, 106.724s/200 iters), loss = 0.182922
I1128 17:19:13.626484  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.100452 (* 1 = 0.100452 loss)
I1128 17:19:13.626500  8649 sgd_solver.cpp:105] Iteration 31000, lr = 0.1
I1128 17:20:47.717190  8649 solver.cpp:218] Iteration 31200 (2.12549 iter/s, 94.0961s/200 iters), loss = 0.184655
I1128 17:20:47.717401  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.165892 (* 1 = 0.165892 loss)
I1128 17:20:47.717420  8649 sgd_solver.cpp:105] Iteration 31200, lr = 0.1
I1128 17:21:10.319303  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:22:21.762187  8649 solver.cpp:218] Iteration 31400 (2.12652 iter/s, 94.0502s/200 iters), loss = 0.200112
I1128 17:22:21.762284  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0716643 (* 1 = 0.0716643 loss)
I1128 17:22:21.762293  8649 sgd_solver.cpp:105] Iteration 31400, lr = 0.1
I1128 17:23:55.589563  8649 solver.cpp:218] Iteration 31600 (2.13145 iter/s, 93.8327s/200 iters), loss = 0.192185
I1128 17:23:55.589705  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.117053 (* 1 = 0.117053 loss)
I1128 17:23:55.589715  8649 sgd_solver.cpp:105] Iteration 31600, lr = 0.1
I1128 17:24:13.775543  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:25:29.410169  8649 solver.cpp:218] Iteration 31800 (2.13161 iter/s, 93.8259s/200 iters), loss = 0.233272
I1128 17:25:29.410327  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.461399 (* 1 = 0.461399 loss)
I1128 17:25:29.410339  8649 sgd_solver.cpp:105] Iteration 31800, lr = 0.1
I1128 17:27:02.769609  8649 solver.cpp:330] Iteration 32000, Testing net (#0)
I1128 17:27:15.282712  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:27:15.334873  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.807101
I1128 17:27:15.334900  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.634627 (* 1 = 0.634627 loss)
I1128 17:27:15.800247  8649 solver.cpp:218] Iteration 32000 (1.87977 iter/s, 106.396s/200 iters), loss = 0.259198
I1128 17:27:15.800298  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.121609 (* 1 = 0.121609 loss)
I1128 17:27:15.800317  8649 sgd_solver.cpp:105] Iteration 32000, lr = 0.01
I1128 17:27:29.529953  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:28:49.627005  8649 solver.cpp:218] Iteration 32200 (2.13147 iter/s, 93.8321s/200 iters), loss = 0.157505
I1128 17:28:49.627143  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.203502 (* 1 = 0.203502 loss)
I1128 17:28:49.627156  8649 sgd_solver.cpp:105] Iteration 32200, lr = 0.01
I1128 17:30:23.452808  8649 solver.cpp:218] Iteration 32400 (2.13149 iter/s, 93.831s/200 iters), loss = 0.142995
I1128 17:30:23.452886  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.067187 (* 1 = 0.067187 loss)
I1128 17:30:23.452896  8649 sgd_solver.cpp:105] Iteration 32400, lr = 0.01
I1128 17:30:32.845472  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:31:09.905699  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_32500.caffemodel
I1128 17:31:09.918095  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_32500.solverstate
I1128 17:31:57.298493  8649 solver.cpp:218] Iteration 32600 (2.13104 iter/s, 93.851s/200 iters), loss = 0.0459755
I1128 17:31:57.298669  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.073861 (* 1 = 0.073861 loss)
I1128 17:31:57.298686  8649 sgd_solver.cpp:105] Iteration 32600, lr = 0.01
I1128 17:33:31.451725  8649 solver.cpp:218] Iteration 32800 (2.12408 iter/s, 94.1584s/200 iters), loss = 0.0751859
I1128 17:33:31.451967  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0424212 (* 1 = 0.0424212 loss)
I1128 17:33:31.451982  8649 sgd_solver.cpp:105] Iteration 32800, lr = 0.01
I1128 17:33:36.401612  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:35:05.132064  8649 solver.cpp:330] Iteration 33000, Testing net (#0)
I1128 17:35:17.749228  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:35:17.801596  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.907202
I1128 17:35:17.801666  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.302131 (* 1 = 0.302131 loss)
I1128 17:35:18.268424  8649 solver.cpp:218] Iteration 33000 (1.87226 iter/s, 106.823s/200 iters), loss = 0.0516228
I1128 17:35:18.268549  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0226747 (* 1 = 0.0226747 loss)
I1128 17:35:18.268579  8649 sgd_solver.cpp:105] Iteration 33000, lr = 0.01
I1128 17:36:52.408000  8649 solver.cpp:218] Iteration 33200 (2.12439 iter/s, 94.1448s/200 iters), loss = 0.0686262
I1128 17:36:52.408098  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0617901 (* 1 = 0.0617901 loss)
I1128 17:36:52.408113  8649 sgd_solver.cpp:105] Iteration 33200, lr = 0.01
I1128 17:36:53.003388  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:38:26.566855  8649 solver.cpp:218] Iteration 33400 (2.12398 iter/s, 94.163s/200 iters), loss = 0.206382
I1128 17:38:26.567019  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.262743 (* 1 = 0.262743 loss)
I1128 17:38:26.567030  8649 sgd_solver.cpp:105] Iteration 33400, lr = 0.01
I1128 17:39:56.840612  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:40:00.732051  8649 solver.cpp:218] Iteration 33600 (2.12387 iter/s, 94.1677s/200 iters), loss = 0.0730532
I1128 17:40:00.732105  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0196442 (* 1 = 0.0196442 loss)
I1128 17:40:00.732117  8649 sgd_solver.cpp:105] Iteration 33600, lr = 0.01
I1128 17:41:34.907456  8649 solver.cpp:218] Iteration 33800 (2.12363 iter/s, 94.1784s/200 iters), loss = 0.0596333
I1128 17:41:34.907585  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0521416 (* 1 = 0.0521416 loss)
I1128 17:41:34.907598  8649 sgd_solver.cpp:105] Iteration 33800, lr = 0.01
I1128 17:43:00.604717  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:43:08.351009  8649 solver.cpp:330] Iteration 34000, Testing net (#0)
I1128 17:43:20.827457  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:43:20.879828  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.905802
I1128 17:43:20.879855  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.314188 (* 1 = 0.314188 loss)
I1128 17:43:21.344219  8649 solver.cpp:218] Iteration 34000 (1.87898 iter/s, 106.44s/200 iters), loss = 0.0578549
I1128 17:43:21.344288  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.103305 (* 1 = 0.103305 loss)
I1128 17:43:21.344300  8649 sgd_solver.cpp:105] Iteration 34000, lr = 0.01
I1128 17:44:55.167588  8649 solver.cpp:218] Iteration 34200 (2.13158 iter/s, 93.8269s/200 iters), loss = 0.0723606
I1128 17:44:55.167685  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0396702 (* 1 = 0.0396702 loss)
I1128 17:44:55.167699  8649 sgd_solver.cpp:105] Iteration 34200, lr = 0.01
I1128 17:46:16.329628  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:46:28.996655  8649 solver.cpp:218] Iteration 34400 (2.13145 iter/s, 93.8328s/200 iters), loss = 0.0633501
I1128 17:46:28.996703  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0422397 (* 1 = 0.0422397 loss)
I1128 17:46:28.996714  8649 sgd_solver.cpp:105] Iteration 34400, lr = 0.01
I1128 17:48:02.815954  8649 solver.cpp:218] Iteration 34600 (2.13167 iter/s, 93.8233s/200 iters), loss = 0.0180058
I1128 17:48:02.816025  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0139511 (* 1 = 0.0139511 loss)
I1128 17:48:02.816037  8649 sgd_solver.cpp:105] Iteration 34600, lr = 0.01
I1128 17:49:19.631916  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:49:36.642896  8649 solver.cpp:218] Iteration 34800 (2.13149 iter/s, 93.831s/200 iters), loss = 0.0772199
I1128 17:49:36.642932  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.040831 (* 1 = 0.040831 loss)
I1128 17:49:36.642940  8649 sgd_solver.cpp:105] Iteration 34800, lr = 0.01
I1128 17:51:09.982054  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_35000.caffemodel
I1128 17:51:09.994570  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_35000.solverstate
I1128 17:51:09.999079  8649 solver.cpp:330] Iteration 35000, Testing net (#0)
I1128 17:51:22.485103  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:51:22.537199  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.907003
I1128 17:51:22.537226  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.33008 (* 1 = 0.33008 loss)
I1128 17:51:23.001631  8649 solver.cpp:218] Iteration 35000 (1.88034 iter/s, 106.364s/200 iters), loss = 0.0715385
I1128 17:51:23.001672  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.03737 (* 1 = 0.03737 loss)
I1128 17:51:23.001680  8649 sgd_solver.cpp:105] Iteration 35000, lr = 0.01
I1128 17:52:35.362026  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:52:56.826786  8649 solver.cpp:218] Iteration 35200 (2.13153 iter/s, 93.8295s/200 iters), loss = 0.062724
I1128 17:52:56.826825  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0682286 (* 1 = 0.0682286 loss)
I1128 17:52:56.826834  8649 sgd_solver.cpp:105] Iteration 35200, lr = 0.01
I1128 17:54:30.651212  8649 solver.cpp:218] Iteration 35400 (2.13154 iter/s, 93.8288s/200 iters), loss = 0.025328
I1128 17:54:30.651284  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0443399 (* 1 = 0.0443399 loss)
I1128 17:54:30.651293  8649 sgd_solver.cpp:105] Iteration 35400, lr = 0.01
I1128 17:55:38.679930  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:56:04.488551  8649 solver.cpp:218] Iteration 35600 (2.13125 iter/s, 93.8418s/200 iters), loss = 0.0324315
I1128 17:56:04.488605  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00948158 (* 1 = 0.00948158 loss)
I1128 17:56:04.488616  8649 sgd_solver.cpp:105] Iteration 35600, lr = 0.01
I1128 17:57:38.304255  8649 solver.cpp:218] Iteration 35800 (2.13174 iter/s, 93.8202s/200 iters), loss = 0.0341824
I1128 17:57:38.304338  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00478807 (* 1 = 0.00478807 loss)
I1128 17:57:38.304350  8649 sgd_solver.cpp:105] Iteration 35800, lr = 0.01
I1128 17:58:41.865366  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:59:11.662725  8649 solver.cpp:330] Iteration 36000, Testing net (#0)
I1128 17:59:24.162865  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 17:59:24.215118  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.900302
I1128 17:59:24.215147  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.353099 (* 1 = 0.353099 loss)
I1128 17:59:24.682847  8649 solver.cpp:218] Iteration 36000 (1.87999 iter/s, 106.384s/200 iters), loss = 0.031067
I1128 17:59:24.682904  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0706965 (* 1 = 0.0706965 loss)
I1128 17:59:24.682914  8649 sgd_solver.cpp:105] Iteration 36000, lr = 0.01
I1128 18:00:58.492943  8649 solver.cpp:218] Iteration 36200 (2.13186 iter/s, 93.8147s/200 iters), loss = 0.0956115
I1128 18:00:58.493031  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0630144 (* 1 = 0.0630144 loss)
I1128 18:00:58.493042  8649 sgd_solver.cpp:105] Iteration 36200, lr = 0.01
I1128 18:01:57.731446  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:02:32.462615  8649 solver.cpp:218] Iteration 36400 (2.12824 iter/s, 93.9743s/200 iters), loss = 0.0698787
I1128 18:02:32.462716  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0375677 (* 1 = 0.0375677 loss)
I1128 18:02:32.462726  8649 sgd_solver.cpp:105] Iteration 36400, lr = 0.01
I1128 18:04:06.620153  8649 solver.cpp:218] Iteration 36600 (2.124 iter/s, 94.1622s/200 iters), loss = 0.0205637
I1128 18:04:06.620288  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00785753 (* 1 = 0.00785753 loss)
I1128 18:04:06.620302  8649 sgd_solver.cpp:105] Iteration 36600, lr = 0.01
I1128 18:05:01.615165  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:05:40.812388  8649 solver.cpp:218] Iteration 36800 (2.12321 iter/s, 94.1969s/200 iters), loss = 0.0318806
I1128 18:05:40.812532  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0368525 (* 1 = 0.0368525 loss)
I1128 18:05:40.812542  8649 sgd_solver.cpp:105] Iteration 36800, lr = 0.01
I1128 18:07:14.497714  8649 solver.cpp:330] Iteration 37000, Testing net (#0)
I1128 18:07:27.156370  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:07:27.209096  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.909302
I1128 18:07:27.209131  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.336735 (* 1 = 0.336735 loss)
I1128 18:07:27.674171  8649 solver.cpp:218] Iteration 37000 (1.87148 iter/s, 106.867s/200 iters), loss = 0.116036
I1128 18:07:27.674247  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0430175 (* 1 = 0.0430175 loss)
I1128 18:07:27.674263  8649 sgd_solver.cpp:105] Iteration 37000, lr = 0.01
I1128 18:08:18.293603  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:09:01.837896  8649 solver.cpp:218] Iteration 37200 (2.12385 iter/s, 94.1684s/200 iters), loss = 0.0242936
I1128 18:09:01.838003  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0349591 (* 1 = 0.0349591 loss)
I1128 18:09:01.838021  8649 sgd_solver.cpp:105] Iteration 37200, lr = 0.01
I1128 18:10:35.988901  8649 solver.cpp:218] Iteration 37400 (2.12414 iter/s, 94.1557s/200 iters), loss = 0.0517553
I1128 18:10:35.989054  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0876919 (* 1 = 0.0876919 loss)
I1128 18:10:35.989070  8649 sgd_solver.cpp:105] Iteration 37400, lr = 0.01
I1128 18:11:22.131285  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:11:22.602532  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_37500.caffemodel
I1128 18:11:22.616013  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_37500.solverstate
I1128 18:12:10.138039  8649 solver.cpp:218] Iteration 37600 (2.12418 iter/s, 94.1538s/200 iters), loss = 0.0255996
I1128 18:12:10.138111  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0178862 (* 1 = 0.0178862 loss)
I1128 18:12:10.138121  8649 sgd_solver.cpp:105] Iteration 37600, lr = 0.01
I1128 18:13:44.031921  8649 solver.cpp:218] Iteration 37800 (2.12996 iter/s, 93.8986s/200 iters), loss = 0.0534606
I1128 18:13:44.031998  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0230523 (* 1 = 0.0230523 loss)
I1128 18:13:44.032008  8649 sgd_solver.cpp:105] Iteration 37800, lr = 0.01
I1128 18:14:25.699043  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:15:17.461068  8649 solver.cpp:330] Iteration 38000, Testing net (#0)
I1128 18:15:29.978415  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:15:30.031819  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.908802
I1128 18:15:30.031848  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.344589 (* 1 = 0.344589 loss)
I1128 18:15:30.497189  8649 solver.cpp:218] Iteration 38000 (1.87845 iter/s, 106.471s/200 iters), loss = 0.0485251
I1128 18:15:30.497238  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0885979 (* 1 = 0.0885979 loss)
I1128 18:15:30.497247  8649 sgd_solver.cpp:105] Iteration 38000, lr = 0.01
I1128 18:17:04.404989  8649 solver.cpp:218] Iteration 38200 (2.12964 iter/s, 93.9126s/200 iters), loss = 0.0155706
I1128 18:17:04.405133  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00387134 (* 1 = 0.00387134 loss)
I1128 18:17:04.405143  8649 sgd_solver.cpp:105] Iteration 38200, lr = 0.01
I1128 18:17:41.773185  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:18:38.596297  8649 solver.cpp:218] Iteration 38400 (2.12323 iter/s, 94.196s/200 iters), loss = 0.0420079
I1128 18:18:38.596406  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0194491 (* 1 = 0.0194491 loss)
I1128 18:18:38.596422  8649 sgd_solver.cpp:105] Iteration 38400, lr = 0.01
I1128 18:20:12.519937  8649 solver.cpp:218] Iteration 38600 (2.12928 iter/s, 93.9284s/200 iters), loss = 0.0443584
I1128 18:20:12.520033  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0377033 (* 1 = 0.0377033 loss)
I1128 18:20:12.520043  8649 sgd_solver.cpp:105] Iteration 38600, lr = 0.01
I1128 18:20:45.389554  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:21:46.417260  8649 solver.cpp:218] Iteration 38800 (2.12988 iter/s, 93.9021s/200 iters), loss = 0.0179895
I1128 18:21:46.417371  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0130133 (* 1 = 0.0130133 loss)
I1128 18:21:46.417384  8649 sgd_solver.cpp:105] Iteration 38800, lr = 0.01
I1128 18:23:20.261950  8649 solver.cpp:330] Iteration 39000, Testing net (#0)
I1128 18:23:33.001701  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:23:33.054831  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.907002
I1128 18:23:33.054868  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.362709 (* 1 = 0.362709 loss)
I1128 18:23:33.520253  8649 solver.cpp:218] Iteration 39000 (1.86727 iter/s, 107.108s/200 iters), loss = 0.0845331
I1128 18:23:33.520318  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.192592 (* 1 = 0.192592 loss)
I1128 18:23:33.520334  8649 sgd_solver.cpp:105] Iteration 39000, lr = 0.01
I1128 18:24:02.071679  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:25:07.895361  8649 solver.cpp:218] Iteration 39200 (2.1191 iter/s, 94.3799s/200 iters), loss = 0.022295
I1128 18:25:07.895480  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0234817 (* 1 = 0.0234817 loss)
I1128 18:25:07.895498  8649 sgd_solver.cpp:105] Iteration 39200, lr = 0.01
I1128 18:26:42.263516  8649 solver.cpp:218] Iteration 39400 (2.11925 iter/s, 94.3729s/200 iters), loss = 0.0275339
I1128 18:26:42.263631  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0197442 (* 1 = 0.0197442 loss)
I1128 18:26:42.263659  8649 sgd_solver.cpp:105] Iteration 39400, lr = 0.01
I1128 18:27:06.443789  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:28:16.716775  8649 solver.cpp:218] Iteration 39600 (2.11734 iter/s, 94.458s/200 iters), loss = 0.0409073
I1128 18:28:16.717232  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00347853 (* 1 = 0.00347853 loss)
I1128 18:28:16.717252  8649 sgd_solver.cpp:105] Iteration 39600, lr = 0.01
I1128 18:29:51.210194  8649 solver.cpp:218] Iteration 39800 (2.11645 iter/s, 94.4978s/200 iters), loss = 0.035794
I1128 18:29:51.210310  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0122962 (* 1 = 0.0122962 loss)
I1128 18:29:51.210328  8649 sgd_solver.cpp:105] Iteration 39800, lr = 0.01
I1128 18:30:10.924893  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:31:25.134781  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_40000.caffemodel
I1128 18:31:25.149526  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_40000.solverstate
I1128 18:31:25.156409  8649 solver.cpp:330] Iteration 40000, Testing net (#0)
I1128 18:31:37.893664  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:31:37.947047  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.909002
I1128 18:31:37.947096  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.358132 (* 1 = 0.358132 loss)
I1128 18:31:38.413766  8649 solver.cpp:218] Iteration 40000 (1.86552 iter/s, 107.209s/200 iters), loss = 0.0383762
I1128 18:31:38.413899  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.034602 (* 1 = 0.034602 loss)
I1128 18:31:38.413928  8649 sgd_solver.cpp:105] Iteration 40000, lr = 0.01
I1128 18:33:12.774313  8649 solver.cpp:218] Iteration 40200 (2.11942 iter/s, 94.3653s/200 iters), loss = 0.0543856
I1128 18:33:12.774461  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0155241 (* 1 = 0.0155241 loss)
I1128 18:33:12.774480  8649 sgd_solver.cpp:105] Iteration 40200, lr = 0.01
I1128 18:33:28.122714  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:34:47.127255  8649 solver.cpp:218] Iteration 40400 (2.11959 iter/s, 94.3577s/200 iters), loss = 0.0385469
I1128 18:34:47.127408  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0897779 (* 1 = 0.0897779 loss)
I1128 18:34:47.127427  8649 sgd_solver.cpp:105] Iteration 40400, lr = 0.01
I1128 18:36:21.498471  8649 solver.cpp:218] Iteration 40600 (2.11918 iter/s, 94.3759s/200 iters), loss = 0.0817439
I1128 18:36:21.498672  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.120459 (* 1 = 0.120459 loss)
I1128 18:36:21.498698  8649 sgd_solver.cpp:105] Iteration 40600, lr = 0.01
I1128 18:36:32.363732  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:37:55.876061  8649 solver.cpp:218] Iteration 40800 (2.11904 iter/s, 94.3823s/200 iters), loss = 0.0615345
I1128 18:37:55.876174  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0163727 (* 1 = 0.0163727 loss)
I1128 18:37:55.876188  8649 sgd_solver.cpp:105] Iteration 40800, lr = 0.01
I1128 18:39:29.775516  8649 solver.cpp:330] Iteration 41000, Testing net (#0)
I1128 18:39:42.503922  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:39:42.556447  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.904802
I1128 18:39:42.556494  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.385555 (* 1 = 0.385555 loss)
I1128 18:39:43.022487  8649 solver.cpp:218] Iteration 41000 (1.86651 iter/s, 107.152s/200 iters), loss = 0.0507953
I1128 18:39:43.022605  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0416423 (* 1 = 0.0416423 loss)
I1128 18:39:43.022626  8649 sgd_solver.cpp:105] Iteration 41000, lr = 0.01
I1128 18:39:49.520830  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:41:17.401880  8649 solver.cpp:218] Iteration 41200 (2.119 iter/s, 94.3842s/200 iters), loss = 0.0103219
I1128 18:41:17.402034  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0135611 (* 1 = 0.0135611 loss)
I1128 18:41:17.402051  8649 sgd_solver.cpp:105] Iteration 41200, lr = 0.01
I1128 18:42:51.454375  8649 solver.cpp:218] Iteration 41400 (2.12637 iter/s, 94.0572s/200 iters), loss = 0.0175826
I1128 18:42:51.454486  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00638076 (* 1 = 0.00638076 loss)
I1128 18:42:51.454499  8649 sgd_solver.cpp:105] Iteration 41400, lr = 0.01
I1128 18:42:53.455787  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:44:25.284024  8649 solver.cpp:218] Iteration 41600 (2.13141 iter/s, 93.8344s/200 iters), loss = 0.0518878
I1128 18:44:25.284173  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0556112 (* 1 = 0.0556112 loss)
I1128 18:44:25.284188  8649 sgd_solver.cpp:105] Iteration 41600, lr = 0.01
I1128 18:45:56.752984  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:45:59.099530  8649 solver.cpp:218] Iteration 41800 (2.13174 iter/s, 93.8202s/200 iters), loss = 0.0458328
I1128 18:45:59.099591  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0365973 (* 1 = 0.0365973 loss)
I1128 18:45:59.099601  8649 sgd_solver.cpp:105] Iteration 41800, lr = 0.01
I1128 18:47:32.439863  8649 solver.cpp:330] Iteration 42000, Testing net (#0)
I1128 18:47:44.936800  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:47:44.988970  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.909702
I1128 18:47:44.989001  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.374273 (* 1 = 0.374273 loss)
I1128 18:47:45.453223  8649 solver.cpp:218] Iteration 42000 (1.88019 iter/s, 106.372s/200 iters), loss = 0.0421049
I1128 18:47:45.453270  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00973991 (* 1 = 0.00973991 loss)
I1128 18:47:45.453280  8649 sgd_solver.cpp:105] Iteration 42000, lr = 0.01
I1128 18:49:12.470679  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:49:19.273810  8649 solver.cpp:218] Iteration 42200 (2.13138 iter/s, 93.836s/200 iters), loss = 0.048875
I1128 18:49:19.273847  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0238511 (* 1 = 0.0238511 loss)
I1128 18:49:19.273856  8649 sgd_solver.cpp:105] Iteration 42200, lr = 0.01
I1128 18:50:53.080588  8649 solver.cpp:218] Iteration 42400 (2.13173 iter/s, 93.8207s/200 iters), loss = 0.00766124
I1128 18:50:53.080672  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00519855 (* 1 = 0.00519855 loss)
I1128 18:50:53.080682  8649 sgd_solver.cpp:105] Iteration 42400, lr = 0.01
I1128 18:51:39.533577  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_42500.caffemodel
I1128 18:51:39.546012  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_42500.solverstate
I1128 18:52:15.800889  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:52:26.940145  8649 solver.cpp:218] Iteration 42600 (2.13056 iter/s, 93.8722s/200 iters), loss = 0.0264465
I1128 18:52:26.940197  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.022237 (* 1 = 0.022237 loss)
I1128 18:52:26.940208  8649 sgd_solver.cpp:105] Iteration 42600, lr = 0.01
I1128 18:54:00.737505  8649 solver.cpp:218] Iteration 42800 (2.13199 iter/s, 93.809s/200 iters), loss = 0.0262735
I1128 18:54:00.737578  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0258744 (* 1 = 0.0258744 loss)
I1128 18:54:00.737589  8649 sgd_solver.cpp:105] Iteration 42800, lr = 0.01
I1128 18:55:18.952010  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:55:34.086212  8649 solver.cpp:330] Iteration 43000, Testing net (#0)
I1128 18:55:46.593127  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:55:46.645849  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.909002
I1128 18:55:46.645879  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.374259 (* 1 = 0.374259 loss)
I1128 18:55:47.110630  8649 solver.cpp:218] Iteration 43000 (1.87996 iter/s, 106.385s/200 iters), loss = 0.00831111
I1128 18:55:47.110669  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0049931 (* 1 = 0.0049931 loss)
I1128 18:55:47.110679  8649 sgd_solver.cpp:105] Iteration 43000, lr = 0.01
I1128 18:57:20.922158  8649 solver.cpp:218] Iteration 43200 (2.13171 iter/s, 93.8215s/200 iters), loss = 0.0097258
I1128 18:57:20.922262  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0071566 (* 1 = 0.0071566 loss)
I1128 18:57:20.922276  8649 sgd_solver.cpp:105] Iteration 43200, lr = 0.01
I1128 18:58:34.806820  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 18:58:54.738971  8649 solver.cpp:218] Iteration 43400 (2.1316 iter/s, 93.8262s/200 iters), loss = 0.00951129
I1128 18:58:54.739025  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0134358 (* 1 = 0.0134358 loss)
I1128 18:58:54.739040  8649 sgd_solver.cpp:105] Iteration 43400, lr = 0.01
I1128 19:00:28.552734  8649 solver.cpp:218] Iteration 43600 (2.13168 iter/s, 93.8227s/200 iters), loss = 0.0442401
I1128 19:00:28.552827  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00211744 (* 1 = 0.00211744 loss)
I1128 19:00:28.552839  8649 sgd_solver.cpp:105] Iteration 43600, lr = 0.01
I1128 19:01:37.980888  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:02:02.372249  8649 solver.cpp:218] Iteration 43800 (2.13156 iter/s, 93.8281s/200 iters), loss = 0.0119496
I1128 19:02:02.372288  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00782078 (* 1 = 0.00782078 loss)
I1128 19:02:02.372297  8649 sgd_solver.cpp:105] Iteration 43800, lr = 0.01
I1128 19:03:35.732623  8649 solver.cpp:330] Iteration 44000, Testing net (#0)
I1128 19:03:48.223822  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:03:48.276078  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.907002
I1128 19:03:48.276113  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.385496 (* 1 = 0.385496 loss)
I1128 19:03:48.740525  8649 solver.cpp:218] Iteration 44000 (1.88009 iter/s, 106.378s/200 iters), loss = 0.0223653
I1128 19:03:48.740571  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0184965 (* 1 = 0.0184965 loss)
I1128 19:03:48.740581  8649 sgd_solver.cpp:105] Iteration 44000, lr = 0.01
I1128 19:04:53.819870  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:05:22.558746  8649 solver.cpp:218] Iteration 44200 (2.1316 iter/s, 93.8262s/200 iters), loss = 0.0301135
I1128 19:05:22.558796  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00310997 (* 1 = 0.00310997 loss)
I1128 19:05:22.558807  8649 sgd_solver.cpp:105] Iteration 44200, lr = 0.01
I1128 19:06:56.389125  8649 solver.cpp:218] Iteration 44400 (2.13133 iter/s, 93.8381s/200 iters), loss = 0.0390399
I1128 19:06:56.389271  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0027563 (* 1 = 0.0027563 loss)
I1128 19:06:56.389286  8649 sgd_solver.cpp:105] Iteration 44400, lr = 0.01
I1128 19:07:57.031361  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:08:30.224584  8649 solver.cpp:218] Iteration 44600 (2.13122 iter/s, 93.8429s/200 iters), loss = 0.0311941
I1128 19:08:30.224673  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0342992 (* 1 = 0.0342992 loss)
I1128 19:08:30.224690  8649 sgd_solver.cpp:105] Iteration 44600, lr = 0.01
I1128 19:10:04.072419  8649 solver.cpp:218] Iteration 44800 (2.13094 iter/s, 93.8552s/200 iters), loss = 0.0198889
I1128 19:10:04.072557  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0622803 (* 1 = 0.0622803 loss)
I1128 19:10:04.072569  8649 sgd_solver.cpp:105] Iteration 44800, lr = 0.01
I1128 19:11:00.385305  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:11:37.460412  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_45000.caffemodel
I1128 19:11:37.473166  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_45000.solverstate
I1128 19:11:37.477947  8649 solver.cpp:330] Iteration 45000, Testing net (#0)
I1128 19:11:49.968956  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:11:50.021721  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.910902
I1128 19:11:50.021751  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.389369 (* 1 = 0.389369 loss)
I1128 19:11:50.486284  8649 solver.cpp:218] Iteration 45000 (1.87931 iter/s, 106.422s/200 iters), loss = 0.0198659
I1128 19:11:50.486327  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00210968 (* 1 = 0.00210968 loss)
I1128 19:11:50.486338  8649 sgd_solver.cpp:105] Iteration 45000, lr = 0.01
I1128 19:13:24.298667  8649 solver.cpp:218] Iteration 45200 (2.13175 iter/s, 93.8195s/200 iters), loss = 0.0299053
I1128 19:13:24.298811  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00718591 (* 1 = 0.00718591 loss)
I1128 19:13:24.298822  8649 sgd_solver.cpp:105] Iteration 45200, lr = 0.01
I1128 19:14:16.140352  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:14:58.119905  8649 solver.cpp:218] Iteration 45400 (2.13156 iter/s, 93.8282s/200 iters), loss = 0.00961213
I1128 19:14:58.119984  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0210007 (* 1 = 0.0210007 loss)
I1128 19:14:58.119994  8649 sgd_solver.cpp:105] Iteration 45400, lr = 0.01
I1128 19:16:31.937093  8649 solver.cpp:218] Iteration 45600 (2.13165 iter/s, 93.8241s/200 iters), loss = 0.0289117
I1128 19:16:31.937188  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0286784 (* 1 = 0.0286784 loss)
I1128 19:16:31.937203  8649 sgd_solver.cpp:105] Iteration 45600, lr = 0.01
I1128 19:17:19.435901  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:18:05.761759  8649 solver.cpp:218] Iteration 45800 (2.13148 iter/s, 93.8315s/200 iters), loss = 0.0207659
I1128 19:18:05.761833  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.030551 (* 1 = 0.030551 loss)
I1128 19:18:05.761844  8649 sgd_solver.cpp:105] Iteration 45800, lr = 0.01
I1128 19:19:39.123473  8649 solver.cpp:330] Iteration 46000, Testing net (#0)
I1128 19:19:51.636293  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:19:51.687914  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.911302
I1128 19:19:51.687944  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.3905 (* 1 = 0.3905 loss)
I1128 19:19:52.153028  8649 solver.cpp:218] Iteration 46000 (1.87972 iter/s, 106.399s/200 iters), loss = 0.0373304
I1128 19:19:52.153081  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0681574 (* 1 = 0.0681574 loss)
I1128 19:19:52.153095  8649 sgd_solver.cpp:105] Iteration 46000, lr = 0.01
I1128 19:20:35.193212  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:21:25.965472  8649 solver.cpp:218] Iteration 46200 (2.13181 iter/s, 93.817s/200 iters), loss = 0.0374924
I1128 19:21:25.965575  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00961068 (* 1 = 0.00961068 loss)
I1128 19:21:25.965591  8649 sgd_solver.cpp:105] Iteration 46200, lr = 0.01
I1128 19:22:59.855504  8649 solver.cpp:218] Iteration 46400 (2.13006 iter/s, 93.8942s/200 iters), loss = 0.0219475
I1128 19:22:59.855609  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0598575 (* 1 = 0.0598575 loss)
I1128 19:22:59.855619  8649 sgd_solver.cpp:105] Iteration 46400, lr = 0.01
I1128 19:23:38.619186  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:24:33.799834  8649 solver.cpp:218] Iteration 46600 (2.12882 iter/s, 93.9489s/200 iters), loss = 0.0261329
I1128 19:24:33.799964  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0112417 (* 1 = 0.0112417 loss)
I1128 19:24:33.799978  8649 sgd_solver.cpp:105] Iteration 46600, lr = 0.01
I1128 19:26:07.746431  8649 solver.cpp:218] Iteration 46800 (2.12876 iter/s, 93.9514s/200 iters), loss = 0.0200715
I1128 19:26:07.746523  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00720035 (* 1 = 0.00720035 loss)
I1128 19:26:07.746538  8649 sgd_solver.cpp:105] Iteration 46800, lr = 0.01
I1128 19:26:42.018983  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:27:41.209486  8649 solver.cpp:330] Iteration 47000, Testing net (#0)
I1128 19:27:53.766984  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:27:53.819046  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.909002
I1128 19:27:53.819072  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.39706 (* 1 = 0.39706 loss)
I1128 19:27:54.284665  8649 solver.cpp:218] Iteration 47000 (1.87716 iter/s, 106.544s/200 iters), loss = 0.0160083
I1128 19:27:54.284711  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00369436 (* 1 = 0.00369436 loss)
I1128 19:27:54.284719  8649 sgd_solver.cpp:105] Iteration 47000, lr = 0.01
I1128 19:29:28.204140  8649 solver.cpp:218] Iteration 47200 (2.12936 iter/s, 93.9248s/200 iters), loss = 0.0216657
I1128 19:29:28.204308  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0299118 (* 1 = 0.0299118 loss)
I1128 19:29:28.204318  8649 sgd_solver.cpp:105] Iteration 47200, lr = 0.01
I1128 19:29:58.170999  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:31:02.156205  8649 solver.cpp:218] Iteration 47400 (2.12862 iter/s, 93.9574s/200 iters), loss = 0.0321293
I1128 19:31:02.156340  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0592674 (* 1 = 0.0592674 loss)
I1128 19:31:02.156352  8649 sgd_solver.cpp:105] Iteration 47400, lr = 0.01
I1128 19:31:48.684263  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_47500.caffemodel
I1128 19:31:48.697157  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_47500.solverstate
I1128 19:32:36.086518  8649 solver.cpp:218] Iteration 47600 (2.12911 iter/s, 93.9359s/200 iters), loss = 0.0132225
I1128 19:32:36.086670  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00217477 (* 1 = 0.00217477 loss)
I1128 19:32:36.086686  8649 sgd_solver.cpp:105] Iteration 47600, lr = 0.01
I1128 19:33:01.543920  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:34:09.901581  8649 solver.cpp:218] Iteration 47800 (2.13173 iter/s, 93.8207s/200 iters), loss = 0.0281476
I1128 19:34:09.901679  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0446026 (* 1 = 0.0446026 loss)
I1128 19:34:09.901696  8649 sgd_solver.cpp:105] Iteration 47800, lr = 0.01
I1128 19:35:43.490092  8649 solver.cpp:330] Iteration 48000, Testing net (#0)
I1128 19:35:55.984486  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:35:56.036314  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.908302
I1128 19:35:56.036341  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.41254 (* 1 = 0.41254 loss)
I1128 19:35:56.503100  8649 solver.cpp:218] Iteration 48000 (1.87603 iter/s, 106.608s/200 iters), loss = 0.0331929
I1128 19:35:56.503146  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.037549 (* 1 = 0.037549 loss)
I1128 19:35:56.503160  8649 sgd_solver.cpp:105] Iteration 48000, lr = 0.001
I1128 19:36:17.620486  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:37:30.326498  8649 solver.cpp:218] Iteration 48200 (2.13153 iter/s, 93.8293s/200 iters), loss = 0.0140339
I1128 19:37:30.326580  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00526913 (* 1 = 0.00526913 loss)
I1128 19:37:30.326589  8649 sgd_solver.cpp:105] Iteration 48200, lr = 0.001
I1128 19:39:04.140202  8649 solver.cpp:218] Iteration 48400 (2.13175 iter/s, 93.8197s/200 iters), loss = 0.0367058
I1128 19:39:04.140367  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0588276 (* 1 = 0.0588276 loss)
I1128 19:39:04.140383  8649 sgd_solver.cpp:105] Iteration 48400, lr = 0.001
I1128 19:39:20.801075  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:40:37.971094  8649 solver.cpp:218] Iteration 48600 (2.13136 iter/s, 93.8368s/200 iters), loss = 0.00153607
I1128 19:40:37.971179  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00124693 (* 1 = 0.00124693 loss)
I1128 19:40:37.971189  8649 sgd_solver.cpp:105] Iteration 48600, lr = 0.001
I1128 19:42:11.782351  8649 solver.cpp:218] Iteration 48800 (2.1318 iter/s, 93.8173s/200 iters), loss = 0.0218648
I1128 19:42:11.782487  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00704871 (* 1 = 0.00704871 loss)
I1128 19:42:11.782497  8649 sgd_solver.cpp:105] Iteration 48800, lr = 0.001
I1128 19:42:24.107803  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:43:45.145956  8649 solver.cpp:330] Iteration 49000, Testing net (#0)
I1128 19:43:57.644558  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:43:57.697046  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.913901
I1128 19:43:57.697073  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.38683 (* 1 = 0.38683 loss)
I1128 19:43:58.161557  8649 solver.cpp:218] Iteration 49000 (1.87995 iter/s, 106.386s/200 iters), loss = 0.00802332
I1128 19:43:58.161592  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0150403 (* 1 = 0.0150403 loss)
I1128 19:43:58.161600  8649 sgd_solver.cpp:105] Iteration 49000, lr = 0.001
I1128 19:45:31.981277  8649 solver.cpp:218] Iteration 49200 (2.13161 iter/s, 93.8259s/200 iters), loss = 0.0224758
I1128 19:45:31.981364  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00804061 (* 1 = 0.00804061 loss)
I1128 19:45:31.981374  8649 sgd_solver.cpp:105] Iteration 49200, lr = 0.001
I1128 19:45:39.844519  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:47:05.803196  8649 solver.cpp:218] Iteration 49400 (2.13156 iter/s, 93.828s/200 iters), loss = 0.0306699
I1128 19:47:05.803360  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00185109 (* 1 = 0.00185109 loss)
I1128 19:47:05.803377  8649 sgd_solver.cpp:105] Iteration 49400, lr = 0.001
I1128 19:48:39.627571  8649 solver.cpp:218] Iteration 49600 (2.1315 iter/s, 93.8305s/200 iters), loss = 0.0121086
I1128 19:48:39.627642  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00453558 (* 1 = 0.00453558 loss)
I1128 19:48:39.627655  8649 sgd_solver.cpp:105] Iteration 49600, lr = 0.001
I1128 19:48:43.156654  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:50:13.610972  8649 solver.cpp:218] Iteration 49800 (2.12789 iter/s, 93.9896s/200 iters), loss = 0.00477493
I1128 19:50:13.611047  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00134769 (* 1 = 0.00134769 loss)
I1128 19:50:13.611059  8649 sgd_solver.cpp:105] Iteration 49800, lr = 0.001
I1128 19:51:46.490623  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:51:46.962182  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_50000.caffemodel
I1128 19:51:46.974572  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_50000.solverstate
I1128 19:51:46.978955  8649 solver.cpp:330] Iteration 50000, Testing net (#0)
I1128 19:51:59.483717  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:51:59.536337  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.913502
I1128 19:51:59.536365  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.389993 (* 1 = 0.389993 loss)
I1128 19:52:00.001291  8649 solver.cpp:218] Iteration 50000 (1.87975 iter/s, 106.397s/200 iters), loss = 0.00835729
I1128 19:52:00.001332  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00232727 (* 1 = 0.00232727 loss)
I1128 19:52:00.001341  8649 sgd_solver.cpp:105] Iteration 50000, lr = 0.001
I1128 19:53:33.813890  8649 solver.cpp:218] Iteration 50200 (2.13177 iter/s, 93.8188s/200 iters), loss = 0.00872569
I1128 19:53:33.813951  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00726774 (* 1 = 0.00726774 loss)
I1128 19:53:33.813961  8649 sgd_solver.cpp:105] Iteration 50200, lr = 0.001
I1128 19:55:02.349548  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:55:07.629122  8649 solver.cpp:218] Iteration 50400 (2.13172 iter/s, 93.8209s/200 iters), loss = 0.0590667
I1128 19:55:07.629179  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.114151 (* 1 = 0.114151 loss)
I1128 19:55:07.629194  8649 sgd_solver.cpp:105] Iteration 50400, lr = 0.001
I1128 19:56:41.437697  8649 solver.cpp:218] Iteration 50600 (2.13188 iter/s, 93.8138s/200 iters), loss = 0.00349539
I1128 19:56:41.437821  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000752352 (* 1 = 0.000752352 loss)
I1128 19:56:41.437839  8649 sgd_solver.cpp:105] Iteration 50600, lr = 0.001
I1128 19:58:05.524055  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 19:58:15.257087  8649 solver.cpp:218] Iteration 50800 (2.13164 iter/s, 93.8246s/200 iters), loss = 0.00442499
I1128 19:58:15.257138  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0114292 (* 1 = 0.0114292 loss)
I1128 19:58:15.257151  8649 sgd_solver.cpp:105] Iteration 50800, lr = 0.001
I1128 19:59:48.595461  8649 solver.cpp:330] Iteration 51000, Testing net (#0)
I1128 20:00:01.085775  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:00:01.137907  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.913302
I1128 20:00:01.137934  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.388683 (* 1 = 0.388683 loss)
I1128 20:00:01.602339  8649 solver.cpp:218] Iteration 51000 (1.88056 iter/s, 106.351s/200 iters), loss = 0.00525603
I1128 20:00:01.602377  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00240013 (* 1 = 0.00240013 loss)
I1128 20:00:01.602386  8649 sgd_solver.cpp:105] Iteration 51000, lr = 0.001
I1128 20:01:21.356859  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:01:35.429401  8649 solver.cpp:218] Iteration 51200 (2.13145 iter/s, 93.8326s/200 iters), loss = 0.0149829
I1128 20:01:35.429455  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0100731 (* 1 = 0.0100731 loss)
I1128 20:01:35.429466  8649 sgd_solver.cpp:105] Iteration 51200, lr = 0.001
I1128 20:03:09.238837  8649 solver.cpp:218] Iteration 51400 (2.13185 iter/s, 93.8151s/200 iters), loss = 0.0291706
I1128 20:03:09.238929  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0919013 (* 1 = 0.0919013 loss)
I1128 20:03:09.238941  8649 sgd_solver.cpp:105] Iteration 51400, lr = 0.001
I1128 20:04:24.534878  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:04:43.068626  8649 solver.cpp:218] Iteration 51600 (2.13139 iter/s, 93.8355s/200 iters), loss = 0.0053727
I1128 20:04:43.068680  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00142811 (* 1 = 0.00142811 loss)
I1128 20:04:43.068691  8649 sgd_solver.cpp:105] Iteration 51600, lr = 0.001
I1128 20:06:16.889974  8649 solver.cpp:218] Iteration 51800 (2.13158 iter/s, 93.8271s/200 iters), loss = 0.033587
I1128 20:06:16.890055  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0596574 (* 1 = 0.0596574 loss)
I1128 20:06:16.890066  8649 sgd_solver.cpp:105] Iteration 51800, lr = 0.001
I1128 20:07:27.837436  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:07:50.243904  8649 solver.cpp:330] Iteration 52000, Testing net (#0)
I1128 20:08:02.756158  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:08:02.808806  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.914302
I1128 20:08:02.808833  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.391346 (* 1 = 0.391346 loss)
I1128 20:08:03.274353  8649 solver.cpp:218] Iteration 52000 (1.87986 iter/s, 106.391s/200 iters), loss = 0.0155299
I1128 20:08:03.274391  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0119748 (* 1 = 0.0119748 loss)
I1128 20:08:03.274400  8649 sgd_solver.cpp:105] Iteration 52000, lr = 0.001
I1128 20:09:37.086923  8649 solver.cpp:218] Iteration 52200 (2.13178 iter/s, 93.8185s/200 iters), loss = 0.00439132
I1128 20:09:37.087000  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00240741 (* 1 = 0.00240741 loss)
I1128 20:09:37.087010  8649 sgd_solver.cpp:105] Iteration 52200, lr = 0.001
I1128 20:10:43.579041  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:11:10.905802  8649 solver.cpp:218] Iteration 52400 (2.13163 iter/s, 93.8248s/200 iters), loss = 0.0081352
I1128 20:11:10.905843  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000312989 (* 1 = 0.000312989 loss)
I1128 20:11:10.905853  8649 sgd_solver.cpp:105] Iteration 52400, lr = 0.001
I1128 20:11:57.366878  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_52500.caffemodel
I1128 20:11:57.379281  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_52500.solverstate
I1128 20:12:44.750299  8649 solver.cpp:218] Iteration 52600 (2.13105 iter/s, 93.8505s/200 iters), loss = 0.0192053
I1128 20:12:44.750535  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0118598 (* 1 = 0.0118598 loss)
I1128 20:12:44.750553  8649 sgd_solver.cpp:105] Iteration 52600, lr = 0.001
I1128 20:13:46.915361  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:14:18.814332  8649 solver.cpp:218] Iteration 52800 (2.12608 iter/s, 94.0699s/200 iters), loss = 0.0104875
I1128 20:14:18.814430  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0109844 (* 1 = 0.0109844 loss)
I1128 20:14:18.814442  8649 sgd_solver.cpp:105] Iteration 52800, lr = 0.001
I1128 20:15:52.173154  8649 solver.cpp:330] Iteration 53000, Testing net (#0)
I1128 20:16:04.680667  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:16:04.732558  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.914102
I1128 20:16:04.732584  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.389885 (* 1 = 0.389885 loss)
I1128 20:16:05.197449  8649 solver.cpp:218] Iteration 53000 (1.87988 iter/s, 106.39s/200 iters), loss = 0.0511199
I1128 20:16:05.197485  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000798986 (* 1 = 0.000798986 loss)
I1128 20:16:05.197494  8649 sgd_solver.cpp:105] Iteration 53000, lr = 0.001
I1128 20:17:02.919201  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:17:39.040658  8649 solver.cpp:218] Iteration 53200 (2.13108 iter/s, 93.8492s/200 iters), loss = 0.0247419
I1128 20:17:39.040765  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 6.7634e-05 (* 1 = 6.7634e-05 loss)
I1128 20:17:39.040776  8649 sgd_solver.cpp:105] Iteration 53200, lr = 0.001
I1128 20:19:12.877825  8649 solver.cpp:218] Iteration 53400 (2.13122 iter/s, 93.8431s/200 iters), loss = 0.00916003
I1128 20:19:12.877902  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0170305 (* 1 = 0.0170305 loss)
I1128 20:19:12.877916  8649 sgd_solver.cpp:105] Iteration 53400, lr = 0.001
I1128 20:20:06.242341  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:20:46.709301  8649 solver.cpp:218] Iteration 53600 (2.13134 iter/s, 93.8375s/200 iters), loss = 0.0235599
I1128 20:20:46.709408  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00460528 (* 1 = 0.00460528 loss)
I1128 20:20:46.709420  8649 sgd_solver.cpp:105] Iteration 53600, lr = 0.001
I1128 20:22:20.549290  8649 solver.cpp:218] Iteration 53800 (2.13115 iter/s, 93.846s/200 iters), loss = 0.0119847
I1128 20:22:20.549365  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00227213 (* 1 = 0.00227213 loss)
I1128 20:22:20.549381  8649 sgd_solver.cpp:105] Iteration 53800, lr = 0.001
I1128 20:23:09.470242  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:23:53.919308  8649 solver.cpp:330] Iteration 54000, Testing net (#0)
I1128 20:24:06.400871  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:24:06.452972  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.914402
I1128 20:24:06.452998  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.391058 (* 1 = 0.391058 loss)
I1128 20:24:06.916647  8649 solver.cpp:218] Iteration 54000 (1.88015 iter/s, 106.374s/200 iters), loss = 0.00657449
I1128 20:24:06.916688  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0206576 (* 1 = 0.0206576 loss)
I1128 20:24:06.916697  8649 sgd_solver.cpp:105] Iteration 54000, lr = 0.001
I1128 20:25:40.727339  8649 solver.cpp:218] Iteration 54200 (2.13182 iter/s, 93.8168s/200 iters), loss = 0.00214735
I1128 20:25:40.727496  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00239033 (* 1 = 0.00239033 loss)
I1128 20:25:40.727505  8649 sgd_solver.cpp:105] Iteration 54200, lr = 0.001
I1128 20:26:25.303792  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:27:14.562815  8649 solver.cpp:218] Iteration 54400 (2.13125 iter/s, 93.8414s/200 iters), loss = 0.0286335
I1128 20:27:14.562911  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0446511 (* 1 = 0.0446511 loss)
I1128 20:27:14.562922  8649 sgd_solver.cpp:105] Iteration 54400, lr = 0.001
I1128 20:28:48.393726  8649 solver.cpp:218] Iteration 54600 (2.13136 iter/s, 93.8366s/200 iters), loss = 0.00858521
I1128 20:28:48.393857  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00884469 (* 1 = 0.00884469 loss)
I1128 20:28:48.393869  8649 sgd_solver.cpp:105] Iteration 54600, lr = 0.001
I1128 20:29:28.506317  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:30:22.217600  8649 solver.cpp:218] Iteration 54800 (2.13156 iter/s, 93.8279s/200 iters), loss = 0.00141737
I1128 20:30:22.217674  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000378269 (* 1 = 0.000378269 loss)
I1128 20:30:22.217684  8649 sgd_solver.cpp:105] Iteration 54800, lr = 0.001
I1128 20:31:55.578088  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_55000.caffemodel
I1128 20:31:55.590845  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_55000.solverstate
I1128 20:31:55.595353  8649 solver.cpp:330] Iteration 55000, Testing net (#0)
I1128 20:32:08.204169  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:32:08.256956  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.914202
I1128 20:32:08.256990  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.39357 (* 1 = 0.39357 loss)
I1128 20:32:08.722736  8649 solver.cpp:218] Iteration 55000 (1.87776 iter/s, 106.51s/200 iters), loss = 0.00600767
I1128 20:32:08.722789  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00295206 (* 1 = 0.00295206 loss)
I1128 20:32:08.722798  8649 sgd_solver.cpp:105] Iteration 55000, lr = 0.001
I1128 20:32:44.618749  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:33:42.886139  8649 solver.cpp:218] Iteration 55200 (2.12386 iter/s, 94.1681s/200 iters), loss = 0.0165643
I1128 20:33:42.886276  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00156993 (* 1 = 0.00156993 loss)
I1128 20:33:42.886288  8649 sgd_solver.cpp:105] Iteration 55200, lr = 0.001
I1128 20:35:17.031947  8649 solver.cpp:218] Iteration 55400 (2.12426 iter/s, 94.1506s/200 iters), loss = 0.0338676
I1128 20:35:17.032049  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.115421 (* 1 = 0.115421 loss)
I1128 20:35:17.032061  8649 sgd_solver.cpp:105] Iteration 55400, lr = 0.001
I1128 20:35:48.443356  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:36:51.154150  8649 solver.cpp:218] Iteration 55600 (2.12479 iter/s, 94.1272s/200 iters), loss = 0.01697
I1128 20:36:51.154315  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00220454 (* 1 = 0.00220454 loss)
I1128 20:36:51.154328  8649 sgd_solver.cpp:105] Iteration 55600, lr = 0.001
I1128 20:38:25.298177  8649 solver.cpp:218] Iteration 55800 (2.12429 iter/s, 94.149s/200 iters), loss = 0.00561204
I1128 20:38:25.298384  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00810536 (* 1 = 0.00810536 loss)
I1128 20:38:25.298398  8649 sgd_solver.cpp:105] Iteration 55800, lr = 0.001
I1128 20:38:52.377557  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:39:59.000401  8649 solver.cpp:330] Iteration 56000, Testing net (#0)
I1128 20:40:11.614656  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:40:11.667443  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917002
I1128 20:40:11.667508  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.393628 (* 1 = 0.393628 loss)
I1128 20:40:12.132277  8649 solver.cpp:218] Iteration 56000 (1.87196 iter/s, 106.84s/200 iters), loss = 0.00300594
I1128 20:40:12.132330  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0067531 (* 1 = 0.0067531 loss)
I1128 20:40:12.132342  8649 sgd_solver.cpp:105] Iteration 56000, lr = 0.001
I1128 20:41:46.284862  8649 solver.cpp:218] Iteration 56200 (2.12409 iter/s, 94.1579s/200 iters), loss = 0.0119066
I1128 20:41:46.285027  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0344971 (* 1 = 0.0344971 loss)
I1128 20:41:46.285044  8649 sgd_solver.cpp:105] Iteration 56200, lr = 0.001
I1128 20:42:08.869884  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:43:20.239480  8649 solver.cpp:218] Iteration 56400 (2.12857 iter/s, 93.9599s/200 iters), loss = 0.0133848
I1128 20:43:20.239559  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000164671 (* 1 = 0.000164671 loss)
I1128 20:43:20.239570  8649 sgd_solver.cpp:105] Iteration 56400, lr = 0.001
I1128 20:44:54.277427  8649 solver.cpp:218] Iteration 56600 (2.12668 iter/s, 94.0434s/200 iters), loss = 0.00873645
I1128 20:44:54.277511  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.015551 (* 1 = 0.015551 loss)
I1128 20:44:54.277521  8649 sgd_solver.cpp:105] Iteration 56600, lr = 0.001
I1128 20:45:12.543493  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:46:29.117504  8649 solver.cpp:218] Iteration 56800 (2.10869 iter/s, 94.8456s/200 iters), loss = 0.0109607
I1128 20:46:29.117645  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.019796 (* 1 = 0.019796 loss)
I1128 20:46:29.117660  8649 sgd_solver.cpp:105] Iteration 56800, lr = 0.001
I1128 20:48:02.620616  8649 solver.cpp:330] Iteration 57000, Testing net (#0)
I1128 20:48:15.142268  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:48:15.194277  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915302
I1128 20:48:15.194304  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.398082 (* 1 = 0.398082 loss)
I1128 20:48:15.658850  8649 solver.cpp:218] Iteration 57000 (1.8771 iter/s, 106.548s/200 iters), loss = 0.00381211
I1128 20:48:15.658900  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00114181 (* 1 = 0.00114181 loss)
I1128 20:48:15.658908  8649 sgd_solver.cpp:105] Iteration 57000, lr = 0.001
I1128 20:48:29.394500  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:49:49.552770  8649 solver.cpp:218] Iteration 57200 (2.12994 iter/s, 93.8995s/200 iters), loss = 0.00652343
I1128 20:49:49.552875  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0089098 (* 1 = 0.0089098 loss)
I1128 20:49:49.552887  8649 sgd_solver.cpp:105] Iteration 57200, lr = 0.001
I1128 20:51:23.454035  8649 solver.cpp:218] Iteration 57400 (2.12977 iter/s, 93.9068s/200 iters), loss = 0.00658167
I1128 20:51:23.454167  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00262165 (* 1 = 0.00262165 loss)
I1128 20:51:23.454179  8649 sgd_solver.cpp:105] Iteration 57400, lr = 0.001
I1128 20:51:32.860690  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:52:09.994076  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_57500.caffemodel
I1128 20:52:10.007681  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_57500.solverstate
I1128 20:52:57.567667  8649 solver.cpp:218] Iteration 57600 (2.12496 iter/s, 94.1192s/200 iters), loss = 0.0127175
I1128 20:52:57.567816  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0102103 (* 1 = 0.0102103 loss)
I1128 20:52:57.567833  8649 sgd_solver.cpp:105] Iteration 57600, lr = 0.001
I1128 20:54:31.579401  8649 solver.cpp:218] Iteration 57800 (2.12727 iter/s, 94.0173s/200 iters), loss = 0.0120381
I1128 20:54:31.579504  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00595863 (* 1 = 0.00595863 loss)
I1128 20:54:31.579522  8649 sgd_solver.cpp:105] Iteration 57800, lr = 0.001
I1128 20:54:36.512747  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:56:04.961311  8649 solver.cpp:330] Iteration 58000, Testing net (#0)
I1128 20:56:17.463928  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:56:17.516386  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915602
I1128 20:56:17.516429  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.395896 (* 1 = 0.395896 loss)
I1128 20:56:17.980926  8649 solver.cpp:218] Iteration 58000 (1.87956 iter/s, 106.408s/200 iters), loss = 0.00912054
I1128 20:56:17.980975  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0213772 (* 1 = 0.0213772 loss)
I1128 20:56:17.980986  8649 sgd_solver.cpp:105] Iteration 58000, lr = 0.001
I1128 20:57:51.811659  8649 solver.cpp:218] Iteration 58200 (2.13137 iter/s, 93.8364s/200 iters), loss = 0.0129329
I1128 20:57:51.811790  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0174266 (* 1 = 0.0174266 loss)
I1128 20:57:51.811806  8649 sgd_solver.cpp:105] Iteration 58200, lr = 0.001
I1128 20:57:52.404521  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 20:59:25.652923  8649 solver.cpp:218] Iteration 58400 (2.13113 iter/s, 93.8469s/200 iters), loss = 0.0150376
I1128 20:59:25.653057  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0126335 (* 1 = 0.0126335 loss)
I1128 20:59:25.653072  8649 sgd_solver.cpp:105] Iteration 58400, lr = 0.001
I1128 21:00:55.628301  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:00:59.500219  8649 solver.cpp:218] Iteration 58600 (2.13099 iter/s, 93.8529s/200 iters), loss = 0.00902263
I1128 21:00:59.500285  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00322396 (* 1 = 0.00322396 loss)
I1128 21:00:59.500299  8649 sgd_solver.cpp:105] Iteration 58600, lr = 0.001
I1128 21:02:33.348404  8649 solver.cpp:218] Iteration 58800 (2.13097 iter/s, 93.8539s/200 iters), loss = 0.0590129
I1128 21:02:33.348502  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00100798 (* 1 = 0.00100798 loss)
I1128 21:02:33.348513  8649 sgd_solver.cpp:105] Iteration 58800, lr = 0.001
I1128 21:03:58.983069  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:04:06.727393  8649 solver.cpp:330] Iteration 59000, Testing net (#0)
I1128 21:04:19.240854  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:04:19.292904  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917602
I1128 21:04:19.292932  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.398597 (* 1 = 0.398597 loss)
I1128 21:04:19.758080  8649 solver.cpp:218] Iteration 59000 (1.87942 iter/s, 106.416s/200 iters), loss = 0.00108039
I1128 21:04:19.758119  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00169923 (* 1 = 0.00169923 loss)
I1128 21:04:19.758128  8649 sgd_solver.cpp:105] Iteration 59000, lr = 0.001
I1128 21:05:53.611192  8649 solver.cpp:218] Iteration 59200 (2.13087 iter/s, 93.8585s/200 iters), loss = 0.0072896
I1128 21:05:53.611361  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00075459 (* 1 = 0.00075459 loss)
I1128 21:05:53.611374  8649 sgd_solver.cpp:105] Iteration 59200, lr = 0.001
I1128 21:07:14.795650  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:07:27.458636  8649 solver.cpp:218] Iteration 59400 (2.131 iter/s, 93.8527s/200 iters), loss = 0.0112217
I1128 21:07:27.458684  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0236995 (* 1 = 0.0236995 loss)
I1128 21:07:27.458695  8649 sgd_solver.cpp:105] Iteration 59400, lr = 0.001
I1128 21:09:01.292485  8649 solver.cpp:218] Iteration 59600 (2.1313 iter/s, 93.8393s/200 iters), loss = 0.00382482
I1128 21:09:01.292574  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00846462 (* 1 = 0.00846462 loss)
I1128 21:09:01.292590  8649 sgd_solver.cpp:105] Iteration 59600, lr = 0.001
I1128 21:10:18.124948  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:10:35.135989  8649 solver.cpp:218] Iteration 59800 (2.13108 iter/s, 93.849s/200 iters), loss = 0.00967658
I1128 21:10:35.136051  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00337184 (* 1 = 0.00337184 loss)
I1128 21:10:35.136063  8649 sgd_solver.cpp:105] Iteration 59800, lr = 0.001
I1128 21:12:08.507936  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_60000.caffemodel
I1128 21:12:08.520669  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_60000.solverstate
I1128 21:12:08.525012  8649 solver.cpp:330] Iteration 60000, Testing net (#0)
I1128 21:12:21.027498  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:12:21.079311  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915202
I1128 21:12:21.079344  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.398726 (* 1 = 0.398726 loss)
I1128 21:12:21.544668  8649 solver.cpp:218] Iteration 60000 (1.87944 iter/s, 106.415s/200 iters), loss = 0.0104712
I1128 21:12:21.544728  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0304025 (* 1 = 0.0304025 loss)
I1128 21:12:21.544742  8649 sgd_solver.cpp:105] Iteration 60000, lr = 0.001
I1128 21:13:33.922201  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:13:55.387718  8649 solver.cpp:218] Iteration 60200 (2.13109 iter/s, 93.8486s/200 iters), loss = 0.00413867
I1128 21:13:55.387778  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.010917 (* 1 = 0.010917 loss)
I1128 21:13:55.387792  8649 sgd_solver.cpp:105] Iteration 60200, lr = 0.001
I1128 21:15:29.580307  8649 solver.cpp:218] Iteration 60400 (2.12318 iter/s, 94.1982s/200 iters), loss = 0.00632747
I1128 21:15:29.580461  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00154505 (* 1 = 0.00154505 loss)
I1128 21:15:29.580479  8649 sgd_solver.cpp:105] Iteration 60400, lr = 0.001
I1128 21:16:37.858892  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:17:03.776415  8649 solver.cpp:218] Iteration 60600 (2.12311 iter/s, 94.2016s/200 iters), loss = 0.00805884
I1128 21:17:03.776473  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0141632 (* 1 = 0.0141632 loss)
I1128 21:17:03.776484  8649 sgd_solver.cpp:105] Iteration 60600, lr = 0.001
I1128 21:18:37.958158  8649 solver.cpp:218] Iteration 60800 (2.12343 iter/s, 94.1874s/200 iters), loss = 0.0496548
I1128 21:18:37.958318  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.156411 (* 1 = 0.156411 loss)
I1128 21:18:37.958335  8649 sgd_solver.cpp:105] Iteration 60800, lr = 0.001
I1128 21:19:41.777475  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:20:11.687589  8649 solver.cpp:330] Iteration 61000, Testing net (#0)
I1128 21:20:24.300350  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:20:24.352663  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.916202
I1128 21:20:24.352722  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.400927 (* 1 = 0.400927 loss)
I1128 21:20:24.818938  8649 solver.cpp:218] Iteration 61000 (1.87148 iter/s, 106.867s/200 iters), loss = 0.00430083
I1128 21:20:24.819128  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0056602 (* 1 = 0.0056602 loss)
I1128 21:20:24.819154  8649 sgd_solver.cpp:105] Iteration 61000, lr = 0.001
I1128 21:21:59.023421  8649 solver.cpp:218] Iteration 61200 (2.12292 iter/s, 94.2101s/200 iters), loss = 0.00363645
I1128 21:21:59.023589  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0074759 (* 1 = 0.0074759 loss)
I1128 21:21:59.023600  8649 sgd_solver.cpp:105] Iteration 61200, lr = 0.001
I1128 21:22:58.480625  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:23:33.198110  8649 solver.cpp:218] Iteration 61400 (2.12359 iter/s, 94.1802s/200 iters), loss = 0.0248261
I1128 21:23:33.198310  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00380332 (* 1 = 0.00380332 loss)
I1128 21:23:33.198321  8649 sgd_solver.cpp:105] Iteration 61400, lr = 0.001
I1128 21:25:07.131018  8649 solver.cpp:218] Iteration 61600 (2.12905 iter/s, 93.9384s/200 iters), loss = 0.0121055
I1128 21:25:07.131109  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00314338 (* 1 = 0.00314338 loss)
I1128 21:25:07.131124  8649 sgd_solver.cpp:105] Iteration 61600, lr = 0.001
I1128 21:26:01.913621  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:26:40.987033  8649 solver.cpp:218] Iteration 61800 (2.1308 iter/s, 93.8616s/200 iters), loss = 0.0390883
I1128 21:26:40.987188  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0012543 (* 1 = 0.0012543 loss)
I1128 21:26:40.987202  8649 sgd_solver.cpp:105] Iteration 61800, lr = 0.001
I1128 21:28:14.358780  8649 solver.cpp:330] Iteration 62000, Testing net (#0)
I1128 21:28:26.849426  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:28:26.901813  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915702
I1128 21:28:26.901839  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.402346 (* 1 = 0.402346 loss)
I1128 21:28:27.366703  8649 solver.cpp:218] Iteration 62000 (1.87995 iter/s, 106.386s/200 iters), loss = 0.0193528
I1128 21:28:27.366749  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0081163 (* 1 = 0.0081163 loss)
I1128 21:28:27.366758  8649 sgd_solver.cpp:105] Iteration 62000, lr = 0.001
I1128 21:29:17.811779  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:30:01.217002  8649 solver.cpp:218] Iteration 62200 (2.13093 iter/s, 93.8559s/200 iters), loss = 0.00636258
I1128 21:30:01.217092  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000950231 (* 1 = 0.000950231 loss)
I1128 21:30:01.217109  8649 sgd_solver.cpp:105] Iteration 62200, lr = 0.001
I1128 21:31:35.064429  8649 solver.cpp:218] Iteration 62400 (2.13099 iter/s, 93.8531s/200 iters), loss = 0.0137638
I1128 21:31:35.064517  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00562836 (* 1 = 0.00562836 loss)
I1128 21:31:35.064532  8649 sgd_solver.cpp:105] Iteration 62400, lr = 0.001
I1128 21:32:21.062027  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:32:21.530869  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_62500.caffemodel
I1128 21:32:21.543352  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_62500.solverstate
I1128 21:33:08.932904  8649 solver.cpp:218] Iteration 62600 (2.13051 iter/s, 93.8741s/200 iters), loss = 0.00333688
I1128 21:33:08.933014  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00868794 (* 1 = 0.00868794 loss)
I1128 21:33:08.933030  8649 sgd_solver.cpp:105] Iteration 62600, lr = 0.001
I1128 21:34:42.810235  8649 solver.cpp:218] Iteration 62800 (2.13031 iter/s, 93.8829s/200 iters), loss = 0.00877173
I1128 21:34:42.810433  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00793289 (* 1 = 0.00793289 loss)
I1128 21:34:42.810448  8649 sgd_solver.cpp:105] Iteration 62800, lr = 0.001
I1128 21:35:24.474889  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:36:16.221972  8649 solver.cpp:330] Iteration 63000, Testing net (#0)
I1128 21:36:28.710204  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:36:28.762388  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915802
I1128 21:36:28.762414  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.401443 (* 1 = 0.401443 loss)
I1128 21:36:29.229210  8649 solver.cpp:218] Iteration 63000 (1.87925 iter/s, 106.425s/200 iters), loss = 0.00196073
I1128 21:36:29.229246  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00209238 (* 1 = 0.00209238 loss)
I1128 21:36:29.229254  8649 sgd_solver.cpp:105] Iteration 63000, lr = 0.001
I1128 21:38:03.327513  8649 solver.cpp:218] Iteration 63200 (2.12537 iter/s, 94.1013s/200 iters), loss = 0.0115579
I1128 21:38:03.327775  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00123695 (* 1 = 0.00123695 loss)
I1128 21:38:03.327800  8649 sgd_solver.cpp:105] Iteration 63200, lr = 0.001
I1128 21:38:41.337354  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:39:39.041741  8649 solver.cpp:218] Iteration 63400 (2.0895 iter/s, 95.7168s/200 iters), loss = 0.0161062
I1128 21:39:39.041918  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000844756 (* 1 = 0.000844756 loss)
I1128 21:39:39.041945  8649 sgd_solver.cpp:105] Iteration 63400, lr = 0.001
I1128 21:41:13.751035  8649 solver.cpp:218] Iteration 63600 (2.11166 iter/s, 94.7124s/200 iters), loss = 0.00400331
I1128 21:41:13.751148  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0107491 (* 1 = 0.0107491 loss)
I1128 21:41:13.751163  8649 sgd_solver.cpp:105] Iteration 63600, lr = 0.001
I1128 21:41:46.598475  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:42:47.591852  8649 solver.cpp:218] Iteration 63800 (2.13119 iter/s, 93.8443s/200 iters), loss = 0.0056106
I1128 21:42:47.592005  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0108149 (* 1 = 0.0108149 loss)
I1128 21:42:47.592017  8649 sgd_solver.cpp:105] Iteration 63800, lr = 0.001
I1128 21:44:21.279122  8649 solver.cpp:330] Iteration 64000, Testing net (#0)
I1128 21:44:33.830072  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:44:33.882277  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915702
I1128 21:44:33.882303  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.406295 (* 1 = 0.406295 loss)
I1128 21:44:34.346310  8649 solver.cpp:218] Iteration 64000 (1.87338 iter/s, 106.759s/200 iters), loss = 0.00390125
I1128 21:44:34.346346  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0070688 (* 1 = 0.0070688 loss)
I1128 21:44:34.346354  8649 sgd_solver.cpp:105] Iteration 64000, lr = 0.001
I1128 21:45:02.731173  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:46:08.169442  8649 solver.cpp:218] Iteration 64200 (2.13158 iter/s, 93.8272s/200 iters), loss = 0.00568348
I1128 21:46:08.169512  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00475517 (* 1 = 0.00475517 loss)
I1128 21:46:08.169523  8649 sgd_solver.cpp:105] Iteration 64200, lr = 0.001
I1128 21:47:41.985844  8649 solver.cpp:218] Iteration 64400 (2.13173 iter/s, 93.8207s/200 iters), loss = 0.00501614
I1128 21:47:41.985946  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00581827 (* 1 = 0.00581827 loss)
I1128 21:47:41.985956  8649 sgd_solver.cpp:105] Iteration 64400, lr = 0.001
I1128 21:48:06.031406  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:49:15.799649  8649 solver.cpp:218] Iteration 64600 (2.13178 iter/s, 93.8182s/200 iters), loss = 0.00597031
I1128 21:49:15.799741  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00307328 (* 1 = 0.00307328 loss)
I1128 21:49:15.799752  8649 sgd_solver.cpp:105] Iteration 64600, lr = 0.001
I1128 21:50:49.608654  8649 solver.cpp:218] Iteration 64800 (2.13189 iter/s, 93.8136s/200 iters), loss = 0.0308999
I1128 21:50:49.608873  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0982661 (* 1 = 0.0982661 loss)
I1128 21:50:49.608885  8649 sgd_solver.cpp:105] Iteration 64800, lr = 0.001
I1128 21:51:09.197566  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:52:22.964391  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_65000.caffemodel
I1128 21:52:22.977262  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_65000.solverstate
I1128 21:52:22.981765  8649 solver.cpp:330] Iteration 65000, Testing net (#0)
I1128 21:52:35.541481  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:52:35.594169  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.916001
I1128 21:52:35.594197  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.40533 (* 1 = 0.40533 loss)
I1128 21:52:36.060676  8649 solver.cpp:218] Iteration 65000 (1.87869 iter/s, 106.457s/200 iters), loss = 0.00426086
I1128 21:52:36.060715  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00438976 (* 1 = 0.00438976 loss)
I1128 21:52:36.060724  8649 sgd_solver.cpp:105] Iteration 65000, lr = 0.001
I1128 21:54:09.876541  8649 solver.cpp:218] Iteration 65200 (2.13173 iter/s, 93.8207s/200 iters), loss = 0.0059099
I1128 21:54:09.876646  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00871396 (* 1 = 0.00871396 loss)
I1128 21:54:09.876657  8649 sgd_solver.cpp:105] Iteration 65200, lr = 0.001
I1128 21:54:25.128082  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:55:43.697352  8649 solver.cpp:218] Iteration 65400 (2.13161 iter/s, 93.8256s/200 iters), loss = 0.0216523
I1128 21:55:43.697512  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0221529 (* 1 = 0.0221529 loss)
I1128 21:55:43.697523  8649 sgd_solver.cpp:105] Iteration 65400, lr = 0.001
I1128 21:57:17.494930  8649 solver.cpp:218] Iteration 65600 (2.13214 iter/s, 93.8024s/200 iters), loss = 0.0121715
I1128 21:57:17.494998  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0144286 (* 1 = 0.0144286 loss)
I1128 21:57:17.495007  8649 sgd_solver.cpp:105] Iteration 65600, lr = 0.001
I1128 21:57:28.286383  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 21:58:51.298516  8649 solver.cpp:218] Iteration 65800 (2.132 iter/s, 93.8086s/200 iters), loss = 0.00840396
I1128 21:58:51.298588  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.024176 (* 1 = 0.024176 loss)
I1128 21:58:51.298599  8649 sgd_solver.cpp:105] Iteration 65800, lr = 0.001
I1128 22:00:24.639906  8649 solver.cpp:330] Iteration 66000, Testing net (#0)
I1128 22:00:37.108547  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:00:37.161368  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.916801
I1128 22:00:37.161393  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.405516 (* 1 = 0.405516 loss)
I1128 22:00:37.625017  8649 solver.cpp:218] Iteration 66000 (1.8809 iter/s, 106.332s/200 iters), loss = 0.0220613
I1128 22:00:37.625056  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0832513 (* 1 = 0.0832513 loss)
I1128 22:00:37.625064  8649 sgd_solver.cpp:105] Iteration 66000, lr = 0.001
I1128 22:00:44.076990  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:02:11.423427  8649 solver.cpp:218] Iteration 66200 (2.13212 iter/s, 93.8035s/200 iters), loss = 0.0014388
I1128 22:02:11.423571  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00182054 (* 1 = 0.00182054 loss)
I1128 22:02:11.423581  8649 sgd_solver.cpp:105] Iteration 66200, lr = 0.001
I1128 22:03:45.234195  8649 solver.cpp:218] Iteration 66400 (2.13184 iter/s, 93.8158s/200 iters), loss = 0.00915758
I1128 22:03:45.234335  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00998027 (* 1 = 0.00998027 loss)
I1128 22:03:45.234344  8649 sgd_solver.cpp:105] Iteration 66400, lr = 0.001
I1128 22:03:47.234408  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:05:19.046157  8649 solver.cpp:218] Iteration 66600 (2.13181 iter/s, 93.817s/200 iters), loss = 0.0100051
I1128 22:05:19.046226  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00180352 (* 1 = 0.00180352 loss)
I1128 22:05:19.046236  8649 sgd_solver.cpp:105] Iteration 66600, lr = 0.001
I1128 22:06:50.509111  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:06:52.850298  8649 solver.cpp:218] Iteration 66800 (2.13198 iter/s, 93.8093s/200 iters), loss = 0.00598371
I1128 22:06:52.850333  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0112723 (* 1 = 0.0112723 loss)
I1128 22:06:52.850342  8649 sgd_solver.cpp:105] Iteration 66800, lr = 0.001
I1128 22:08:26.201633  8649 solver.cpp:330] Iteration 67000, Testing net (#0)
I1128 22:08:38.674867  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:08:38.726728  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.916102
I1128 22:08:38.726755  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.410648 (* 1 = 0.410648 loss)
I1128 22:08:39.191079  8649 solver.cpp:218] Iteration 67000 (1.88064 iter/s, 106.347s/200 iters), loss = 0.00882246
I1128 22:08:39.191118  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0308752 (* 1 = 0.0308752 loss)
I1128 22:08:39.191128  8649 sgd_solver.cpp:105] Iteration 67000, lr = 0.001
I1128 22:10:06.209700  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:10:13.010789  8649 solver.cpp:218] Iteration 67200 (2.13163 iter/s, 93.8249s/200 iters), loss = 0.0474485
I1128 22:10:13.010828  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0033222 (* 1 = 0.0033222 loss)
I1128 22:10:13.010838  8649 sgd_solver.cpp:105] Iteration 67200, lr = 0.001
I1128 22:11:46.825510  8649 solver.cpp:218] Iteration 67400 (2.13164 iter/s, 93.8245s/200 iters), loss = 0.0158393
I1128 22:11:46.825655  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000975289 (* 1 = 0.000975289 loss)
I1128 22:11:46.825665  8649 sgd_solver.cpp:105] Iteration 67400, lr = 0.001
I1128 22:12:33.255053  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_67500.caffemodel
I1128 22:12:33.267328  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_67500.solverstate
I1128 22:13:09.500393  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:13:20.634680  8649 solver.cpp:218] Iteration 67600 (2.13169 iter/s, 93.8223s/200 iters), loss = 0.01307
I1128 22:13:20.634717  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000753498 (* 1 = 0.000753498 loss)
I1128 22:13:20.634726  8649 sgd_solver.cpp:105] Iteration 67600, lr = 0.001
I1128 22:14:54.431959  8649 solver.cpp:218] Iteration 67800 (2.13198 iter/s, 93.8094s/200 iters), loss = 0.00366965
I1128 22:14:54.432018  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00345425 (* 1 = 0.00345425 loss)
I1128 22:14:54.432026  8649 sgd_solver.cpp:105] Iteration 67800, lr = 0.001
I1128 22:16:12.635951  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:16:27.767863  8649 solver.cpp:330] Iteration 68000, Testing net (#0)
I1128 22:16:40.242197  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:16:40.294965  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.916102
I1128 22:16:40.294992  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.409286 (* 1 = 0.409286 loss)
I1128 22:16:40.759603  8649 solver.cpp:218] Iteration 68000 (1.88075 iter/s, 106.34s/200 iters), loss = 0.0067322
I1128 22:16:40.759641  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00234866 (* 1 = 0.00234866 loss)
I1128 22:16:40.759650  8649 sgd_solver.cpp:105] Iteration 68000, lr = 0.001
I1128 22:18:14.564105  8649 solver.cpp:218] Iteration 68200 (2.13186 iter/s, 93.8149s/200 iters), loss = 0.00699405
I1128 22:18:14.564298  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00108348 (* 1 = 0.00108348 loss)
I1128 22:18:14.564316  8649 sgd_solver.cpp:105] Iteration 68200, lr = 0.001
I1128 22:19:28.439569  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:19:48.370036  8649 solver.cpp:218] Iteration 68400 (2.13184 iter/s, 93.8156s/200 iters), loss = 0.0120955
I1128 22:19:48.370080  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000867205 (* 1 = 0.000867205 loss)
I1128 22:19:48.370091  8649 sgd_solver.cpp:105] Iteration 68400, lr = 0.001
I1128 22:21:22.185328  8649 solver.cpp:218] Iteration 68600 (2.13164 iter/s, 93.8246s/200 iters), loss = 0.0117231
I1128 22:21:22.185396  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00601889 (* 1 = 0.00601889 loss)
I1128 22:21:22.185405  8649 sgd_solver.cpp:105] Iteration 68600, lr = 0.001
I1128 22:22:31.602509  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:22:55.983986  8649 solver.cpp:218] Iteration 68800 (2.13203 iter/s, 93.8075s/200 iters), loss = 0.00143525
I1128 22:22:55.984024  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00261378 (* 1 = 0.00261378 loss)
I1128 22:22:55.984032  8649 sgd_solver.cpp:105] Iteration 68800, lr = 0.001
I1128 22:24:29.337169  8649 solver.cpp:330] Iteration 69000, Testing net (#0)
I1128 22:24:41.813915  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:24:41.866360  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915602
I1128 22:24:41.866390  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.410435 (* 1 = 0.410435 loss)
I1128 22:24:42.330603  8649 solver.cpp:218] Iteration 69000 (1.88047 iter/s, 106.356s/200 iters), loss = 0.00609863
I1128 22:24:42.330641  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00243205 (* 1 = 0.00243205 loss)
I1128 22:24:42.330651  8649 sgd_solver.cpp:105] Iteration 69000, lr = 0.001
I1128 22:25:47.435138  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:26:16.173476  8649 solver.cpp:218] Iteration 69200 (2.13104 iter/s, 93.851s/200 iters), loss = 0.00937677
I1128 22:26:16.173522  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00144462 (* 1 = 0.00144462 loss)
I1128 22:26:16.173532  8649 sgd_solver.cpp:105] Iteration 69200, lr = 0.001
I1128 22:27:50.025136  8649 solver.cpp:218] Iteration 69400 (2.13084 iter/s, 93.8596s/200 iters), loss = 0.0164687
I1128 22:27:50.025282  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00190339 (* 1 = 0.00190339 loss)
I1128 22:27:50.025297  8649 sgd_solver.cpp:105] Iteration 69400, lr = 0.001
I1128 22:28:50.681560  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:29:23.886183  8649 solver.cpp:218] Iteration 69600 (2.13064 iter/s, 93.8687s/200 iters), loss = 0.0047128
I1128 22:29:23.886272  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00549196 (* 1 = 0.00549196 loss)
I1128 22:29:23.886281  8649 sgd_solver.cpp:105] Iteration 69600, lr = 0.001
I1128 22:30:57.741734  8649 solver.cpp:218] Iteration 69800 (2.13076 iter/s, 93.8631s/200 iters), loss = 0.00322876
I1128 22:30:57.741899  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000358303 (* 1 = 0.000358303 loss)
I1128 22:30:57.741912  8649 sgd_solver.cpp:105] Iteration 69800, lr = 0.001
I1128 22:31:54.062119  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:32:31.129339  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_70000.caffemodel
I1128 22:32:31.141952  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_70000.solverstate
I1128 22:32:31.146535  8649 solver.cpp:330] Iteration 70000, Testing net (#0)
I1128 22:32:43.641422  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:32:43.694157  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.915902
I1128 22:32:43.694188  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.41265 (* 1 = 0.41265 loss)
I1128 22:32:44.158814  8649 solver.cpp:218] Iteration 70000 (1.87925 iter/s, 106.425s/200 iters), loss = 0.00486121
I1128 22:32:44.158864  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000222401 (* 1 = 0.000222401 loss)
I1128 22:32:44.158876  8649 sgd_solver.cpp:105] Iteration 70000, lr = 0.001
I1128 22:34:18.356405  8649 solver.cpp:218] Iteration 70200 (2.12303 iter/s, 94.2049s/200 iters), loss = 0.0114735
I1128 22:34:18.356555  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00299209 (* 1 = 0.00299209 loss)
I1128 22:34:18.356568  8649 sgd_solver.cpp:105] Iteration 70200, lr = 0.001
I1128 22:35:10.270851  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:35:52.343164  8649 solver.cpp:218] Iteration 70400 (2.1278 iter/s, 93.9939s/200 iters), loss = 0.00288875
I1128 22:35:52.343272  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00944869 (* 1 = 0.00944869 loss)
I1128 22:35:52.343304  8649 sgd_solver.cpp:105] Iteration 70400, lr = 0.001
I1128 22:37:26.981464  8649 solver.cpp:218] Iteration 70600 (2.11315 iter/s, 94.6454s/200 iters), loss = 0.0174444
I1128 22:37:26.981636  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.001166 (* 1 = 0.001166 loss)
I1128 22:37:26.981660  8649 sgd_solver.cpp:105] Iteration 70600, lr = 0.001
I1128 22:38:14.714062  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:39:01.184828  8649 solver.cpp:218] Iteration 70800 (2.12291 iter/s, 94.2103s/200 iters), loss = 0.00215018
I1128 22:39:01.184970  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00210711 (* 1 = 0.00210711 loss)
I1128 22:39:01.184990  8649 sgd_solver.cpp:105] Iteration 70800, lr = 0.001
I1128 22:40:34.645735  8649 solver.cpp:330] Iteration 71000, Testing net (#0)
I1128 22:40:47.240761  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:40:47.294616  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.916702
I1128 22:40:47.294648  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.414503 (* 1 = 0.414503 loss)
I1128 22:40:47.760058  8649 solver.cpp:218] Iteration 71000 (1.87647 iter/s, 106.583s/200 iters), loss = 0.00364969
I1128 22:40:47.760109  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00163455 (* 1 = 0.00163455 loss)
I1128 22:40:47.760119  8649 sgd_solver.cpp:105] Iteration 71000, lr = 0.001
I1128 22:41:30.858212  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:42:21.720788  8649 solver.cpp:218] Iteration 71200 (2.12839 iter/s, 93.9677s/200 iters), loss = 0.00319256
I1128 22:42:21.720886  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00153411 (* 1 = 0.00153411 loss)
I1128 22:42:21.720896  8649 sgd_solver.cpp:105] Iteration 71200, lr = 0.001
I1128 22:43:55.670908  8649 solver.cpp:218] Iteration 71400 (2.12863 iter/s, 93.957s/200 iters), loss = 0.00859371
I1128 22:43:55.671012  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0190787 (* 1 = 0.0190787 loss)
I1128 22:43:55.671023  8649 sgd_solver.cpp:105] Iteration 71400, lr = 0.001
I1128 22:44:34.471253  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:45:29.661548  8649 solver.cpp:218] Iteration 71600 (2.12779 iter/s, 93.9943s/200 iters), loss = 0.0102064
I1128 22:45:29.661654  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00559046 (* 1 = 0.00559046 loss)
I1128 22:45:29.661669  8649 sgd_solver.cpp:105] Iteration 71600, lr = 0.001
I1128 22:47:03.640502  8649 solver.cpp:218] Iteration 71800 (2.12826 iter/s, 93.9733s/200 iters), loss = 0.00992774
I1128 22:47:03.640733  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00447392 (* 1 = 0.00447392 loss)
I1128 22:47:03.640746  8649 sgd_solver.cpp:105] Iteration 71800, lr = 0.001
I1128 22:47:37.935217  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:48:37.128612  8649 solver.cpp:330] Iteration 72000, Testing net (#0)
I1128 22:48:49.664976  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:48:49.717804  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.916601
I1128 22:48:49.717839  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.416285 (* 1 = 0.416285 loss)
I1128 22:48:50.184075  8649 solver.cpp:218] Iteration 72000 (1.87724 iter/s, 106.539s/200 iters), loss = 0.00890989
I1128 22:48:50.184128  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00160355 (* 1 = 0.00160355 loss)
I1128 22:48:50.184146  8649 sgd_solver.cpp:105] Iteration 72000, lr = 0.0001
I1128 22:50:24.914346  8649 solver.cpp:218] Iteration 72200 (2.11131 iter/s, 94.728s/200 iters), loss = 0.00662178
I1128 22:50:24.914445  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0131104 (* 1 = 0.0131104 loss)
I1128 22:50:24.914458  8649 sgd_solver.cpp:105] Iteration 72200, lr = 0.0001
I1128 22:50:54.888769  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:51:58.876979  8649 solver.cpp:218] Iteration 72400 (2.12853 iter/s, 93.9615s/200 iters), loss = 0.00822167
I1128 22:51:58.877135  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00238128 (* 1 = 0.00238128 loss)
I1128 22:51:58.877146  8649 sgd_solver.cpp:105] Iteration 72400, lr = 0.0001
I1128 22:52:45.399082  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_72500.caffemodel
I1128 22:52:45.411540  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_72500.solverstate
I1128 22:53:32.853009  8649 solver.cpp:218] Iteration 72600 (2.12821 iter/s, 93.9758s/200 iters), loss = 0.0151093
I1128 22:53:32.853164  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0209119 (* 1 = 0.0209119 loss)
I1128 22:53:32.853181  8649 sgd_solver.cpp:105] Iteration 72600, lr = 0.0001
I1128 22:53:58.333820  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:55:06.715736  8649 solver.cpp:218] Iteration 72800 (2.13076 iter/s, 93.8633s/200 iters), loss = 0.035988
I1128 22:55:06.715837  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0219916 (* 1 = 0.0219916 loss)
I1128 22:55:06.715849  8649 sgd_solver.cpp:105] Iteration 72800, lr = 0.0001
I1128 22:56:40.075589  8649 solver.cpp:330] Iteration 73000, Testing net (#0)
I1128 22:56:52.538873  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:56:52.591493  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917102
I1128 22:56:52.591526  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.415859 (* 1 = 0.415859 loss)
I1128 22:56:53.056233  8649 solver.cpp:218] Iteration 73000 (1.88073 iter/s, 106.342s/200 iters), loss = 0.00357053
I1128 22:56:53.056282  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00139549 (* 1 = 0.00139549 loss)
I1128 22:56:53.056293  8649 sgd_solver.cpp:105] Iteration 73000, lr = 0.0001
I1128 22:57:14.174263  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 22:58:26.863092  8649 solver.cpp:218] Iteration 73200 (2.132 iter/s, 93.8088s/200 iters), loss = 0.00143501
I1128 22:58:26.863185  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000754234 (* 1 = 0.000754234 loss)
I1128 22:58:26.863200  8649 sgd_solver.cpp:105] Iteration 73200, lr = 0.0001
I1128 23:00:00.672168  8649 solver.cpp:218] Iteration 73400 (2.13194 iter/s, 93.8114s/200 iters), loss = 0.013346
I1128 23:00:00.672303  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0159099 (* 1 = 0.0159099 loss)
I1128 23:00:00.672312  8649 sgd_solver.cpp:105] Iteration 73400, lr = 0.0001
I1128 23:00:17.329407  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:01:34.477285  8649 solver.cpp:218] Iteration 73600 (2.13202 iter/s, 93.8077s/200 iters), loss = 0.00397464
I1128 23:01:34.477363  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000921276 (* 1 = 0.000921276 loss)
I1128 23:01:34.477372  8649 sgd_solver.cpp:105] Iteration 73600, lr = 0.0001
I1128 23:03:08.282505  8649 solver.cpp:218] Iteration 73800 (2.13201 iter/s, 93.8082s/200 iters), loss = 0.00553165
I1128 23:03:08.282586  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000550519 (* 1 = 0.000550519 loss)
I1128 23:03:08.282601  8649 sgd_solver.cpp:105] Iteration 73800, lr = 0.0001
I1128 23:03:20.597321  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:04:41.615927  8649 solver.cpp:330] Iteration 74000, Testing net (#0)
I1128 23:04:54.075904  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:04:54.128206  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.916902
I1128 23:04:54.128258  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.415529 (* 1 = 0.415529 loss)
I1128 23:04:54.591784  8649 solver.cpp:218] Iteration 74000 (1.88124 iter/s, 106.313s/200 iters), loss = 0.0172575
I1128 23:04:54.591820  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00189064 (* 1 = 0.00189064 loss)
I1128 23:04:54.591828  8649 sgd_solver.cpp:105] Iteration 74000, lr = 0.0001
I1128 23:06:28.599112  8649 solver.cpp:218] Iteration 74200 (2.12741 iter/s, 94.0108s/200 iters), loss = 0.00314915
I1128 23:06:28.599229  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00133239 (* 1 = 0.00133239 loss)
I1128 23:06:28.599247  8649 sgd_solver.cpp:105] Iteration 74200, lr = 0.0001
I1128 23:06:36.501873  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:08:02.965077  8649 solver.cpp:218] Iteration 74400 (2.11933 iter/s, 94.3696s/200 iters), loss = 0.014263
I1128 23:08:02.965194  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000291004 (* 1 = 0.000291004 loss)
I1128 23:08:02.965212  8649 sgd_solver.cpp:105] Iteration 74400, lr = 0.0001
I1128 23:09:37.347260  8649 solver.cpp:218] Iteration 74600 (2.11896 iter/s, 94.386s/200 iters), loss = 0.00679553
I1128 23:09:37.347471  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00221646 (* 1 = 0.00221646 loss)
I1128 23:09:37.347491  8649 sgd_solver.cpp:105] Iteration 74600, lr = 0.0001
I1128 23:09:40.889583  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:11:11.708771  8649 solver.cpp:218] Iteration 74800 (2.11942 iter/s, 94.3653s/200 iters), loss = 0.00181758
I1128 23:11:11.708916  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000494115 (* 1 = 0.000494115 loss)
I1128 23:11:11.708932  8649 sgd_solver.cpp:105] Iteration 74800, lr = 0.0001
I1128 23:12:45.142745  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:12:45.615734  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_75000.caffemodel
I1128 23:12:45.632083  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_75000.solverstate
I1128 23:12:45.637058  8649 solver.cpp:330] Iteration 75000, Testing net (#0)
I1128 23:12:58.351806  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:12:58.405233  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917202
I1128 23:12:58.405282  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.415524 (* 1 = 0.415524 loss)
I1128 23:12:58.870468  8649 solver.cpp:218] Iteration 75000 (1.86626 iter/s, 107.166s/200 iters), loss = 0.00681868
I1128 23:12:58.870592  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000329711 (* 1 = 0.000329711 loss)
I1128 23:12:58.870620  8649 sgd_solver.cpp:105] Iteration 75000, lr = 0.0001
I1128 23:14:33.227756  8649 solver.cpp:218] Iteration 75200 (2.11951 iter/s, 94.3614s/200 iters), loss = 0.0123593
I1128 23:14:33.227951  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00567141 (* 1 = 0.00567141 loss)
I1128 23:14:33.228022  8649 sgd_solver.cpp:105] Iteration 75200, lr = 0.0001
I1128 23:16:02.233511  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:16:07.511490  8649 solver.cpp:218] Iteration 75400 (2.12116 iter/s, 94.2878s/200 iters), loss = 0.00944992
I1128 23:16:07.511533  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00642069 (* 1 = 0.00642069 loss)
I1128 23:16:07.511541  8649 sgd_solver.cpp:105] Iteration 75400, lr = 0.0001
I1128 23:17:41.319130  8649 solver.cpp:218] Iteration 75600 (2.13192 iter/s, 93.8119s/200 iters), loss = 0.00901008
I1128 23:17:41.319211  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000538964 (* 1 = 0.000538964 loss)
I1128 23:17:41.319222  8649 sgd_solver.cpp:105] Iteration 75600, lr = 0.0001
I1128 23:19:05.407483  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:19:15.134356  8649 solver.cpp:218] Iteration 75800 (2.13175 iter/s, 93.8195s/200 iters), loss = 0.00135866
I1128 23:19:15.134395  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000252644 (* 1 = 0.000252644 loss)
I1128 23:19:15.134404  8649 sgd_solver.cpp:105] Iteration 75800, lr = 0.0001
I1128 23:20:48.483634  8649 solver.cpp:330] Iteration 76000, Testing net (#0)
I1128 23:21:00.958039  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:21:01.010148  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917602
I1128 23:21:01.010174  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.415426 (* 1 = 0.415426 loss)
I1128 23:21:01.475122  8649 solver.cpp:218] Iteration 76000 (1.88068 iter/s, 106.345s/200 iters), loss = 0.00104047
I1128 23:21:01.475172  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000336024 (* 1 = 0.000336024 loss)
I1128 23:21:01.475183  8649 sgd_solver.cpp:105] Iteration 76000, lr = 0.0001
I1128 23:22:21.234654  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:22:35.305335  8649 solver.cpp:218] Iteration 76200 (2.13143 iter/s, 93.8338s/200 iters), loss = 0.0180256
I1128 23:22:35.305382  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000568956 (* 1 = 0.000568956 loss)
I1128 23:22:35.305394  8649 sgd_solver.cpp:105] Iteration 76200, lr = 0.0001
I1128 23:24:09.115862  8649 solver.cpp:218] Iteration 76400 (2.13187 iter/s, 93.8143s/200 iters), loss = 0.00547387
I1128 23:24:09.115958  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000756049 (* 1 = 0.000756049 loss)
I1128 23:24:09.115973  8649 sgd_solver.cpp:105] Iteration 76400, lr = 0.0001
I1128 23:25:24.413462  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:25:42.941020  8649 solver.cpp:218] Iteration 76600 (2.13154 iter/s, 93.829s/200 iters), loss = 0.00169279
I1128 23:25:42.941063  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000701042 (* 1 = 0.000701042 loss)
I1128 23:25:42.941073  8649 sgd_solver.cpp:105] Iteration 76600, lr = 0.0001
I1128 23:27:16.743316  8649 solver.cpp:218] Iteration 76800 (2.13205 iter/s, 93.8062s/200 iters), loss = 0.0105672
I1128 23:27:16.743433  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00738785 (* 1 = 0.00738785 loss)
I1128 23:27:16.743444  8649 sgd_solver.cpp:105] Iteration 76800, lr = 0.0001
I1128 23:28:27.686422  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:28:50.085552  8649 solver.cpp:330] Iteration 77000, Testing net (#0)
I1128 23:29:02.544999  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:29:02.596994  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917502
I1128 23:29:02.597020  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.415896 (* 1 = 0.415896 loss)
I1128 23:29:03.061017  8649 solver.cpp:218] Iteration 77000 (1.88107 iter/s, 106.322s/200 iters), loss = 0.0029277
I1128 23:29:03.061066  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00398985 (* 1 = 0.00398985 loss)
I1128 23:29:03.061076  8649 sgd_solver.cpp:105] Iteration 77000, lr = 0.0001
I1128 23:30:36.873993  8649 solver.cpp:218] Iteration 77200 (2.13181 iter/s, 93.8171s/200 iters), loss = 0.00258342
I1128 23:30:36.874070  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000169192 (* 1 = 0.000169192 loss)
I1128 23:30:36.874083  8649 sgd_solver.cpp:105] Iteration 77200, lr = 0.0001
I1128 23:31:43.561944  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:32:10.906363  8649 solver.cpp:218] Iteration 77400 (2.12683 iter/s, 94.0365s/200 iters), loss = 0.0045688
I1128 23:32:10.906404  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00275506 (* 1 = 0.00275506 loss)
I1128 23:32:10.906414  8649 sgd_solver.cpp:105] Iteration 77400, lr = 0.0001
I1128 23:32:57.367264  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_77500.caffemodel
I1128 23:32:57.380090  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_77500.solverstate
I1128 23:33:44.783335  8649 solver.cpp:218] Iteration 77600 (2.13035 iter/s, 93.8812s/200 iters), loss = 0.0144702
I1128 23:33:44.783418  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00127116 (* 1 = 0.00127116 loss)
I1128 23:33:44.783430  8649 sgd_solver.cpp:105] Iteration 77600, lr = 0.0001
I1128 23:34:46.971124  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:35:18.650288  8649 solver.cpp:218] Iteration 77800 (2.13058 iter/s, 93.8712s/200 iters), loss = 0.00336605
I1128 23:35:18.650379  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.001827 (* 1 = 0.001827 loss)
I1128 23:35:18.650395  8649 sgd_solver.cpp:105] Iteration 77800, lr = 0.0001
I1128 23:36:52.179203  8649 solver.cpp:330] Iteration 78000, Testing net (#0)
I1128 23:37:04.792990  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:37:04.846539  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917702
I1128 23:37:04.846573  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.415806 (* 1 = 0.415806 loss)
I1128 23:37:05.312999  8649 solver.cpp:218] Iteration 78000 (1.87498 iter/s, 106.668s/200 iters), loss = 0.0037894
I1128 23:37:05.313051  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00140396 (* 1 = 0.00140396 loss)
I1128 23:37:05.313063  8649 sgd_solver.cpp:105] Iteration 78000, lr = 0.0001
I1128 23:38:03.205718  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:38:39.449640  8649 solver.cpp:218] Iteration 78200 (2.12447 iter/s, 94.1409s/200 iters), loss = 0.0217065
I1128 23:38:39.449743  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00693251 (* 1 = 0.00693251 loss)
I1128 23:38:39.449757  8649 sgd_solver.cpp:105] Iteration 78200, lr = 0.0001
I1128 23:40:13.543050  8649 solver.cpp:218] Iteration 78400 (2.12545 iter/s, 94.0977s/200 iters), loss = 0.0166406
I1128 23:40:13.543146  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0525649 (* 1 = 0.0525649 loss)
I1128 23:40:13.543161  8649 sgd_solver.cpp:105] Iteration 78400, lr = 0.0001
I1128 23:41:07.100023  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:41:47.676873  8649 solver.cpp:218] Iteration 78600 (2.12454 iter/s, 94.1381s/200 iters), loss = 0.00349452
I1128 23:41:47.677085  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00288452 (* 1 = 0.00288452 loss)
I1128 23:41:47.677096  8649 sgd_solver.cpp:105] Iteration 78600, lr = 0.0001
I1128 23:43:21.744257  8649 solver.cpp:218] Iteration 78800 (2.12604 iter/s, 94.0716s/200 iters), loss = 0.00288249
I1128 23:43:21.744350  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000861615 (* 1 = 0.000861615 loss)
I1128 23:43:21.744365  8649 sgd_solver.cpp:105] Iteration 78800, lr = 0.0001
I1128 23:44:10.792526  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:44:55.320168  8649 solver.cpp:330] Iteration 79000, Testing net (#0)
I1128 23:45:07.887037  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:45:07.939862  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917102
I1128 23:45:07.939911  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.416065 (* 1 = 0.416065 loss)
I1128 23:45:08.405478  8649 solver.cpp:218] Iteration 79000 (1.87501 iter/s, 106.666s/200 iters), loss = 0.00336274
I1128 23:45:08.405530  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00273214 (* 1 = 0.00273214 loss)
I1128 23:45:08.405540  8649 sgd_solver.cpp:105] Iteration 79000, lr = 0.0001
I1128 23:46:42.321169  8649 solver.cpp:218] Iteration 79200 (2.12947 iter/s, 93.9201s/200 iters), loss = 0.00285542
I1128 23:46:42.321260  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000899271 (* 1 = 0.000899271 loss)
I1128 23:46:42.321282  8649 sgd_solver.cpp:105] Iteration 79200, lr = 0.0001
I1128 23:47:26.888727  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:48:16.134932  8649 solver.cpp:218] Iteration 79400 (2.13178 iter/s, 93.8181s/200 iters), loss = 0.00409059
I1128 23:48:16.135087  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0055187 (* 1 = 0.0055187 loss)
I1128 23:48:16.135098  8649 sgd_solver.cpp:105] Iteration 79400, lr = 0.0001
I1128 23:49:49.958539  8649 solver.cpp:218] Iteration 79600 (2.13156 iter/s, 93.8279s/200 iters), loss = 0.00759182
I1128 23:49:49.958597  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00447533 (* 1 = 0.00447533 loss)
I1128 23:49:49.958606  8649 sgd_solver.cpp:105] Iteration 79600, lr = 0.0001
I1128 23:50:30.074957  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:51:23.791574  8649 solver.cpp:218] Iteration 79800 (2.13135 iter/s, 93.8374s/200 iters), loss = 0.00583523
I1128 23:51:23.791723  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0031309 (* 1 = 0.0031309 loss)
I1128 23:51:23.791734  8649 sgd_solver.cpp:105] Iteration 79800, lr = 0.0001
I1128 23:52:57.143311  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_80000.caffemodel
I1128 23:52:57.155468  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_80000.solverstate
I1128 23:52:57.160496  8649 solver.cpp:330] Iteration 80000, Testing net (#0)
I1128 23:53:09.632221  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:53:09.684283  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917102
I1128 23:53:09.684310  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.416211 (* 1 = 0.416211 loss)
I1128 23:53:10.148859  8649 solver.cpp:218] Iteration 80000 (1.88037 iter/s, 106.362s/200 iters), loss = 0.0101838
I1128 23:53:10.148905  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000295338 (* 1 = 0.000295338 loss)
I1128 23:53:10.148914  8649 sgd_solver.cpp:105] Iteration 80000, lr = 0.0001
I1128 23:53:45.934651  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:54:44.145869  8649 solver.cpp:218] Iteration 80200 (2.12754 iter/s, 94.0053s/200 iters), loss = 0.00789332
I1128 23:54:44.145987  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00666662 (* 1 = 0.00666662 loss)
I1128 23:54:44.145998  8649 sgd_solver.cpp:105] Iteration 80200, lr = 0.0001
I1128 23:56:17.979838  8649 solver.cpp:218] Iteration 80400 (2.13123 iter/s, 93.8423s/200 iters), loss = 0.00664969
I1128 23:56:17.980075  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00817245 (* 1 = 0.00817245 loss)
I1128 23:56:17.980094  8649 sgd_solver.cpp:105] Iteration 80400, lr = 0.0001
I1128 23:56:49.302129  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1128 23:57:51.826498  8649 solver.cpp:218] Iteration 80600 (2.13096 iter/s, 93.8544s/200 iters), loss = 0.00257054
I1128 23:57:51.826629  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00262024 (* 1 = 0.00262024 loss)
I1128 23:57:51.826644  8649 sgd_solver.cpp:105] Iteration 80600, lr = 0.0001
I1128 23:59:25.666931  8649 solver.cpp:218] Iteration 80800 (2.13111 iter/s, 93.8478s/200 iters), loss = 0.00411747
I1128 23:59:25.666993  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00410182 (* 1 = 0.00410182 loss)
I1128 23:59:25.667006  8649 sgd_solver.cpp:105] Iteration 80800, lr = 0.0001
I1128 23:59:52.646595  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:00:59.027025  8649 solver.cpp:330] Iteration 81000, Testing net (#0)
I1129 00:01:11.541595  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:01:11.593678  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917302
I1129 00:01:11.593706  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.415864 (* 1 = 0.415864 loss)
I1129 00:01:12.058120  8649 solver.cpp:218] Iteration 81000 (1.87971 iter/s, 106.399s/200 iters), loss = 0.00227405
I1129 00:01:12.058164  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00280534 (* 1 = 0.00280534 loss)
I1129 00:01:12.058173  8649 sgd_solver.cpp:105] Iteration 81000, lr = 0.0001
I1129 00:02:45.887984  8649 solver.cpp:218] Iteration 81200 (2.13136 iter/s, 93.8366s/200 iters), loss = 0.0211294
I1129 00:02:45.888147  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0014445 (* 1 = 0.0014445 loss)
I1129 00:02:45.888164  8649 sgd_solver.cpp:105] Iteration 81200, lr = 0.0001
I1129 00:03:08.413609  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:04:19.710222  8649 solver.cpp:218] Iteration 81400 (2.13155 iter/s, 93.8286s/200 iters), loss = 0.00719726
I1129 00:04:19.710315  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000125524 (* 1 = 0.000125524 loss)
I1129 00:04:19.710325  8649 sgd_solver.cpp:105] Iteration 81400, lr = 0.0001
I1129 00:05:53.538548  8649 solver.cpp:218] Iteration 81600 (2.13141 iter/s, 93.8345s/200 iters), loss = 0.0143151
I1129 00:05:53.538653  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00148603 (* 1 = 0.00148603 loss)
I1129 00:05:53.538663  8649 sgd_solver.cpp:105] Iteration 81600, lr = 0.0001
I1129 00:06:11.780102  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:07:27.667364  8649 solver.cpp:218] Iteration 81800 (2.12461 iter/s, 94.1348s/200 iters), loss = 0.0107249
I1129 00:07:27.667531  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0227102 (* 1 = 0.0227102 loss)
I1129 00:07:27.667551  8649 sgd_solver.cpp:105] Iteration 81800, lr = 0.0001
I1129 00:09:01.342636  8649 solver.cpp:330] Iteration 82000, Testing net (#0)
I1129 00:09:13.949184  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:09:14.001883  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917402
I1129 00:09:14.001915  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.416091 (* 1 = 0.416091 loss)
I1129 00:09:14.466994  8649 solver.cpp:218] Iteration 82000 (1.87255 iter/s, 106.806s/200 iters), loss = 0.0112926
I1129 00:09:14.467041  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00155878 (* 1 = 0.00155878 loss)
I1129 00:09:14.467051  8649 sgd_solver.cpp:105] Iteration 82000, lr = 0.0001
I1129 00:09:28.248590  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:10:48.629679  8649 solver.cpp:218] Iteration 82200 (2.12385 iter/s, 94.1685s/200 iters), loss = 0.0064932
I1129 00:10:48.629885  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0159723 (* 1 = 0.0159723 loss)
I1129 00:10:48.629896  8649 sgd_solver.cpp:105] Iteration 82200, lr = 0.0001
I1129 00:12:22.803759  8649 solver.cpp:218] Iteration 82400 (2.1236 iter/s, 94.1796s/200 iters), loss = 0.0124177
I1129 00:12:22.803851  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00382748 (* 1 = 0.00382748 loss)
I1129 00:12:22.803863  8649 sgd_solver.cpp:105] Iteration 82400, lr = 0.0001
I1129 00:12:32.223868  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:13:09.411303  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_82500.caffemodel
I1129 00:13:09.425525  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_82500.solverstate
I1129 00:13:56.957988  8649 solver.cpp:218] Iteration 82600 (2.12405 iter/s, 94.1598s/200 iters), loss = 0.00284247
I1129 00:13:56.958076  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00404569 (* 1 = 0.00404569 loss)
I1129 00:13:56.958086  8649 sgd_solver.cpp:105] Iteration 82600, lr = 0.0001
I1129 00:15:31.091038  8649 solver.cpp:218] Iteration 82800 (2.12453 iter/s, 94.1386s/200 iters), loss = 0.00125878
I1129 00:15:31.091128  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000725768 (* 1 = 0.000725768 loss)
I1129 00:15:31.091141  8649 sgd_solver.cpp:105] Iteration 82800, lr = 0.0001
I1129 00:15:36.040035  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:17:04.530577  8649 solver.cpp:330] Iteration 83000, Testing net (#0)
I1129 00:17:17.077692  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:17:17.130148  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917602
I1129 00:17:17.130190  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.415885 (* 1 = 0.415885 loss)
I1129 00:17:17.595397  8649 solver.cpp:218] Iteration 83000 (1.87775 iter/s, 106.511s/200 iters), loss = 0.00586902
I1129 00:17:17.595438  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00162319 (* 1 = 0.00162319 loss)
I1129 00:17:17.595448  8649 sgd_solver.cpp:105] Iteration 83000, lr = 0.0001
I1129 00:18:51.418037  8649 solver.cpp:218] Iteration 83200 (2.13156 iter/s, 93.8281s/200 iters), loss = 0.0104343
I1129 00:18:51.418118  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00123984 (* 1 = 0.00123984 loss)
I1129 00:18:51.418133  8649 sgd_solver.cpp:105] Iteration 83200, lr = 0.0001
I1129 00:18:52.011420  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:20:25.249488  8649 solver.cpp:218] Iteration 83400 (2.13136 iter/s, 93.8368s/200 iters), loss = 0.0125775
I1129 00:20:25.249550  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00803551 (* 1 = 0.00803551 loss)
I1129 00:20:25.249562  8649 sgd_solver.cpp:105] Iteration 83400, lr = 0.0001
I1129 00:21:55.208982  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:21:59.081379  8649 solver.cpp:218] Iteration 83600 (2.13135 iter/s, 93.8372s/200 iters), loss = 0.00326569
I1129 00:21:59.081426  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000250321 (* 1 = 0.000250321 loss)
I1129 00:21:59.081439  8649 sgd_solver.cpp:105] Iteration 83600, lr = 0.0001
I1129 00:23:32.905977  8649 solver.cpp:218] Iteration 83800 (2.13152 iter/s, 93.8299s/200 iters), loss = 0.00290419
I1129 00:23:32.906110  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0019008 (* 1 = 0.0019008 loss)
I1129 00:23:32.906121  8649 sgd_solver.cpp:105] Iteration 83800, lr = 0.0001
I1129 00:24:58.535698  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:25:06.281982  8649 solver.cpp:330] Iteration 84000, Testing net (#0)
I1129 00:25:18.760562  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:25:18.812826  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917102
I1129 00:25:18.812860  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.416276 (* 1 = 0.416276 loss)
I1129 00:25:19.277762  8649 solver.cpp:218] Iteration 84000 (1.88009 iter/s, 106.378s/200 iters), loss = 0.00321819
I1129 00:25:19.277806  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00020905 (* 1 = 0.00020905 loss)
I1129 00:25:19.277815  8649 sgd_solver.cpp:105] Iteration 84000, lr = 0.0001
I1129 00:26:53.087318  8649 solver.cpp:218] Iteration 84200 (2.13186 iter/s, 93.8148s/200 iters), loss = 0.00809528
I1129 00:26:53.087389  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00678705 (* 1 = 0.00678705 loss)
I1129 00:26:53.087399  8649 sgd_solver.cpp:105] Iteration 84200, lr = 0.0001
I1129 00:28:14.257405  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:28:26.920902  8649 solver.cpp:218] Iteration 84400 (2.13137 iter/s, 93.8365s/200 iters), loss = 0.0276148
I1129 00:28:26.920938  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0959741 (* 1 = 0.0959741 loss)
I1129 00:28:26.920946  8649 sgd_solver.cpp:105] Iteration 84400, lr = 0.0001
I1129 00:30:00.748627  8649 solver.cpp:218] Iteration 84600 (2.13153 iter/s, 93.8294s/200 iters), loss = 0.00311799
I1129 00:30:00.748782  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0028376 (* 1 = 0.0028376 loss)
I1129 00:30:00.748792  8649 sgd_solver.cpp:105] Iteration 84600, lr = 0.0001
I1129 00:31:17.565371  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:31:34.572365  8649 solver.cpp:218] Iteration 84800 (2.13161 iter/s, 93.8258s/200 iters), loss = 0.00475086
I1129 00:31:34.572412  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0111947 (* 1 = 0.0111947 loss)
I1129 00:31:34.572422  8649 sgd_solver.cpp:105] Iteration 84800, lr = 0.0001
I1129 00:33:07.942003  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_85000.caffemodel
I1129 00:33:07.954293  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_85000.solverstate
I1129 00:33:07.959095  8649 solver.cpp:330] Iteration 85000, Testing net (#0)
I1129 00:33:20.441375  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:33:20.493368  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917802
I1129 00:33:20.493402  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.416202 (* 1 = 0.416202 loss)
I1129 00:33:20.958148  8649 solver.cpp:218] Iteration 85000 (1.8799 iter/s, 106.389s/200 iters), loss = 0.00391851
I1129 00:33:20.958194  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00309938 (* 1 = 0.00309938 loss)
I1129 00:33:20.958206  8649 sgd_solver.cpp:105] Iteration 85000, lr = 0.0001
I1129 00:34:33.332808  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:34:54.798444  8649 solver.cpp:218] Iteration 85200 (2.13121 iter/s, 93.8433s/200 iters), loss = 0.0246848
I1129 00:34:54.798481  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0787747 (* 1 = 0.0787747 loss)
I1129 00:34:54.798491  8649 sgd_solver.cpp:105] Iteration 85200, lr = 0.0001
I1129 00:36:28.643275  8649 solver.cpp:218] Iteration 85400 (2.1311 iter/s, 93.8481s/200 iters), loss = 0.00229589
I1129 00:36:28.643375  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00634602 (* 1 = 0.00634602 loss)
I1129 00:36:28.643388  8649 sgd_solver.cpp:105] Iteration 85400, lr = 0.0001
I1129 00:37:36.711199  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:38:02.533773  8649 solver.cpp:218] Iteration 85600 (2.13006 iter/s, 93.894s/200 iters), loss = 0.00225296
I1129 00:38:02.533823  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 6.47651e-05 (* 1 = 6.47651e-05 loss)
I1129 00:38:02.533831  8649 sgd_solver.cpp:105] Iteration 85600, lr = 0.0001
I1129 00:39:36.417541  8649 solver.cpp:218] Iteration 85800 (2.13021 iter/s, 93.8875s/200 iters), loss = 0.00129514
I1129 00:39:36.417695  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000738829 (* 1 = 0.000738829 loss)
I1129 00:39:36.417706  8649 sgd_solver.cpp:105] Iteration 85800, lr = 0.0001
I1129 00:40:40.026104  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:41:09.834131  8649 solver.cpp:330] Iteration 86000, Testing net (#0)
I1129 00:41:22.353631  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:41:22.405866  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917302
I1129 00:41:22.405899  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.416932 (* 1 = 0.416932 loss)
I1129 00:41:22.870493  8649 solver.cpp:218] Iteration 86000 (1.87869 iter/s, 106.457s/200 iters), loss = 0.00771496
I1129 00:41:22.870565  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0161245 (* 1 = 0.0161245 loss)
I1129 00:41:22.870580  8649 sgd_solver.cpp:105] Iteration 86000, lr = 0.0001
I1129 00:42:56.755934  8649 solver.cpp:218] Iteration 86200 (2.13016 iter/s, 93.8895s/200 iters), loss = 0.00452371
I1129 00:42:56.756026  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00400399 (* 1 = 0.00400399 loss)
I1129 00:42:56.756039  8649 sgd_solver.cpp:105] Iteration 86200, lr = 0.0001
I1129 00:43:56.033548  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:44:30.655267  8649 solver.cpp:218] Iteration 86400 (2.12985 iter/s, 93.9035s/200 iters), loss = 0.00682631
I1129 00:44:30.655360  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0013031 (* 1 = 0.0013031 loss)
I1129 00:44:30.655372  8649 sgd_solver.cpp:105] Iteration 86400, lr = 0.0001
I1129 00:46:04.538468  8649 solver.cpp:218] Iteration 86600 (2.13021 iter/s, 93.8874s/200 iters), loss = 0.0037757
I1129 00:46:04.538595  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.001612 (* 1 = 0.001612 loss)
I1129 00:46:04.538609  8649 sgd_solver.cpp:105] Iteration 86600, lr = 0.0001
I1129 00:46:59.345469  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:47:38.414885  8649 solver.cpp:218] Iteration 86800 (2.13036 iter/s, 93.8807s/200 iters), loss = 0.0110885
I1129 00:47:38.414975  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0125721 (* 1 = 0.0125721 loss)
I1129 00:47:38.414989  8649 sgd_solver.cpp:105] Iteration 86800, lr = 0.0001
I1129 00:49:11.818699  8649 solver.cpp:330] Iteration 87000, Testing net (#0)
I1129 00:49:24.346431  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:49:24.399047  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917302
I1129 00:49:24.399082  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.416581 (* 1 = 0.416581 loss)
I1129 00:49:24.863186  8649 solver.cpp:218] Iteration 87000 (1.87876 iter/s, 106.453s/200 iters), loss = 0.00336629
I1129 00:49:24.863241  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00187891 (* 1 = 0.00187891 loss)
I1129 00:49:24.863256  8649 sgd_solver.cpp:105] Iteration 87000, lr = 0.0001
I1129 00:50:15.331197  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:50:58.750347  8649 solver.cpp:218] Iteration 87200 (2.13012 iter/s, 93.8916s/200 iters), loss = 0.0112881
I1129 00:50:58.750434  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00174928 (* 1 = 0.00174928 loss)
I1129 00:50:58.750447  8649 sgd_solver.cpp:105] Iteration 87200, lr = 0.0001
I1129 00:52:32.618432  8649 solver.cpp:218] Iteration 87400 (2.13055 iter/s, 93.8726s/200 iters), loss = 0.00338269
I1129 00:52:32.618505  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00528181 (* 1 = 0.00528181 loss)
I1129 00:52:32.618520  8649 sgd_solver.cpp:105] Iteration 87400, lr = 0.0001
I1129 00:53:18.624784  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:53:19.095818  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_87500.caffemodel
I1129 00:53:19.122149  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_87500.solverstate
I1129 00:54:06.532527  8649 solver.cpp:218] Iteration 87600 (2.1295 iter/s, 93.9186s/200 iters), loss = 0.00300318
I1129 00:54:06.532632  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00313484 (* 1 = 0.00313484 loss)
I1129 00:54:06.532646  8649 sgd_solver.cpp:105] Iteration 87600, lr = 0.0001
I1129 00:55:40.393357  8649 solver.cpp:218] Iteration 87800 (2.13071 iter/s, 93.8654s/200 iters), loss = 0.00432779
I1129 00:55:40.393450  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00429114 (* 1 = 0.00429114 loss)
I1129 00:55:40.393463  8649 sgd_solver.cpp:105] Iteration 87800, lr = 0.0001
I1129 00:56:22.141798  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:57:14.075507  8649 solver.cpp:330] Iteration 88000, Testing net (#0)
I1129 00:57:26.707957  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 00:57:26.761328  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917702
I1129 00:57:26.761363  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.4168 (* 1 = 0.4168 loss)
I1129 00:57:27.226696  8649 solver.cpp:218] Iteration 88000 (1.87198 iter/s, 106.839s/200 iters), loss = 0.0119131
I1129 00:57:27.226745  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0444949 (* 1 = 0.0444949 loss)
I1129 00:57:27.226755  8649 sgd_solver.cpp:105] Iteration 88000, lr = 0.0001
I1129 00:59:01.448462  8649 solver.cpp:218] Iteration 88200 (2.12255 iter/s, 94.2264s/200 iters), loss = 0.00112225
I1129 00:59:01.448551  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000892938 (* 1 = 0.000892938 loss)
I1129 00:59:01.448566  8649 sgd_solver.cpp:105] Iteration 88200, lr = 0.0001
I1129 00:59:38.803571  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:00:35.703130  8649 solver.cpp:218] Iteration 88400 (2.12181 iter/s, 94.2593s/200 iters), loss = 0.0203949
I1129 01:00:35.703219  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0574657 (* 1 = 0.0574657 loss)
I1129 01:00:35.703233  8649 sgd_solver.cpp:105] Iteration 88400, lr = 0.0001
I1129 01:02:09.913308  8649 solver.cpp:218] Iteration 88600 (2.12277 iter/s, 94.2167s/200 iters), loss = 0.0064762
I1129 01:02:09.913420  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0122644 (* 1 = 0.0122644 loss)
I1129 01:02:09.913435  8649 sgd_solver.cpp:105] Iteration 88600, lr = 0.0001
I1129 01:02:42.891957  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:03:44.090941  8649 solver.cpp:218] Iteration 88800 (2.12341 iter/s, 94.1881s/200 iters), loss = 0.00139547
I1129 01:03:44.091063  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0004087 (* 1 = 0.0004087 loss)
I1129 01:03:44.091078  8649 sgd_solver.cpp:105] Iteration 88800, lr = 0.0001
I1129 01:05:17.821574  8649 solver.cpp:330] Iteration 89000, Testing net (#0)
I1129 01:05:30.456387  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:05:30.509193  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917302
I1129 01:05:30.509227  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.417462 (* 1 = 0.417462 loss)
I1129 01:05:30.975548  8649 solver.cpp:218] Iteration 89000 (1.87099 iter/s, 106.895s/200 iters), loss = 0.0209458
I1129 01:05:30.975637  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0175607 (* 1 = 0.0175607 loss)
I1129 01:05:30.975649  8649 sgd_solver.cpp:105] Iteration 89000, lr = 0.0001
I1129 01:05:59.453233  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:07:04.891218  8649 solver.cpp:218] Iteration 89200 (2.12937 iter/s, 93.9246s/200 iters), loss = 0.0225892
I1129 01:07:04.891335  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0417137 (* 1 = 0.0417137 loss)
I1129 01:07:04.891348  8649 sgd_solver.cpp:105] Iteration 89200, lr = 0.0001
I1129 01:08:38.720250  8649 solver.cpp:218] Iteration 89400 (2.13135 iter/s, 93.8374s/200 iters), loss = 0.00373454
I1129 01:08:38.720319  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00381182 (* 1 = 0.00381182 loss)
I1129 01:08:38.720331  8649 sgd_solver.cpp:105] Iteration 89400, lr = 0.0001
I1129 01:09:02.781944  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:10:12.553979  8649 solver.cpp:218] Iteration 89600 (2.13125 iter/s, 93.8417s/200 iters), loss = 0.00263993
I1129 01:10:12.554040  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00213202 (* 1 = 0.00213202 loss)
I1129 01:10:12.554052  8649 sgd_solver.cpp:105] Iteration 89600, lr = 0.0001
I1129 01:11:46.380113  8649 solver.cpp:218] Iteration 89800 (2.13143 iter/s, 93.8337s/200 iters), loss = 0.00880621
I1129 01:11:46.380192  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00432858 (* 1 = 0.00432858 loss)
I1129 01:11:46.380203  8649 sgd_solver.cpp:105] Iteration 89800, lr = 0.0001
I1129 01:12:05.977119  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:13:19.755353  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_90000.caffemodel
I1129 01:13:19.767776  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_90000.solverstate
I1129 01:13:19.772482  8649 solver.cpp:330] Iteration 90000, Testing net (#0)
I1129 01:13:32.253594  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:13:32.305603  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917302
I1129 01:13:32.305629  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.417118 (* 1 = 0.417118 loss)
I1129 01:13:32.770180  8649 solver.cpp:218] Iteration 90000 (1.87973 iter/s, 106.398s/200 iters), loss = 0.0315108
I1129 01:13:32.770225  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00644796 (* 1 = 0.00644796 loss)
I1129 01:13:32.770233  8649 sgd_solver.cpp:105] Iteration 90000, lr = 0.0001
I1129 01:15:06.602501  8649 solver.cpp:218] Iteration 90200 (2.1313 iter/s, 93.8393s/200 iters), loss = 0.0225293
I1129 01:15:06.602598  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000451086 (* 1 = 0.000451086 loss)
I1129 01:15:06.602614  8649 sgd_solver.cpp:105] Iteration 90200, lr = 0.0001
I1129 01:15:21.856237  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:16:40.457537  8649 solver.cpp:218] Iteration 90400 (2.13079 iter/s, 93.8618s/200 iters), loss = 0.00282399
I1129 01:16:40.457636  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00237566 (* 1 = 0.00237566 loss)
I1129 01:16:40.457648  8649 sgd_solver.cpp:105] Iteration 90400, lr = 0.0001
I1129 01:18:14.330783  8649 solver.cpp:218] Iteration 90600 (2.13038 iter/s, 93.8798s/200 iters), loss = 0.0121175
I1129 01:18:14.330880  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0258766 (* 1 = 0.0258766 loss)
I1129 01:18:14.330895  8649 sgd_solver.cpp:105] Iteration 90600, lr = 0.0001
I1129 01:18:25.133261  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:19:48.193922  8649 solver.cpp:218] Iteration 90800 (2.13062 iter/s, 93.8696s/200 iters), loss = 0.00476129
I1129 01:19:48.194025  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00959489 (* 1 = 0.00959489 loss)
I1129 01:19:48.194038  8649 sgd_solver.cpp:105] Iteration 90800, lr = 0.0001
I1129 01:21:21.592713  8649 solver.cpp:330] Iteration 91000, Testing net (#0)
I1129 01:21:34.110664  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:21:34.163053  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917202
I1129 01:21:34.163084  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.41716 (* 1 = 0.41716 loss)
I1129 01:21:34.627760  8649 solver.cpp:218] Iteration 91000 (1.87898 iter/s, 106.441s/200 iters), loss = 0.00269824
I1129 01:21:34.627812  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000348687 (* 1 = 0.000348687 loss)
I1129 01:21:34.627823  8649 sgd_solver.cpp:105] Iteration 91000, lr = 0.0001
I1129 01:21:41.091302  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:23:08.516384  8649 solver.cpp:218] Iteration 91200 (2.13004 iter/s, 93.8949s/200 iters), loss = 0.00364567
I1129 01:23:08.516464  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00246079 (* 1 = 0.00246079 loss)
I1129 01:23:08.516474  8649 sgd_solver.cpp:105] Iteration 91200, lr = 0.0001
I1129 01:24:42.380408  8649 solver.cpp:218] Iteration 91400 (2.1306 iter/s, 93.8701s/200 iters), loss = 0.00460675
I1129 01:24:42.380494  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00488836 (* 1 = 0.00488836 loss)
I1129 01:24:42.380511  8649 sgd_solver.cpp:105] Iteration 91400, lr = 0.0001
I1129 01:24:44.383796  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:26:16.261310  8649 solver.cpp:218] Iteration 91600 (2.13022 iter/s, 93.887s/200 iters), loss = 0.0143596
I1129 01:26:16.261399  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00247069 (* 1 = 0.00247069 loss)
I1129 01:26:16.261413  8649 sgd_solver.cpp:105] Iteration 91600, lr = 0.0001
I1129 01:27:47.796195  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:27:50.141394  8649 solver.cpp:218] Iteration 91800 (2.13024 iter/s, 93.8861s/200 iters), loss = 0.0141584
I1129 01:27:50.141443  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0191285 (* 1 = 0.0191285 loss)
I1129 01:27:50.141453  8649 sgd_solver.cpp:105] Iteration 91800, lr = 0.0001
I1129 01:29:23.549430  8649 solver.cpp:330] Iteration 92000, Testing net (#0)
I1129 01:29:36.054185  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:29:36.106403  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917502
I1129 01:29:36.106438  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.416991 (* 1 = 0.416991 loss)
I1129 01:29:36.572926  8649 solver.cpp:218] Iteration 92000 (1.87902 iter/s, 106.438s/200 iters), loss = 0.0174449
I1129 01:29:36.572979  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0226766 (* 1 = 0.0226766 loss)
I1129 01:29:36.572990  8649 sgd_solver.cpp:105] Iteration 92000, lr = 0.0001
I1129 01:31:03.651453  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:31:10.462468  8649 solver.cpp:218] Iteration 92200 (2.13003 iter/s, 93.8955s/200 iters), loss = 0.00551056
I1129 01:31:10.462517  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00310847 (* 1 = 0.00310847 loss)
I1129 01:31:10.462528  8649 sgd_solver.cpp:105] Iteration 92200, lr = 0.0001
I1129 01:32:44.333456  8649 solver.cpp:218] Iteration 92400 (2.13045 iter/s, 93.8769s/200 iters), loss = 0.0105477
I1129 01:32:44.333537  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00661392 (* 1 = 0.00661392 loss)
I1129 01:32:44.333551  8649 sgd_solver.cpp:105] Iteration 92400, lr = 0.0001
I1129 01:33:30.813395  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_92500.caffemodel
I1129 01:33:30.825714  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_92500.solverstate
I1129 01:34:07.092026  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:34:18.241842  8649 solver.cpp:218] Iteration 92600 (2.1296 iter/s, 93.9142s/200 iters), loss = 0.00832549
I1129 01:34:18.241892  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00812576 (* 1 = 0.00812576 loss)
I1129 01:34:18.241902  8649 sgd_solver.cpp:105] Iteration 92600, lr = 0.0001
I1129 01:35:52.110695  8649 solver.cpp:218] Iteration 92800 (2.1305 iter/s, 93.8746s/200 iters), loss = 0.00844486
I1129 01:35:52.110864  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.026025 (* 1 = 0.026025 loss)
I1129 01:35:52.110875  8649 sgd_solver.cpp:105] Iteration 92800, lr = 0.0001
I1129 01:37:10.549229  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:37:25.722524  8649 solver.cpp:330] Iteration 93000, Testing net (#0)
I1129 01:37:38.305018  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:37:38.357570  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917002
I1129 01:37:38.357612  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.417166 (* 1 = 0.417166 loss)
I1129 01:37:38.822787  8649 solver.cpp:218] Iteration 93000 (1.87414 iter/s, 106.716s/200 iters), loss = 0.00595754
I1129 01:37:38.822839  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00605297 (* 1 = 0.00605297 loss)
I1129 01:37:38.822851  8649 sgd_solver.cpp:105] Iteration 93000, lr = 0.0001
I1129 01:39:12.901124  8649 solver.cpp:218] Iteration 93200 (2.1258 iter/s, 94.0821s/200 iters), loss = 0.00609158
I1129 01:39:12.901207  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.002901 (* 1 = 0.002901 loss)
I1129 01:39:12.901221  8649 sgd_solver.cpp:105] Iteration 93200, lr = 0.0001
I1129 01:40:26.958917  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:40:46.948400  8649 solver.cpp:218] Iteration 93400 (2.1265 iter/s, 94.0513s/200 iters), loss = 0.00206135
I1129 01:40:46.948518  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00484919 (* 1 = 0.00484919 loss)
I1129 01:40:46.948537  8649 sgd_solver.cpp:105] Iteration 93400, lr = 0.0001
I1129 01:42:20.993979  8649 solver.cpp:218] Iteration 93600 (2.12653 iter/s, 94.0498s/200 iters), loss = 0.0194086
I1129 01:42:20.994071  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00199597 (* 1 = 0.00199597 loss)
I1129 01:42:20.994086  8649 sgd_solver.cpp:105] Iteration 93600, lr = 0.0001
I1129 01:43:30.586474  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:43:55.050967  8649 solver.cpp:218] Iteration 93800 (2.12627 iter/s, 94.0615s/200 iters), loss = 0.00814694
I1129 01:43:55.051030  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0192753 (* 1 = 0.0192753 loss)
I1129 01:43:55.051043  8649 sgd_solver.cpp:105] Iteration 93800, lr = 0.0001
I1129 01:45:28.673254  8649 solver.cpp:330] Iteration 94000, Testing net (#0)
I1129 01:45:41.258986  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:45:41.311816  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917202
I1129 01:45:41.311858  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.417256 (* 1 = 0.417256 loss)
I1129 01:45:41.777014  8649 solver.cpp:218] Iteration 94000 (1.87386 iter/s, 106.731s/200 iters), loss = 0.00181566
I1129 01:45:41.777093  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00106534 (* 1 = 0.00106534 loss)
I1129 01:45:41.777108  8649 sgd_solver.cpp:105] Iteration 94000, lr = 0.0001
I1129 01:46:46.890472  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:47:15.628343  8649 solver.cpp:218] Iteration 94200 (2.13092 iter/s, 93.8561s/200 iters), loss = 0.00941679
I1129 01:47:15.628396  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00104994 (* 1 = 0.00104994 loss)
I1129 01:47:15.628410  8649 sgd_solver.cpp:105] Iteration 94200, lr = 0.0001
I1129 01:48:49.446036  8649 solver.cpp:218] Iteration 94400 (2.13168 iter/s, 93.8226s/200 iters), loss = 0.00516717
I1129 01:48:49.446130  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000561424 (* 1 = 0.000561424 loss)
I1129 01:48:49.446141  8649 sgd_solver.cpp:105] Iteration 94400, lr = 0.0001
I1129 01:49:50.082099  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:50:23.272157  8649 solver.cpp:218] Iteration 94600 (2.13149 iter/s, 93.8311s/200 iters), loss = 0.00782482
I1129 01:50:23.272263  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0197711 (* 1 = 0.0197711 loss)
I1129 01:50:23.272277  8649 sgd_solver.cpp:105] Iteration 94600, lr = 0.0001
I1129 01:51:57.087841  8649 solver.cpp:218] Iteration 94800 (2.13172 iter/s, 93.8207s/200 iters), loss = 0.00709384
I1129 01:51:57.087927  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0128678 (* 1 = 0.0128678 loss)
I1129 01:51:57.087937  8649 sgd_solver.cpp:105] Iteration 94800, lr = 0.0001
I1129 01:52:53.380103  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:53:30.447103  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_95000.caffemodel
I1129 01:53:30.459728  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_95000.solverstate
I1129 01:53:30.464982  8649 solver.cpp:330] Iteration 95000, Testing net (#0)
I1129 01:53:42.945281  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:53:42.997601  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917502
I1129 01:53:42.997629  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.417609 (* 1 = 0.417609 loss)
I1129 01:53:43.461544  8649 solver.cpp:218] Iteration 95000 (1.88006 iter/s, 106.38s/200 iters), loss = 0.00868794
I1129 01:53:43.461601  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000995495 (* 1 = 0.000995495 loss)
I1129 01:53:43.461617  8649 sgd_solver.cpp:105] Iteration 95000, lr = 0.0001
I1129 01:55:17.287206  8649 solver.cpp:218] Iteration 95200 (2.13149 iter/s, 93.8309s/200 iters), loss = 0.00242175
I1129 01:55:17.287379  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000533005 (* 1 = 0.000533005 loss)
I1129 01:55:17.287392  8649 sgd_solver.cpp:105] Iteration 95200, lr = 0.0001
I1129 01:56:09.155369  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:56:51.198330  8649 solver.cpp:218] Iteration 95400 (2.12956 iter/s, 93.9163s/200 iters), loss = 0.00363297
I1129 01:56:51.198434  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00122906 (* 1 = 0.00122906 loss)
I1129 01:56:51.198447  8649 sgd_solver.cpp:105] Iteration 95400, lr = 0.0001
I1129 01:58:25.141750  8649 solver.cpp:218] Iteration 95600 (2.12882 iter/s, 93.9487s/200 iters), loss = 0.0143101
I1129 01:58:25.141839  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0215589 (* 1 = 0.0215589 loss)
I1129 01:58:25.141854  8649 sgd_solver.cpp:105] Iteration 95600, lr = 0.0001
I1129 01:59:12.724047  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 01:59:59.119793  8649 solver.cpp:218] Iteration 95800 (2.12804 iter/s, 93.9833s/200 iters), loss = 0.00409666
I1129 01:59:59.119918  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.001911 (* 1 = 0.001911 loss)
I1129 01:59:59.119931  8649 sgd_solver.cpp:105] Iteration 95800, lr = 0.0001
I1129 02:01:32.613149  8649 solver.cpp:330] Iteration 96000, Testing net (#0)
I1129 02:01:45.144435  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:01:45.196619  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917602
I1129 02:01:45.196646  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.417674 (* 1 = 0.417674 loss)
I1129 02:01:45.662339  8649 solver.cpp:218] Iteration 96000 (1.87708 iter/s, 106.549s/200 iters), loss = 0.00981591
I1129 02:01:45.662386  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00456745 (* 1 = 0.00456745 loss)
I1129 02:01:45.662400  8649 sgd_solver.cpp:105] Iteration 96000, lr = 1e-05
I1129 02:02:28.750089  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:03:19.613097  8649 solver.cpp:218] Iteration 96200 (2.12865 iter/s, 93.9561s/200 iters), loss = 0.00446135
I1129 02:03:19.613210  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000659098 (* 1 = 0.000659098 loss)
I1129 02:03:19.613225  8649 sgd_solver.cpp:105] Iteration 96200, lr = 1e-05
I1129 02:04:53.554623  8649 solver.cpp:218] Iteration 96400 (2.12886 iter/s, 93.9469s/200 iters), loss = 0.00615862
I1129 02:04:53.554728  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000290977 (* 1 = 0.000290977 loss)
I1129 02:04:53.554738  8649 sgd_solver.cpp:105] Iteration 96400, lr = 1e-05
I1129 02:05:32.324030  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:06:27.477514  8649 solver.cpp:218] Iteration 96600 (2.12928 iter/s, 93.9283s/200 iters), loss = 0.00345739
I1129 02:06:27.477649  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00111263 (* 1 = 0.00111263 loss)
I1129 02:06:27.477661  8649 sgd_solver.cpp:105] Iteration 96600, lr = 1e-05
I1129 02:08:01.274468  8649 solver.cpp:218] Iteration 96800 (2.13214 iter/s, 93.8023s/200 iters), loss = 0.00255032
I1129 02:08:01.274626  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0038887 (* 1 = 0.0038887 loss)
I1129 02:08:01.274641  8649 sgd_solver.cpp:105] Iteration 96800, lr = 1e-05
I1129 02:08:35.518411  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:09:34.620632  8649 solver.cpp:330] Iteration 97000, Testing net (#0)
I1129 02:09:47.127197  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:09:47.179064  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917402
I1129 02:09:47.179091  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.417719 (* 1 = 0.417719 loss)
I1129 02:09:47.643081  8649 solver.cpp:218] Iteration 97000 (1.88015 iter/s, 106.375s/200 iters), loss = 0.00733833
I1129 02:09:47.643118  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00149502 (* 1 = 0.00149502 loss)
I1129 02:09:47.643127  8649 sgd_solver.cpp:105] Iteration 97000, lr = 1e-05
I1129 02:11:21.457967  8649 solver.cpp:218] Iteration 97200 (2.13175 iter/s, 93.8198s/200 iters), loss = 0.00342066
I1129 02:11:21.458034  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00624437 (* 1 = 0.00624437 loss)
I1129 02:11:21.458042  8649 sgd_solver.cpp:105] Iteration 97200, lr = 1e-05
I1129 02:11:51.365921  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:12:55.267787  8649 solver.cpp:218] Iteration 97400 (2.13186 iter/s, 93.8147s/200 iters), loss = 0.00592753
I1129 02:12:55.267889  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00531597 (* 1 = 0.00531597 loss)
I1129 02:12:55.267899  8649 sgd_solver.cpp:105] Iteration 97400, lr = 1e-05
I1129 02:13:41.702775  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_97500.caffemodel
I1129 02:13:41.715366  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_97500.solverstate
I1129 02:14:29.088392  8649 solver.cpp:218] Iteration 97600 (2.13162 iter/s, 93.8255s/200 iters), loss = 0.00632117
I1129 02:14:29.088547  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00224293 (* 1 = 0.00224293 loss)
I1129 02:14:29.088559  8649 sgd_solver.cpp:105] Iteration 97600, lr = 1e-05
I1129 02:14:54.546527  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:16:02.961045  8649 solver.cpp:218] Iteration 97800 (2.13043 iter/s, 93.8776s/200 iters), loss = 0.0119553
I1129 02:16:02.961205  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 8.06979e-05 (* 1 = 8.06979e-05 loss)
I1129 02:16:02.961225  8649 sgd_solver.cpp:105] Iteration 97800, lr = 1e-05
I1129 02:17:36.727118  8649 solver.cpp:330] Iteration 98000, Testing net (#0)
I1129 02:17:49.391496  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:17:49.443673  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917202
I1129 02:17:49.443709  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.417709 (* 1 = 0.417709 loss)
I1129 02:17:49.910555  8649 solver.cpp:218] Iteration 98000 (1.86994 iter/s, 106.955s/200 iters), loss = 0.00178922
I1129 02:17:49.910609  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000689266 (* 1 = 0.000689266 loss)
I1129 02:17:49.910620  8649 sgd_solver.cpp:105] Iteration 98000, lr = 1e-05
I1129 02:18:11.127440  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:19:24.135653  8649 solver.cpp:218] Iteration 98200 (2.12246 iter/s, 94.2303s/200 iters), loss = 0.000383435
I1129 02:19:24.135767  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000219122 (* 1 = 0.000219122 loss)
I1129 02:19:24.135790  8649 sgd_solver.cpp:105] Iteration 98200, lr = 1e-05
I1129 02:20:58.358707  8649 solver.cpp:218] Iteration 98400 (2.12251 iter/s, 94.2282s/200 iters), loss = 0.0248394
I1129 02:20:58.358811  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0764688 (* 1 = 0.0764688 loss)
I1129 02:20:58.358829  8649 sgd_solver.cpp:105] Iteration 98400, lr = 1e-05
I1129 02:21:15.089154  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:22:32.613770  8649 solver.cpp:218] Iteration 98600 (2.12179 iter/s, 94.2602s/200 iters), loss = 0.00127905
I1129 02:22:32.613875  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0033721 (* 1 = 0.0033721 loss)
I1129 02:22:32.613893  8649 sgd_solver.cpp:105] Iteration 98600, lr = 1e-05
I1129 02:24:06.866515  8649 solver.cpp:218] Iteration 98800 (2.12184 iter/s, 94.2579s/200 iters), loss = 0.00816279
I1129 02:24:06.866672  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00128305 (* 1 = 0.00128305 loss)
I1129 02:24:06.866691  8649 sgd_solver.cpp:105] Iteration 98800, lr = 1e-05
I1129 02:24:19.242975  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:25:40.615552  8649 solver.cpp:330] Iteration 99000, Testing net (#0)
I1129 02:25:53.263134  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:25:53.314971  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917602
I1129 02:25:53.315002  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.417599 (* 1 = 0.417599 loss)
I1129 02:25:53.780028  8649 solver.cpp:218] Iteration 99000 (1.87057 iter/s, 106.919s/200 iters), loss = 0.00665238
I1129 02:25:53.780191  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00541243 (* 1 = 0.00541243 loss)
I1129 02:25:53.780236  8649 sgd_solver.cpp:105] Iteration 99000, lr = 1e-05
I1129 02:27:27.700827  8649 solver.cpp:218] Iteration 99200 (2.12934 iter/s, 93.926s/200 iters), loss = 0.00583591
I1129 02:27:27.700973  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0077969 (* 1 = 0.0077969 loss)
I1129 02:27:27.701010  8649 sgd_solver.cpp:105] Iteration 99200, lr = 1e-05
I1129 02:27:35.579686  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:29:01.654824  8649 solver.cpp:218] Iteration 99400 (2.12858 iter/s, 93.9592s/200 iters), loss = 0.00138583
I1129 02:29:01.654978  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00363361 (* 1 = 0.00363361 loss)
I1129 02:29:01.654989  8649 sgd_solver.cpp:105] Iteration 99400, lr = 1e-05
I1129 02:30:35.612351  8649 solver.cpp:218] Iteration 99600 (2.1285 iter/s, 93.9627s/200 iters), loss = 0.00278013
I1129 02:30:35.612447  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00058978 (* 1 = 0.00058978 loss)
I1129 02:30:35.612457  8649 sgd_solver.cpp:105] Iteration 99600, lr = 1e-05
I1129 02:30:39.144479  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:32:09.574239  8649 solver.cpp:218] Iteration 99800 (2.1284 iter/s, 93.9672s/200 iters), loss = 0.00398538
I1129 02:32:09.574405  8649 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000201141 (* 1 = 0.000201141 loss)
I1129 02:32:09.574415  8649 sgd_solver.cpp:105] Iteration 99800, lr = 1e-05
I1129 02:33:42.588783  8655 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:33:43.060647  8649 solver.cpp:447] Snapshotting to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_100000.caffemodel
I1129 02:33:43.073737  8649 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/ljf/caffe-master/examples/ljftest_cifar10_ResNet_pool/model_save/caffe_ljftest_train_iter_100000.solverstate
I1129 02:33:43.112509  8649 solver.cpp:310] Iteration 100000, loss = 0.000107416
I1129 02:33:43.112537  8649 solver.cpp:330] Iteration 100000, Testing net (#0)
I1129 02:32:09.574415  8649 sgd_solver.cpp:105] Iteration 100000, lr = 1e-05
I1129 02:33:55.644296  8656 data_layer.cpp:73] Restarting data prefetching from start.
I1129 02:33:55.697636  8649 solver.cpp:397]     Test net output #0: Accuracy1 = 0.917302
I1129 02:33:55.697665  8649 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.417853 (* 1 = 0.417853 loss)
I1129 02:33:55.697671  8649 solver.cpp:315] Optimization Done.
I1129 02:33:55.697675  8649 caffe.cpp:259] Optimization Done.
